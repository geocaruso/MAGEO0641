[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and R for geographers",
    "section": "",
    "text": "Preface\nThis is the syllabus for the course Introduction to Data Analysis for Geographers with R (MAGEO0641) at the Department of Geography and Spatial Planning at the University of Luxembourg. It has been produced as a Quarto book by Geoffrey Caruso and Léandre Fabri with the aim of aggregating and homogenizing different material accumulated over the past few years.\nAs of September 2024, the assemblage is still a work in progress. We kindly ask you to refer to your notes during class to prepare for the examination and assignments.\nThe general structure and base material shown here have been organized by Geoffrey Caruso, who originally inherited teaching material from Dominique Peeters. We are grateful to the previous teaching assistants, Mirjam Schindler and Marlène Boura, as well as to David Dabin and Jonathan Jones, for their contributions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001_timetable.html",
    "href": "001_timetable.html",
    "title": "Timetable",
    "section": "",
    "text": "The course is made of 13 sessions. Each session comprises 5 (teaching) units of 45 minutes.\nWe have attempted to balance theoretical explanations and R practice in each session.\nThe 2024-2025 schedule is set as follows. However, please refer to your student guichet and the Moodle platform for potential changes during the semester.\n\nScheduled: 16 Sep 2024 at 14:00 to 18:00, CEST\nScheduled: 23 Sep 2024 at 14:00 to 18:00, CEST\nNo session on 30 Sep 2024\nScheduled: 7 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 14 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 21 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 28 Oct 2024 at 14:00 to 18:00, CET\nScheduled: 4 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 11 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 18 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 25 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 2 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 9 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 16 Dec 2024 at 14:00 to 18:00, CET",
    "crumbs": [
      "Timetable"
    ]
  },
  {
    "objectID": "002_learning.html",
    "href": "002_learning.html",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Objectives:",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#objectives",
    "href": "002_learning.html#objectives",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Overview of key concepts and formalisation in data analysis in geographical contexts\nUnderstanding and describing the statistical and spatial distribution of data with univariate statistical analysis\nUnderstanding how a geographical pattern relate or can be understood from others with bivariate and regression analysis\nRaising awareness as to the characteristics and difficulties of statistical and econometric (regression) analysis with geographical data.\nMastering essential R software skills for tabular data management, uni and bi-variate analysis and regression, and producing graphics",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#expected-outcomes",
    "href": "002_learning.html#expected-outcomes",
    "title": "Learning outcomes and evaluation",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nOn completion, each student should be able to\n\nDescribe the key concepts in spatial statistical analysis and the specificities of geographical space and spatial data\nDemonstrate a good command of R to handle statistical datasets and perform univariate, bivariate and multiple regressions analyses with good diagnostics\nExplain and use common univariate and bivariate statistics\nExplain and use exploratory methods\nExplain and apply standard regression methods and diagnostics, and discuss limits and problems when applied to geographical data\nExplain the principles and methods used to identify local effects and spatial autocorrelation\nRead and discuss detailed results of an empirical research article that deal with data analysis including a multivariate regression in a spatial or non-spatial context\nExplain and use mixed methods (Q-Methodology: hybrid approach between quantitative and qualitative methods)",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#evaluation",
    "href": "002_learning.html#evaluation",
    "title": "Learning outcomes and evaluation",
    "section": "Evaluation",
    "text": "Evaluation\n\nIndividual\n20% Continuous assessment: small tests in class and weekly exercises:\n40% R exam (3h) in GIS room, perform R analysis, answer a questionnaire and provide script\n40% Oral exam (30min): presenting a paper and answering questions about its details and the course",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "011_scope.html",
    "href": "011_scope.html",
    "title": "1  Statistical data analysis for geographers",
    "section": "",
    "text": "1.1 Scope\nThis course is an introduction to standard statistical techniques that geographers often encounter. Being at the crossroads of different fields, it is necessary for geographers to have a basic set of tools with which to interact with other experts and modelers who, although they may use different wording and have their own technical habits, use a common set of concepts and tools to analyze data and test their hypotheses.\nIn their own work and in their interactions with others, it is also important for geographers to keep in mind that the data they use are quite specific because they are about located objects or subjects, about places and their interactions. Most of the time, the data they use is georeferenced in some way (accurately or not). This inherently geographic aspect brings with it a number of challenges. In this course, we will highlight these challenges when performing standard data analysis. However, we won’t solve any of these geographic problems, and we won’t even explicitly use geographic features, i.e., no mapping, no use of georeferencing as such. Our goal is to equip students with standard statistical methods also used in related fields, with some critical thinking about their geographical nature or underlying spatial processes. In a nutshell, this is a journey from elementary statistics to spatial autocorrelation with a standard regression detour.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "href": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "title": "1  Statistical data analysis for geographers",
    "section": "1.2 Transparent geographical analysis empowered with R",
    "text": "1.2 Transparent geographical analysis empowered with R\nThe course is a blend of theory and practice, which we believe enhances intuition and understanding. Direct practice also helps, especially for human geographers with little training in quantitative methods, to demystify statistical concepts and provide confidence after repeated applications and interpretations.\nSoftware for statistical analysis has evolved rapidly and R is prominent in many disciplines. It is open and free. It is simply fantastic for spatial data analysis and may well be the only tool geographers really need in their data undertaking, even replacing GIS (Geographic Information Systems) software. You just need to get started with R.\n\n\n\n\n\nMost of our students have had some sort of theoretical statistics course so far in their studies, and have probably seen most of the content. Sometimes our students have had some practice with SPSS (or similar) software, but most have not used any real statistical software at all, and have a spreadsheet (e.g., Excel) as their only reference for data management, analysis, and graphing.\nWe chose R for its openness, leadership, large community, and later for its spatial features, but also because it forces students to be transparent and think about every step they take. Data analysis and visualization can be misleading and dangerous, it is necessary to think and facilitate replication for oneself and for others. R, and more generally the use of scripts and command lines, is absolutely necessary to bring robustness and trust.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html",
    "href": "012_spatial_analysis.html",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "2.1 Interdisciplinarity and perspective\nData analysis and statistics are used in many scientific fields. When the focus is on geographic objects, subjects, their patterns or relationships, we like to talk about spatial analysis. However, because geographic data is relevant to many fields and statistical methods are widely used, spatial analysis can be defined and approached differently.\nWithin geography, we understand the term spatial analysis to refer to all quantitative approaches, as opposed to qualitative approaches (although these can also be spatial and analytical). Within the quantitative part of the discipline, however, spatial analysis would mostly refer to applied statistical approaches, in contrast to GIS modeling, geosimulation, transportation modeling, or mathematical models. In this sense, this course is a spatial analysis course.\nHowever, for those researchers involved in spatial analysis close to regional science and economic geography (and perhaps close to landscape ecology and GIScience), the terms spatial data analysis or spatial statistics would more strictly refer to the explicit use of geographic information in the modeling process, not just the consideration of geographic elements. See for example the handbook of Fischer and Getis for discussion and examples.\nSimilarly, Goodchild and Longley (https://www.geos.ed.ac.uk/~gisteac/gis_book_abridged/files/ch40.pdf) suggest more broadly that spatial analysis could simply be a set of methods useful when the data are spatial (i.e. referenced in 2D frame). This definition however as they suggest would be too broad, if it does not address the question of whether the 2D frame actually matters. Rather spatial analysis is",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "href": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "the subset of analytic techniques whose results will change if the frame changes, or if objects are repositioned within it.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#links-with-theory",
    "href": "012_spatial_analysis.html#links-with-theory",
    "title": "2  Spatial data analysis: a definition",
    "section": "2.2 Links with theory",
    "text": "2.2 Links with theory\nScience progresses with tools and techniques but also by testing hypotheses and updating models and theory. How spatial analysis is linked to theory also depends on fields or sub-fields.\nFrom a quantitative geography viewpoint (adapted from Denise Pumain https://hypergeo.eu/theories-of-spatial-analysis/?lang=en), spatial analysis focuses on uncovering spatial structures and organizations. These structures can often be generalized into models, such as center-periphery relationships, gravity models, and urban hierarchies and networks.\nThe ultimate goal of spatial analysis is then to understand the processes that lead to the formation of these spatial structures.\nFrom a spatial economic or regional science viewpoint (as understood by a European quantitative geographer) spatial analysis consists of a set of techniques designed to:\n\nDescribe the location of activities and how they change over time\nEstimate reduced form models\n\nUnlike structural form models, which are direct representations or formulations of theoretical concepts, reduced form models are designed to better align with and fit the data.\nThere is probably no such a reduced or structural form model in quantitative geography, but in both case anyway, spatial analysis ultimately aims at testing and updating theories.\nWe very much agree with this perspective here, leading to giving more importance to the falsification of ideas and the interpretation of estimated coefficients than to prediction using as many data as possible.\nIf a variable is used it is because we have some idea of its importance and influence on others or its relevance, not just to obtain a fit. Hence we won’t use automatic models constructions (no stepwise regression for example) and leave out all the methods (neural networks, random forests, etc.) from which coefficients (if any) are difficult to interpret, even if these methods can be considered to belong to spatial analysis and use geographic data. This course is not about data mining or data crunching. We use a statistical lens to examine variations across space and how spatial relationships influence socio-economic patterns and behaviors or environmental effects.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html",
    "href": "013_space_axiom.html",
    "title": "3  Geographical space",
    "section": "",
    "text": "3.1 Absolute or pre-geographical space:",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "href": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "title": "3  Geographical space",
    "section": "",
    "text": "A set of places or locations \\(S\\)\nIdentified by their coordinates \\(x,y\\)\nSeparated by a distance \\(d(L)\\)\nDistance being measured along a given metric \\(L\\)",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#geographical-space",
    "href": "013_space_axiom.html#geographical-space",
    "title": "3  Geographical space",
    "section": "3.2 Geographical space:",
    "text": "3.2 Geographical space:\nS can be endowed with various attributes to form a geographical space:\n\nThe surface attribute \\(m\\), measured along a given metric related to coordinates\nVarious attributes \\(Z\\)\nDensity measures, i.e. any \\(Z/m\\)\n\n\n\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An Axiomatic Approach to Geographical Space.” Geographical Analysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html",
    "href": "014_space_issues.html",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "4.1 Equivalence and Independence\n(Based on discussions in Jayet, p. 2-13)\nStatistical analysis is based on two key principles, or invariants:\nHowever, both of these principles are challenged when applied in a spatial context. Spatial data often exhibit dependencies due to geographic proximity, which violates the assumption of independence. Similarly, the notion of statistical equivalence becomes problematic as spatial heterogeneity introduces variability across observations in different locations and because the spatial definition of objects may vary and their sampling irregular.\nThese challenges highlight the need for specialized approaches in spatial analysis, where standard statistical methods must be adapted to account for the structure and dependencies present in the spatial data.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence-and-independence",
    "href": "014_space_issues.html#equivalence-and-independence",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "All observations must be statistically equivalent: This means that no individual observation should be systematically different from others in the sample set. Each data point must have the same probability distribution, ensuring uniformity and comparability.\nAll observations must be independent from each other: In any statistical model, the assumption is that the occurrence of one observation does not influence or depend on another. Independence ensures the integrity of statistical results.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence",
    "href": "014_space_issues.html#equivalence",
    "title": "4  Spatial data issues",
    "section": "4.2 Equivalence",
    "text": "4.2 Equivalence\n\n4.2.1 Irregularity of observations and the nature of data\nWhile there are time cycles, making repetitive data logging along the time dimension doable, there is no such think as a spatial cycle for geographical data recording.\nMost spatial data has a irregular covering in space, which already challenges the equivalence of observations\n\n(source to be added, apologies if you are the author, I am happy to adapt)\n\nObservations (countries) are of different size, i.e. the surface attribute \\(m\\) matters here.\nSuppose \\(Z_{pop}\\) is the country population. One can expect \\(Z_{pop}\\) to relate to \\(m\\) if processes are homogeneous across space. However a \\(Z/m\\) density variables would still show these objects are very different.\nYet, other \\(Z_i\\) variables could still be compared using that \\(Z_{pop}\\) attributes. For example the active population of the place (country) as a percentage of its total population, or using other transformations (linear or not).\nNote that variations in volume/mass/size such as \\(Z_{pop}\\) are very common, with very few objects having a very large size compared to most others. Such a size effect\n\nimpacts on the total and central (mean, median) value of variables\nthe distribution of values when made in different observations’ regions\ntypically leads to outliers problems or heteroskedasticity (non constant variance)\n\nHowever, there are raster maps and some information is “regular”, such a precipitation, or can be “regularized”, such as population grids.\n\n(https://human-settlement.emergency.copernicus.eu/)\nThe discretization of geographic space should however be internally homogeneous, meaning the attributes within each grid cell (or other nwe objects) supposed to apply to every part of that cell.\nA difference is often made between continuous field data and discrete space objects.\nSee below the tabulation of these against the types of measurements by Haining (2010)\n\n\n\n4.2.2 Modifiable Areal unit Problem - MAUP\nOpenshaw (1984)\n\nAreal units = spatial objects such as zones or places or towns or regions\n\n\nGeography has consistently and dismally failed to tackle its entitation problems, and in that more than anything else lies the root of so many of its problems (Chapman 77)\n\n\nInsufficient thought is given to precisely what it is that is being studied. […] Little concern has been expressed about the nature and definition of the spatial objects under study\n\n\nFor many purposes the zones in a zoning system constitute the objects, or geographical individuals, that are the basic units for the observation and measurement of spatial phenomena.\n\n\nWith areal data, the spatial objects only exist after data collected for one set of entities (e.g. people) are subjected to an arbitrary aggregation (see also regularity discussion above) to produce a set of spatial units.\n\n\nHowever, there are no rules for areal aggregation, no standards, and no international conventions to guide the spatial aggregation process.\n\n\nThe areal units (zonal objects) used in many geographical studies are arbitrary, modifiable. Census areas have rarely an intrinsic geographical meaning\n\n\nA unmanageable combinatorial problem: There are approximately 10^12 different aggregations of 1,000 objects into 20 groups. If the aggregation process is constrained so that the groups consist of internally contiguous objects (i.e. all the objects assigned to the same group are geographical neighbours) then this huge number is reduced, but only by a few orders of magnitude.\n\nStan Openshaw distingues 2 interrelated issues, within the MAUP:\n\nThe scale problem: the variation in results that can often be obtained when data for one set of areal units are progressively aggregated into fewer and larger units for analysis.\nThe aggregation problem: the problem of alternative combinations of areal units at equal or similar scales. Any variation in results due to the use of alternative units of analysis when the number of units is held constant\n\n\nExample of effects on correlation coefficients:\n\nCorrelation between percentage vote for Republican candidates in the congressional election of 1968 and the percentage of the population over 60 years\nCorrelation at the 99 county level is 0.34\nAfter aggregation into six zones: 0.26 for the 6 congressional districts and 0.86 for a simple typology of Iowa into 6 rural-urban types (Openshaw and Taylor 77)\nCompare mean and dispersion of correlation coefficient after random zoning (using contiguity) and random sampling (grouping)\n\n\n\nNo systematic scale effect on correlation mean\nConsiderable variability about the mean values but reduces with increasing numbers of units\nThe standard deviations of the zoning distributions are considerably smaller than the corresponding sampling distributions but exhibit a greater degree of bias &gt; spatial autocorrelation effect",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#spatial-in-dependence",
    "href": "014_space_issues.html#spatial-in-dependence",
    "title": "4  Spatial data issues",
    "section": "4.3 Spatial (in-)dependence",
    "text": "4.3 Spatial (in-)dependence\n\n4.3.1 Interactions between observations\n\nNot only the dimensions and structures of observations is of importance but also their relative position in space\nThe distance (between objects), \\(d(L)\\), is at the very heart of geographical analysis AND the source of statistical difficulties\nThe level of interactions increases with proximity (distance functions or contiguities) (see gravity-based theories)\n\n\n\n4.3.2 Tobler’s first law of geography\nTobler (1970)\n\n\n\n4.3.3 Spatial autocorrelation\nAnselin, Luc and Bera,I (1998)\n\n\n\n\n\n\nAnselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear Regression Models with an Introduction to Spatial Econometrics: Regression Models with an Anselin Bera i. INTRODUCTION.” In. CRC Press.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced Data.” In, edited by Manfred M. Fischer and Arthur Getis, 197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem. Concepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth in the Detroit Region in Economic Geography 46: 2, 234240.” Clark University, Worcester, MA.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html",
    "href": "021_Rstudio.html",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "5.1 R, RStudio and its interface\nIn class demonstration of how to",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#r-rstudio-and-its-interface",
    "href": "021_Rstudio.html#r-rstudio-and-its-interface",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "Get R and RStudio installed\nNavigating the R studio interface\nScripting area, console, files,…\nUsing colors and TOC outline in RStudio\nAuto-completion using tabs\nNavigating history with the up/down arrows\nUnderstanding help (necessary arguments and default options)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#projects-and-workflows",
    "href": "021_Rstudio.html#projects-and-workflows",
    "title": "5  Getting started with RStudio",
    "section": "5.2 Projects and workflows",
    "text": "5.2 Projects and workflows\nLet’s compute a value in the console and store it to an object first\n\n#This is a comment\n1+2 #This is also a comment\n#&gt; [1] 3\na&lt;-3+4\na\n#&gt; [1] 7\n\nSee that we now have an object in the environment!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#objects",
    "href": "021_Rstudio.html#objects",
    "title": "5  Getting started with RStudio",
    "section": "5.3 Objects",
    "text": "5.3 Objects\nAn object is not defined ex-ante and is automatically overwritten\n\nX&lt;-3 #This X will soon be replaced\nX&lt;-1:10 \nY&lt;-X^2\n\nR is case sensitive. The following returns an Error\n\nx # x is lower case and does not exist\n\nIMPORTANT:\n\nAlways worry about Errors. They stop your process.\nAlways read and try to understand Warnings. They do not stop your process but usually indicate the result may not be as expected!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#working-directory",
    "href": "021_Rstudio.html#working-directory",
    "title": "5  Getting started with RStudio",
    "section": "5.4 Working directory!!!",
    "text": "5.4 Working directory!!!\nSuppose we want to produce a text from the above and save it to a file:\n\na_sentence&lt;-paste(\"I have computed a sum, which equals\", a)\na_sentence #Let's see this in the console\n#&gt; [1] \"I have computed a sum, which equals 7\"\ncat(a_sentence,file=\"brol/a_sentence.txt\")\n\nWhere is the file? the directory? Have you been able to run this?\nAlways indicate where you work!\nThe classical way is\n\ngetwd() #get (default) working directory\nsetwd(\"/Users/geoffrey.caruso/Dropbox/GitHub/MAGEO0641/brol\") #set working directory to YOUR OWN NETWORK SPACE HERE!\n\nThere is a more practical way in RStudio: Make an .Rproj from/to a directory\nYou can create a directory in your finder/file explorer, or create a directory or subdirectory from within the R console. This will be very useful at a more advanced level when you create many outputs and directory names result from some data processing. Think of processing something across many countries/cities.\n\ndir.create(\"brol\")\ndir.create(\"Today\")\n\n#and for removing a directory\nunlink(\"Today\", recursive = TRUE) #see help: If recursive = FALSE directories are not deleted, not even empty ones.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "href": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "title": "5  Getting started with RStudio",
    "section": "5.5 Commented R scripts vs markdown documents",
    "text": "5.5 Commented R scripts vs markdown documents\nThe above is a commented script, using #, which you can save as an .R file and re-run later.\nThe problem with this approach is that you won’t see results of the codes until you run it. So you can’t really comment your output (although many, and I, would still do it) thus mixing explanation of what is done and interpretation.\nA more advanced approach is to make a document where you integrate text, code chunks and results of the code.The text can thus document what is going to be done and the results, while the code chunks can thus document both the code itself and its result as it is processed by the Console.\nLet’s have a look at the structure of this syllabus, written using Quarto markdown.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "href": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "title": "5  Getting started with RStudio",
    "section": "5.6 Good practice with files handling and objects naming",
    "text": "5.6 Good practice with files handling and objects naming\nRproj in an excellent way to keep things at the same place.\nWhen using an Rproj, use relative path only, i.e. from the root folder of the project, not from your machine. This is the way you can easily transfer your project to friends. Only external data (e.g. from the web) should be referenced in full.\nIt is also good practice to have a specific “data” folder or subfolders for all data so you clearly differentiate your outputs with the inputs. Similarly a R folder with your scripts when you have many.\nAlthough there is no naming convention agreed by everyone, it is important to apply a consistent style for yourself and colleagues. Also you would avoid spaces and rarer characters, especially in a multilingual environment.\nA folder of file named “source data” or “data_für_rémi” are not great ideas. (This is also true for variable names in data frame, see later)\nIn general, I personally like files to be all lowercase with underscores and using action verbs to explain what is done in the R file, such as\n\nestimate_model.R\nget_statec_data.R\n\nFiles numbering can be of good help for heavier projects where there is a logical sequence (time)\n\n01_estimate_model.R\n02_get_statec_data.R\n\nFor variable names and objects I tend to use the “UpperCamelCase” form especially for vectors\n\nLuxCities\nGrowthRates\n\nand tend to add an “df” or “_lst” to disambiguate where needs be between some classes\n\nLuxCities_df\nLuxCities_lst\n\nI also like to use lowercase single letters for input parameters, such as a pvalue or number of neighbours, e.g.\n\np&lt;-0.05\nk&lt;-2\n\nor Greec symbols written in full, especially when there is a theoretical link\n\nbeta &lt;-model$coefficient[1]\nrho &lt;- 0.5\n\nFor functions (see later) I also like to use action verbs and include dots, such as:\n\nplot.bmi()\nextract.boundary()\n\nIn all case, be concise but specific and consistent within a project or even across.\n\nNeighboursCompute_Europe.Paris_project3 would be very long and inconsistent\ndf_new2bis not to use both as an object or a file\n\n…but to to be honest I have quite a number of tests.r and plot.test2.jpg files peppered in my machine.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "022_vectors.html",
    "href": "022_vectors.html",
    "title": "6  Vectors",
    "section": "",
    "text": "6.1 Introducing vectors\nVectors are the basic units of information in R, and many functions apply to vectors. If you are coming from the “spreadsheet” world (MS Excel or Open Office), where the basic unit is a cell, the change in perspective is quite important: while a cell has a single value (information), a vector contains multiple values.\nVectors, being a combination of values, are created by the combine (or concatenate) function c() or obtained from external sources.\nThere are two kinds of vectors:\nThis chapter is about atomic vectors, we will introduce lists later.\nAs a first example, see below a character (atomic) vector and a numeric (atomic) vector:\nCountries&lt;-c(\"Romania\",\"Russia\",\"Morocco\",\"Iran\",\"France\")\nmode(Countries)\n#&gt; [1] \"character\"\nAges&lt;-c(20,25,22,22,49)\nmode(Ages)\n#&gt; [1] \"numeric\"\nNote that in case you would have a long list to input manually, the scan() function is a little more interactive. Try! (I also have heard there are ways to copy-paste from external sources…(if you fancy less transparent clicking approaches, be curious and find out, for example here: Chapter 31 ;-), most of the times anyway we rather read values from readable text files).",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#introducing-vectors",
    "href": "022_vectors.html#introducing-vectors",
    "title": "6  Vectors",
    "section": "",
    "text": "Atomic vectors (which we tend to refer to simply as vectors), which are homogeneous in the sense that they contain only one “type” of data, such as characters or numbers.\nLists, which can have heterogeneous contents.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#attributes-of-vectors",
    "href": "022_vectors.html#attributes-of-vectors",
    "title": "6  Vectors",
    "section": "6.2 Attributes of vectors",
    "text": "6.2 Attributes of vectors\nA vector is always characterized by its type and length.\n\n6.2.1 Types\nA vector is a combination of values from the same type (or mode). So if you combine data from different types they will be coerced to the less demanding one, i.e. “character” in the following cases:\n\nA&lt;-c(1,\"two\",3)\nA\n#&gt; [1] \"1\"   \"two\" \"3\"\nmode(A)\n#&gt; [1] \"character\"\n\nc(Countries, Ages)\n#&gt;  [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\"  \"20\"      \"25\"     \n#&gt;  [8] \"22\"      \"22\"      \"49\"\nmode(c(Countries, Ages))\n#&gt; [1] \"character\"\n\nYou can also see from these examples that mode()` gets a vector’s “type”. R vectors have one and only one “mode”, either “numeric”, “character” or “logical” (plus “raw” and “complex”, which we don’t consider here).\nYou can also use typeof() in case you are interested to know how the data is actually encrypted, which essentially differentiates the “numeric” mode into “integer” and “double”. R vectors have one and only one “typof”, either “integer”, “double”, “character” or “logical”\nCoercion to the same type applies to the “typeof” as you can see below\n\nBNum&lt;-c(2,10,99)\ntypeof(BNum)\n#&gt; [1] \"double\"\nmode(BNum)\n#&gt; [1] \"numeric\"\nBInt&lt;-c(2L,10L,99L)\ntypeof(BInt)\n#&gt; [1] \"integer\"\nmode(BInt)\n#&gt; [1] \"numeric\"\nB&lt;-c(BNum,BInt)\ntypeof(B)\n#&gt; [1] \"double\"\nmode(B)\n#&gt; [1] \"numeric\"\n\nIn practice, you will rarely use mode() or typeof(), which distinguish well between vectors, but not between most other objects. Instead, you will use the class() function in order to know what kind of data you have and what you can do with it. For most atomic vectors, class() is basically typeof(). The difference is that “class” is not a mutually exclusive property. A vector, or any object, can belong to several classes and thus be used with different functions\n\nclass(c(20L,50L,70L))\n#&gt; [1] \"integer\"\nclass(c(20,50,70))\n#&gt; [1] \"numeric\"\nclass(Countries)\n#&gt; [1] \"character\"\nclass(Ages)\n#&gt; [1] \"numeric\"\nclass(c(Countries,Ages)) #coercion\n#&gt; [1] \"character\"\n\nSometimes, depending on some calculations, you may need to transform the type of data, especially from numeric to character and vice versa. Coercion may apply automatically but not always, so you will need to explicitly transform the data type using “as.a type” or”as.a mode”: as.numeric(), as.character(), which you will use quite often, or as.integer(),as.double(), or as.logical().\n\nA&lt;-c(1,2,3)\nB&lt;-c(1L,2L,3L)\nC&lt;-c(\"1\",\"2\",\"3\")\nA+B\n#&gt; [1] 2 4 6\nA+C\n#&gt; Error in A + C: non-numeric argument to binary operator\nA+as.numeric(C)\n#&gt; [1] 2 4 6\nas.character(A)\n#&gt; [1] \"1\" \"2\" \"3\"\n\n\n\n6.2.2 Length\nWhile a numeric vector of length one is mathematically a scalar, the way you input a scalar in R is as a vector of length 1. A vector can also be empty, i.e. of length 0, in which case its type is unknown, unless you apply one of the above transformations.\n\nX&lt;-3 #equivalent to X&lt;-c(3)\nclass(X)\n#&gt; [1] \"numeric\"\n\nY&lt;-c()\nclass(Y)\n#&gt; [1] \"NULL\"\n\nY&lt;-as.numeric(c())\nclass(Y)\n#&gt; [1] \"numeric\"\n\nY&lt;-as.integer(c())\nclass(Y)\n#&gt; [1] \"integer\"\n\nYou get the length of a vector using the function length(), which you will also use quite a lot.`\n\nlength(Countries)\n#&gt; [1] 5\nlength(X)\n#&gt; [1] 1\nlength(Y)\n#&gt; [1] 0\n\nNote that if you manually change the length of an existing vector, this will trim the vector end or extend the vector with empty values.\n\nZ&lt;-c(2, 4, 6, 7 , 10)\nlength(Z)&lt;-3\nZ\n#&gt; [1] 2 4 6\nlength(Z)&lt;-12\nZ\n#&gt;  [1]  2  4  6 NA NA NA NA NA NA NA NA NA\n\nBesides their class() and length() attributes, vectors (and other objects) can be endowed with a number of other attributes, which you will obtain from attributes(). None of the vectors we used as example so far has additional attributes, but you can define any attribute yourself. See below how we add a “source” attribute to the vector of “Countries” we created earlier and assign a “character string” to it to describe the source. Retrieving a specific attribute is done with the attr(,\"Whatever attribute\") function.\n\nattributes(Countries)\n#&gt; NULL\nattr(Countries, \"source\")&lt;-\"MAGEO students input\"\nattributes(Countries)\n#&gt; $source\n#&gt; [1] \"MAGEO students input\"",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sec-factors",
    "href": "022_vectors.html#sec-factors",
    "title": "6  Vectors",
    "section": "6.3 Factors",
    "text": "6.3 Factors\nRemember from the introduction, that we (in geography and elsewhere) usually consider discrete and continuous data, each type being split into 2 measurement levels:\n\nDiscrete: nominal (blue, green) or ordinal (high, low)\nContinuous: interval (10°C, 20°C) or ratio (10 apples, 20 apples) (difference being that 20 apples is twice as much as much as 10 apples, but 20°C is not twice as hot as 10°C because zero is not the absolute zero, hence a ratio in this case makes little sense, only differences are sensical).\n\nThis vocabulary is not used directly in R:\n\nContinuous (ratio/interval) data is coded as numeric or integer\nDiscrete (categorical) data (nominal/ordinal) is preferably coded as a factor.\n\nNominal data can still be in the “character” type, but the idea of a “factor” is that there is only a limited set of characters’ strings that you will find in a vector (e.g. set of countries, set of land uses) and you are able (willing) to enumerate them. For ordinal data, the order is important, hence it gets closer to an integer set than to a character (e.g. education levels: primary, secondary tertiary education), yet you can sum integer data but should not sum ordinal data.\n\nFactors are designed to properly solve the use of discrete/categorical data. It is based on the integer type (in the sense of typeof) on top of which a “level” attribute is added, and potentially whether it is ordered or not. They are fabricated with the as.factor() or factor() functions.\n\nCountries\n#&gt; [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\" \n#&gt; attr(,\"source\")\n#&gt; [1] \"MAGEO students input\"\ntypeof(Countries)\n#&gt; [1] \"character\"\n\nCountries_f&lt;-as.factor(Countries)\nCountries_f\n#&gt; [1] Romania Russia  Morocco Iran    France \n#&gt; Levels: France Iran Morocco Romania Russia\ntypeof(Countries_f)\n#&gt; [1] \"integer\"\nclass(Countries_f)\n#&gt; [1] \"factor\"\n\nWhile the categories are displayed with the factor, the levels() function returns those categories as a character vector.\n\nlevels(Countries_f)\n#&gt; [1] \"France\"  \"Iran\"    \"Morocco\" \"Romania\" \"Russia\"\n\nBy default the levels in a factor uses the alphabetical order. In many cases however, you will want to at least define the first one, in order to use it as a reference (typically in regression analysis with a categorical explanatory variable) or choose you own order for plotting or other purposes.\n\n6.3.1 Re-defining the reference level\nSuppose you want to use “Morocco” as the first, reference level instead of “France”.\nThe relevel() function re-orders the levels so that the one indicated as “ref” is used first and the others are moved down the series.\n\nCountries_f2&lt;-relevel(Countries_f, ref=\"Morocco\")\n\nYou could also set the order directly at the time of creating the factor using the “level” argument, using the factor() function, not as.factor(). We used as.factor() before because it is generally quicker, and there is not always a need to adapt the order of levels.\n\nCountries_f3&lt;-factor(Countries, level=c(\"Morocco\",\"France\",\"Romania\", \"Iran\",\"Belgium\"))\n\nBe careful, however, because if you don’t use the complete list of possibilities, the values that are not specified in the levels vector, will simply be ignored and turned into NA’s. In the above example we forgot “Russia” and therefore have now a NA within our vector. Conversely, we have been able to indicate a “Belgium” level, although it was not present. There is no automatic correspondence between the levels of a factor and its set of values. If you want a match, you can drop unused levels using droplevels() but for those characters (e.g. Russia) that were not taken at the moment of creating the factor, it is too late, they remain a NA.\n\nCountries_f4&lt;-droplevels(Countries_f3)\nCountries_f4\n#&gt; [1] Romania &lt;NA&gt;    Morocco Iran    France \n#&gt; Levels: Morocco France Romania Iran\n\nSee how the different factors we made so far change the order of the levels and the data when a case is made absent from the list:\nTo compare one to one, we use to column-binding function cbind() (compare with c()). You also see here that the values of the factors are integers, not characters as for the first vector:\n\ncbind(Countries,Countries_f,Countries_f2,Countries_f3, Countries_f4)\n#&gt;      Countries Countries_f Countries_f2 Countries_f3 Countries_f4\n#&gt; [1,] \"Romania\" \"4\"         \"4\"          \"3\"          \"3\"         \n#&gt; [2,] \"Russia\"  \"5\"         \"5\"          NA           NA          \n#&gt; [3,] \"Morocco\" \"3\"         \"1\"          \"1\"          \"1\"         \n#&gt; [4,] \"Iran\"    \"2\"         \"3\"          \"4\"          \"4\"         \n#&gt; [5,] \"France\"  \"1\"         \"2\"          \"2\"          \"2\"\n\n\n\n6.3.2 Ordering a factor based on occurrence:\nTo make a more realistic case, we have scraped the Wikipedia table indicating the Tour de France winners since 1903. See “TourDeFrance” in the data folder for the data and scraping script (from R). The data is in the form of a data.frame and saved as a RDS (an effective way to save R objects onto your disc). See the relevant chapters for data.frame and saving to file later.\nThere was no Tour de France during World War I and II and no winner from 1999 to 2005 because Lance Armstrong cheated. Hence we have a series of ” - ” in our levels, which are not interesting, and thus not considered when we factor our vector. This doesn’t remove the vector elements but creates several NA’s.\n\nLeTour_df&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nWinners&lt;-factor(LeTour_df$Country, exclude = \"—\")\nWinners\n#&gt;   [1] France        France        France        France        France       \n#&gt;   [6] France        Luxembourg    France        France        Belgium      \n#&gt;  [11] Belgium       Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [16] &lt;NA&gt;          Belgium       Belgium       Belgium       Belgium      \n#&gt;  [21] France        Italy         Italy         Belgium       Luxembourg   \n#&gt;  [26] Luxembourg    Belgium       France        France        France       \n#&gt;  [31] France        France        Belgium       Belgium       France       \n#&gt;  [36] Italy         Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [41] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          France       \n#&gt;  [46] Italy         Italy         Switzerland   Switzerland   Italy        \n#&gt;  [51] France        France        France        France        France       \n#&gt;  [56] Luxembourg    Spain         Italy         France        France       \n#&gt;  [61] France        France        Italy         France        France       \n#&gt;  [66] Netherlands   Belgium       Belgium       Belgium       Belgium      \n#&gt;  [71] Spain         Belgium       France        Belgium       France       \n#&gt;  [76] France        France        Netherlands   France        France       \n#&gt;  [81] France        France        France        United States Ireland      \n#&gt;  [86] Spain         United States United States Spain         Spain        \n#&gt;  [91] Spain         Spain         Spain         Denmark       Germany      \n#&gt;  [96] Italy         &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt; [101] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          Spain         Spain        \n#&gt; [106] Spain         Spain         Luxembourg    Australia     Great Britain\n#&gt; [111] Great Britain Italy         Great Britain Great Britain Great Britain\n#&gt; [116] Great Britain Colombia      Slovenia      Slovenia      Denmark      \n#&gt; [121] Denmark       Slovenia     \n#&gt; 15 Levels: Australia Belgium Colombia Denmark France Germany ... United States\n\nlength(Winners) #this includes NA's!\n#&gt; [1] 122\nlevels(Winners)\n#&gt;  [1] \"Australia\"     \"Belgium\"       \"Colombia\"      \"Denmark\"      \n#&gt;  [5] \"France\"        \"Germany\"       \"Great Britain\" \"Ireland\"      \n#&gt;  [9] \"Italy\"         \"Luxembourg\"    \"Netherlands\"   \"Slovenia\"     \n#&gt; [13] \"Spain\"         \"Switzerland\"   \"United States\"\n\nSuppose you want to to know and plot how many times each of the 15 countries (who won Le Tour at least once) won.\nFrequencies can be observed directly from a basic plot:\n\nplot(Winners,las=2)\n\n\n\n\n\n\n\n\nGet the victories counts in a vector can be made with table(), which applies to anything (numeric or character) that can be coerced to a factor\n\ntable(Winners)\n#&gt; Winners\n#&gt;     Australia       Belgium      Colombia       Denmark        France \n#&gt;             1            18             1             3            36 \n#&gt;       Germany Great Britain       Ireland         Italy    Luxembourg \n#&gt;             1             6             1            10             5 \n#&gt;   Netherlands      Slovenia         Spain   Switzerland United States \n#&gt;             2             3            12             2             3\n\nBoth the plot and counts however follow the order of the levels, which is alphabetical\nOne possibility to improve the graph is to change the order of the levels based on the count table and redoing the graph\n\n#Vector of levels in a new order\nLevelsFrq&lt;-levels(Winners)[order(table(Winners), decreasing = TRUE)]\n  #note the order function and the square brackets\nLevelsFrq\n#&gt;  [1] \"France\"        \"Belgium\"       \"Spain\"         \"Italy\"        \n#&gt;  [5] \"Great Britain\" \"Luxembourg\"    \"Denmark\"       \"Slovenia\"     \n#&gt;  [9] \"United States\" \"Netherlands\"   \"Switzerland\"   \"Australia\"    \n#&gt; [13] \"Colombia\"      \"Germany\"       \"Ireland\"\n\n#Use that order when creating the factor from the character vector\nWinnersFrq&lt;-factor(Winners, levels = LevelsFrq) \n\n#plot\nplot(WinnersFrq,las=2)\n\n\n\n\n\n\n\n\n\n\n6.3.3 Ordered factors\nSometimes you will want to make a categorical data explicitly ordinal for graphical or statistical purpose . For example if you have used a Likert scale within a survey, it can be interesting it is stored as an ordered factor. You can order an already existing factor, using the function ordered(), which adds a logical flag to the factor to indicate it is order (see first example), or at the moment you create the factor using factor() (second example). Once the factor is ordered, the levels are displayed in order and separated by a ” &lt; “. Also a new class,”ordered”, is added to the object.\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") #in the expected order\nResponses&lt;-c(\"Neither agree nor disagree\", \"Disagree\", \"Disagree\", \"Agree\", \"Agree\", \"Neither agree nor disagree\", \"Strongly agree\", \"Strongly Agree\", \"Disagree\")\nResponses_f&lt;-factor(Responses, levels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree Disagree Neither agree nor disagree ... Strongly agree\n\nResponses_f2&lt;-ordered(Responses_f)\nResponses_f2 #Notice that if we don't provide again the levels, it is made from the existing ones, so there is only 4 levels now (\"Strongly Disagree\" not being answered)\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; Levels: Disagree &lt; Neither agree nor disagree &lt; Agree &lt; Strongly agree\n\nResponses_f3&lt;-factor(Responses, levels=LikertScale, ordered = TRUE)\nResponses_f3\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nclass(Responses_f3) #see the additional class\n#&gt; [1] \"ordered\" \"factor\"\n\nIn the above example you will have noticed a NA, due to some misspelling. This is a good case to remind that categorical data should not necessarily rely on character strings. In geography we often need to treat countries, regions, municipalities whose names can be very complicated, especially in multilingual context. It is good advise to rather use codes for geographical units (e.g. BE351, BE352, BE353, for the NUTS 3 classification of Belgium), land use (e.g. https://land.copernicus.eu/content/corine-land-cover-nomenclature-guidelines/html) and relate them to a corresponding table with proper labels.\nIt is also true for other types of categories, including Likert scale. An option is to use integer values (or characters for geographical codes) together with a series of labels. You add the labels at the moment of creating the factor:\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") \nLikertLevels&lt;-c(1L,2L,3L,4L,5L) #would work as well with numeric c(1,2,3,4,5)\nResponses&lt;-c(3L, 2L, 2L, 4L, 4L, 3L, 5L, 5L, 2L)\n\nResponses_f&lt;-factor(Responses, ordered=TRUE, levels=LikertLevels, labels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             Strongly agree            \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nThe result is the same as previously, but you avoid typos when inputing the responses and can also adapt the labels at will, without changing the data itself.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#operations-on-vectors",
    "href": "022_vectors.html#operations-on-vectors",
    "title": "6  Vectors",
    "section": "6.4 Operations on vectors",
    "text": "6.4 Operations on vectors\n\n6.4.1 Arithmetic operations and recycling\nR is a calculator and perform the arithmetic operations + - * / ^\nTwo numeric vectors of the same length can be added, multiplied, etc. When vectors of different length are provided, the shorter one is recycled until the end of the longer vector. Usually a vector of length 1 is recycled, thus allowing R to add, divide,etc. a scalar to a vector in the same way as between two vectors (in a “parallel way”). But any shorter vector is also recycled (you get a warning though). See:\n\nX&lt;-c(0,1,2,3,4,5)\nX^2\n#&gt; [1]  0  1  4  9 16 25\nY&lt;-c(10,9,8,7,6,5)\n\nX+Y\n#&gt; [1] 10 10 10 10 10 10\nY/X\n#&gt; [1]      Inf 9.000000 4.000000 2.333333 1.500000 1.000000\nX*c(0,1)\n#&gt; [1] 0 1 0 3 0 5\n\nYou can also apply a series of common mathematical functions to any numeric vector.\n\na&lt;-c(-2,-1,0,1,2,6,10)\nabs(a) #absolute\n#&gt; [1]  2  1  0  1  2  6 10\n\na^(-1) #inverse\n#&gt; [1] -0.5000000 -1.0000000        Inf  1.0000000  0.5000000  0.1666667  0.1000000\n\nlog(a) #ln or Natural logarithm (or Napierian logarithm)\n#&gt; Warning in log(a): NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.6931472 1.7917595 2.3025851\nlog10(a) # base 10 logarithm\n#&gt; Warning: NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.3010300 0.7781513 1.0000000\n\nexp(a) #base e exponential\n#&gt; [1] 1.353353e-01 3.678794e-01 1.000000e+00 2.718282e+00 7.389056e+00\n#&gt; [6] 4.034288e+02 2.202647e+04\n10^a #base 10 exponential\n#&gt; [1] 1e-02 1e-01 1e+00 1e+01 1e+02 1e+06 1e+10\n\nsqrt(a) #square root\n#&gt; Warning in sqrt(a): NaNs produced\n#&gt; [1]      NaN      NaN 0.000000 1.000000 1.414214 2.449490 3.162278\n\nSee Crawley (2012) p.11 for more examples:\n\n\n\nCrawley Table 2.1\n\n\nBe careful that these functions must lead to proper results. As you can see from the warnings. “NaN” stands for “Not a Number” and is considered as NA (“not available”). Which is not the case of infinity, which is a numeric\n\na&lt;-c(-2,-1,0,1,2,6,10)\nis.na(NaN)\n#&gt; [1] TRUE\nis.na(-Inf)\n#&gt; [1] FALSE\nis.finite(log10(a))\n#&gt; Warning: NaNs produced\n#&gt; [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\n6.4.2 Euclidean division and rounding\nThe modulo operation for integers is obtained with %/% to get the integral part of the Euclidean division and %% to get the remainder. This the way you will know an integer is odd or even. The function can also be applied to numeric vectors with decimals.\n\nc(1,3,5,44,444,4444)%/%2\n#&gt; [1]    0    1    2   22  222 2222\nc(1,3,5,44,444,4444)%%2 #remainder of dividing by 2, thus indicating odd/even\n#&gt; [1] 1 1 1 0 0 0\n99.99%/%2\n#&gt; [1] 49\n99.99%%2\n#&gt; [1] 1.99\n\nFor ratio and interval numbers with decimals you sometimes need to get rid of the decimals or display only a part of them Examine the following:\n\nx&lt;-c(33.33, 666.166, 50.5, 49.5)\ntrunc(x) #compares with as.integer() but does not change the class to integer\n#&gt; [1]  33 666  50  49\nround(x) #note the rounding of a 5 to the even digit (international standard) \n#&gt; [1]  33 666  50  50\nround(x, digits=2)\n#&gt; [1]  33.33 666.17  50.50  49.50\nceiling(x)\n#&gt; [1]  34 667  51  50\nfloor(x)\n#&gt; [1]  33 666  50  49\n\nsignif(x, digits = 5)\n#&gt; [1]  33.33 666.17  50.50  49.50\n\nWith geographical data, ceiling() can for example be applied to latitudes and longitudes in order to make grids. A simple example is how to obtain (theoretical) time zones from a set of longitudes.\nLet’s take the opportunity to learn about set.seed() and about uniform random number generation runif():\n\nset.seed(101)\nLongitudes&lt;-runif(n=10,min=-180, max=180)\nLongitudes\n#&gt;  [1]  -46.00858 -164.22307   75.48625   56.76854  -90.05194  -71.98026\n#&gt;  [7]   30.55199  -59.95183   43.92431   16.49828\n\nceiling(Longitudes*24/360)\n#&gt;  [1]  -3 -10   6   4  -6  -4   3  -3   3   2\n\n\n\n\nWikipedia, Time Zones\n\n\n\n\n6.4.3 Logical and Boolean operations\nA series of operations return a logical vector, i.e. a vector of Boolean values TRUE and FALSE.\nThose operations are &gt;, &gt;=, &lt;, &lt;=, ==, !=\nSome examples with numeric and character vectors:\nIMPORTANT: Use == for comparison, not =, which is an assignment!\n\nc(1,2,3) &lt; c(2,3,3)\n#&gt; [1]  TRUE  TRUE FALSE\nc(1,2,3) &gt;= 3 #see right hand side is recycled and applied \"one to one\"\n#&gt; [1] FALSE FALSE  TRUE\n\"Bernadette, elle est très chouette\" == \"Bernadette, elle est très chouette\"\n#&gt; [1] TRUE\n\"Bernadette, elle est très chouette\" != \"Mais sa cousine, elle est divine\"\n#&gt; [1] TRUE\n\"Julian\" &gt;= \"Julien\" #alphabetical order\n#&gt; [1] FALSE\n\nBy the way we see here vectors of the logical class:\n\nclass(\"Julian\" &gt;= \"Julien\")\n#&gt; [1] \"logical\"\n\nIn addition to those comparisons, many functions, especially structured as “is.xxx” return a TRUE or FALSE and are very useful within you data management process. You may make quite some use also of the %in% function, when you have a long vector, to check whether a particular value (or several) is present within a vector.\n\nis.numeric(\"a\")\n#&gt; [1] FALSE\nis.numeric(\"2\")\n#&gt; [1] FALSE\nis.numeric(3L)\n#&gt; [1] TRUE\nis.numeric(FALSE)\n#&gt; [1] FALSE\n!is.numeric(FALSE) #Note the negation here!\n#&gt; [1] TRUE\n\n\"urban\" %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1] TRUE\nc(\"urban\",\"industry\") %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1]  TRUE FALSE\n\nBoolean values are usually (always?) coerced to a 1 and 0, in case an arithmetic operation is then demanded.\n\nmean(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 0.75\nsum(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 3\n3*(c(TRUE,FALSE)+TRUE)\n#&gt; [1] 6 3\n\nA set of functions then apply specifically to the Boolean values TRUE or FALSE, i.e. to logical vectors.\nThese logical operators are: !, &, |, xor and typically used to select elements that match 2 or more conditions.\nThe graphic below, reproduced from R for Data Science (highly recommended!), demonstrate their outcome for 2 sets and an example is provided after creating 2 logical vectors, x and y from the LETTERS character vector.\n\n\n\nsource: Fig.12.1 from https://r4ds.hadley.nz/logicals\n\n\n\nLETTERS[1:6]\n#&gt; [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\nx&lt;-LETTERS[1:6]&lt;\"E\"\ny&lt;-LETTERS[1:6]&gt;\"B\"\nx\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\ny\n#&gt; [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx & !y\n#&gt; [1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n!x & y\n#&gt; [1] FALSE FALSE FALSE FALSE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx | y\n#&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE\nxor(x,y)\n#&gt; [1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#vectors-functions",
    "href": "022_vectors.html#vectors-functions",
    "title": "6  Vectors",
    "section": "6.5 Vectors’ functions",
    "text": "6.5 Vectors’ functions\nA number of functions are evaluated over an entire vector (vectorisation is there to avoid loops) and used to describe and understand the distribution of values. They apply mostly to numeric vectors but also to logicals (being 0 or 1 anyway) as well as characters, where possible (based on alphabetical order).\n\n6.5.1 Range, cumulative values, positions and sorting\n\nExamples of functions evaluated over an entire vector\n\n\nmin(x)\ncummin(x)\nwhich.min(x)\nsort(x)\n\n\nmax(x)\ncummax(x)\nwhich.max(x)\norder(x)\n\n\nrange(x)\n\n\nrank(x)\n\n\nsum(x)\ncumsum(x)\n\n\n\n\n\nLet’s explore some of those for x being a numeric, a logical and character. In class exploration.\nIt is sometimes difficult to remember differences between sort(x),order(x) and rank(x)\nSee two examples below\n\nset.seed(101)\nx&lt;-round(runif(10,100,200))\nx\n#&gt;  [1] 137 104 171 166 125 130 158 133 162 155\n\ncbind(Original=x,Sorted=sort(x),Rank=rank(x),Order=order(x))\n#&gt;       Original Sorted Rank Order\n#&gt;  [1,]      137    104    5     2\n#&gt;  [2,]      104    125    1     5\n#&gt;  [3,]      171    130   10     6\n#&gt;  [4,]      166    133    9     8\n#&gt;  [5,]      125    137    2     1\n#&gt;  [6,]      130    155    3    10\n#&gt;  [7,]      158    158    7     7\n#&gt;  [8,]      133    162    4     9\n#&gt;  [9,]      162    166    8     4\n#&gt; [10,]      155    171    6     3\n\n\nM&lt;-month.name[1:6]\nM\n#&gt; [1] \"January\"  \"February\" \"March\"    \"April\"    \"May\"      \"June\"\n\ncbind(Months=M,Sorted=sort(M),Rank=rank(M),Order=order(M))\n#&gt;      Months     Sorted     Rank Order\n#&gt; [1,] \"January\"  \"April\"    \"3\"  \"4\"  \n#&gt; [2,] \"February\" \"February\" \"2\"  \"2\"  \n#&gt; [3,] \"March\"    \"January\"  \"5\"  \"1\"  \n#&gt; [4,] \"April\"    \"June\"     \"1\"  \"6\"  \n#&gt; [5,] \"May\"      \"March\"    \"6\"  \"3\"  \n#&gt; [6,] \"June\"     \"May\"      \"4\"  \"5\"\n\nTo those operations, we should add all univariate statistics such as mean(), median(), var(), quantile(), but we leave them aside for now as they are introduced later with univariate statistics and distributions.\nSpecific to logical vectors, the any() and all() functions are particularly useful in the case you check a very long vector with only very few having a TRUE or a FALSE.\nSee basic examples:\n\nany(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] TRUE\nany(c(TRUE,TRUE,TRUE,TRUE))\n#&gt; [1] TRUE\nall(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] FALSE\n\nAnd an example where we suppose a random set of values from a normal distribution and we want to check manually whether there is an upper outlier. Typically (as in boxplots) this outlier is calculated as being any value above the 3rd quartile plus 1.5 times the interquartile range.\n\nset.seed(102)\nx&lt;-rnorm(100)\nq75&lt;-quantile(x, p=0.75)\niqr&lt;-IQR(x)\nany(x&gt;(q75+1.5*iqr))\n#&gt; [1] TRUE\nboxplot(x) #\n\n\n\n\n\n\n\nx[x&gt;(q75+1.5*iqr)] #to identify them\n#&gt; [1] 3.114333\n\nset.seed(101) #Check with this seed!\n\n\n\n6.5.2 Summary\nOne of the most used function for analysis (if not THE most used) is summary(). We will see its result may change substantially based on the object class it is applied to. Its basic functioning for a simple numeric vector and character vector is:\n\nsummary(c(1,2,3,4,NA,6))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;     1.0     2.0     3.0     3.2     4.0     6.0       1\nsummary(c(\"A\",\"B\",\"C\",NA,NA,\"F\"))\n#&gt;    Length     Class      Mode \n#&gt;         6 character character\n\n\n\n6.5.3 Element-wise functions\nOther functions apply to two or more vectors. They need an x and a y vector as input, e.g. cor(x,y). Most of those you will encounter use two vectors of the same length and type, i.e. no recycling, which leads us to the notion of a data frame (see next chapter).\nAt this time, we show only two simple “parallel” or “element-wise” computations: pmin() and pmax(), which can be useful for example when there is a repeated measure for a given set of individuals. Cases are not rare when you make a new vector (or column in a data frame) based on such element-wise calculations. (Yet you would probably assemble the data into a data frame first and then make a row-wise calculation).\n\nt1&lt;-c(25,35,45,55)\nt2&lt;-c(26,36,44,54)\nt3&lt;-c(27,34,43,56)\n\npmin(t1,t2,t3)\n#&gt; [1] 25 34 43 54\npmax(t1,t2,t3)\n#&gt; [1] 27 36 45 56",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#indexing-and-subsetting",
    "href": "022_vectors.html#indexing-and-subsetting",
    "title": "6  Vectors",
    "section": "6.6 Indexing and subsetting",
    "text": "6.6 Indexing and subsetting\nNow you have seen that vectors are the key elements in R, you will still want to access its elements individually or parts of it based on positions or conditions\nSquare brackets I used to get into vector elements by their position (or by their name if named).\nYou can supply one position but also several positions and ranges. Examine the following:\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx #all\n#&gt;  [1] 3 1 9 7 8 2 3 6 3 4\nx[4] #4th element\n#&gt; [1] 7\nx[6:8] #a sequence\n#&gt; [1] 2 3 6\nx[c(4,6:8)] #both\n#&gt; [1] 7 2 3 6\nx[-4] #all but the 4th element\n#&gt; [1] 3 1 9 8 2 3 6 3 4\nx[-length(x)] #all but the last one\n#&gt; [1] 3 1 9 7 8 2 3 6 3\nx[-((length(x)-1):length(x))]  #all but the last two\n#&gt; [1] 3 1 9 7 8 2 3 6\n\nThis is very flexible because you can use any vector representing an index (position) to make a new vector.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nchicken&lt;-c(10,10,10,4,2,3)\nx[chicken]\n#&gt; [1] 4 4 4 7 1 9\n\nSuch a vector can be a logical, then the elements where the logical is TRUE are returned.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx&gt;3\n#&gt;  [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\nx[x&gt;3]\n#&gt; [1] 9 7 8 6 4\n\nx %% 2 == 0\n#&gt;  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\nx[x %% 2 == 0] #even numbers only\n#&gt; [1] 8 2 6 4\n\nx[x&gt;mean(x)]\n#&gt; [1] 9 7 8 6\n\nx[as.logical(c(0,1))] #?? recyling is in effect here\n#&gt; [1] 1 7 2 6 4\n\nPossibilities are endless, especially if you think the indexing vector can come from another vector than x.\nVectors are sometimes named, and names the used to retrieve the data:\n\ny&lt;-c(1000,1500,900,1200)\nnames(y)&lt;-c(\"UK\",\"BE\",\"LU\",\"FR\")\n\ny[\"LU\"]\n#&gt;  LU \n#&gt; 900\n\nLast but not least, once selected an element can then be assigned a new value:\n\nz&lt;-c(0,0,0,4,2,3,0)\nz\n#&gt; [1] 0 0 0 4 2 3 0\n\nz[2]&lt;-5\nz\n#&gt; [1] 0 5 0 4 2 3 0\n\nz[z==0]&lt;-NA  #a very much used use case\nz\n#&gt; [1] NA  5 NA  4  2  3 NA",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sequences-and-repetitions",
    "href": "022_vectors.html#sequences-and-repetitions",
    "title": "6  Vectors",
    "section": "6.7 Sequences and repetitions",
    "text": "6.7 Sequences and repetitions\nThere are many instances where you will need to create a repeated set of values or sequence of values. The functions seq() and rep() are used extensively.\nImagine as a first example you have 5 regions named 2,4,6,8,10 and each of them send a flow of cars to each other, thus building a 5 x 5 matrix =25 records of flows.\n\nregions&lt;-seq(from=2, to=10, by=2)\nregions\n#&gt; [1]  2  4  6  8 10\norigins&lt;-rep(regions, 5)\norigins\n#&gt;  [1]  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10\ndestinations&lt;-rep(regions, each=5)\ndestinations\n#&gt;  [1]  2  2  2  2  2  4  4  4  4  4  6  6  6  6  6  8  8  8  8  8 10 10 10 10 10\nflows&lt;-paste(\"from \",origins, \" to \", destinations)\nflows\n#&gt;  [1] \"from  2  to  2\"   \"from  4  to  2\"   \"from  6  to  2\"   \"from  8  to  2\"  \n#&gt;  [5] \"from  10  to  2\"  \"from  2  to  4\"   \"from  4  to  4\"   \"from  6  to  4\"  \n#&gt;  [9] \"from  8  to  4\"   \"from  10  to  4\"  \"from  2  to  6\"   \"from  4  to  6\"  \n#&gt; [13] \"from  6  to  6\"   \"from  8  to  6\"   \"from  10  to  6\"  \"from  2  to  8\"  \n#&gt; [17] \"from  4  to  8\"   \"from  6  to  8\"   \"from  8  to  8\"   \"from  10  to  8\" \n#&gt; [21] \"from  2  to  10\"  \"from  4  to  10\"  \"from  6  to  10\"  \"from  8  to  10\" \n#&gt; [25] \"from  10  to  10\"\n\nIf you like to give a unique number to identify each flow with a number starting at 100 and don’t calculate that you have 25 of them, seq_along`is your tool:\n\nseq_along(flows)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\ncbind(seq_along(flows),flows)\n#&gt;            flows             \n#&gt;  [1,] \"1\"  \"from  2  to  2\"  \n#&gt;  [2,] \"2\"  \"from  4  to  2\"  \n#&gt;  [3,] \"3\"  \"from  6  to  2\"  \n#&gt;  [4,] \"4\"  \"from  8  to  2\"  \n#&gt;  [5,] \"5\"  \"from  10  to  2\" \n#&gt;  [6,] \"6\"  \"from  2  to  4\"  \n#&gt;  [7,] \"7\"  \"from  4  to  4\"  \n#&gt;  [8,] \"8\"  \"from  6  to  4\"  \n#&gt;  [9,] \"9\"  \"from  8  to  4\"  \n#&gt; [10,] \"10\" \"from  10  to  4\" \n#&gt; [11,] \"11\" \"from  2  to  6\"  \n#&gt; [12,] \"12\" \"from  4  to  6\"  \n#&gt; [13,] \"13\" \"from  6  to  6\"  \n#&gt; [14,] \"14\" \"from  8  to  6\"  \n#&gt; [15,] \"15\" \"from  10  to  6\" \n#&gt; [16,] \"16\" \"from  2  to  8\"  \n#&gt; [17,] \"17\" \"from  4  to  8\"  \n#&gt; [18,] \"18\" \"from  6  to  8\"  \n#&gt; [19,] \"19\" \"from  8  to  8\"  \n#&gt; [20,] \"20\" \"from  10  to  8\" \n#&gt; [21,] \"21\" \"from  2  to  10\" \n#&gt; [22,] \"22\" \"from  4  to  10\" \n#&gt; [23,] \"23\" \"from  6  to  10\" \n#&gt; [24,] \"24\" \"from  8  to  10\" \n#&gt; [25,] \"25\" \"from  10  to  10\"\n\nWith seq you can also partition a range of values into a given number of intervals without knowing what the value of each interval is. Suppose you have 14 teams to provide water to Marathonians. At which distance are you going to place those teams along the path, knowing you need one at the end and one at the start?\n\nStands&lt;-seq(0,42195,length=14)\n\nAnd suppose that at every third stand you provide some snacks, not just water. So you repeat the pattern “water,water,food” until the end of the vector of Stands\n\nSnacks&lt;-rep(c(FALSE,FALSE,TRUE), length.out = length(Stands))\nSnacks\n#&gt;  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n#&gt; [13] FALSE FALSE\n\nYou also use sequences to display some functions you like to understand better, such as a polynomial or quadratic.\n\nx&lt;-seq(from=1, to=500, length = 100) #m from school\ny&lt;-1000 - 0.001*x^2 + 0.3*x #house rental value\nplot(x=x,y=y)\n\n\n\n\n\n\n\n\n\n#although you could use curve() here as well:\ncurve(1000 - 0.001*x^2 + 0.3*x, from=1, to=500)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html",
    "href": "023_df_lst.html",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1 Data frames",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#data-frames",
    "href": "023_df_lst.html#data-frames",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1.1 A Data frame … your beloved spreadsheet\n\ndf&lt;-data.frame() #an empty one\n\ndf&lt;-data.frame(a = 1:5,\n           b = letters[1:5],\n           c = rnorm(n = 5))\ndf\n#&gt;   a b          c\n#&gt; 1 1 a -0.2564503\n#&gt; 2 2 b -0.3925500\n#&gt; 3 3 c  0.6861389\n#&gt; 4 4 d  0.8513324\n#&gt; 5 5 e -0.7379774\n\nSee how it is summarized. Basically summarizing each vector in columns.\n\nsummary(df)\n#&gt;        a          b                   c          \n#&gt;  Min.   :1   Length:5           Min.   :-0.7380  \n#&gt;  1st Qu.:2   Class :character   1st Qu.:-0.3926  \n#&gt;  Median :3   Mode  :character   Median :-0.2565  \n#&gt;  Mean   :3                      Mean   : 0.0301  \n#&gt;  3rd Qu.:4                      3rd Qu.: 0.6861  \n#&gt;  Max.   :5                      Max.   : 0.8513\n\n\n\n7.1.2 Accessing and subsetting\n!!! THIS IS EXTREMELY IMPORTANT !!! !!! Don’t forget the comma\nIdentifying\n\ndf[,\"b\"] #all rows but only the column named \"b\"\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\ndf$b #same !\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nSubsetting based on position\n\ndf$c[1:2] #subsetting a vector: only first 2 values\n#&gt; [1] -0.2564503 -0.3925500\ndf[1:2,\"b\"]\n#&gt; [1] \"a\" \"b\"\ndf[1:2,] #first 2 records, all variables. Don't forget the comma !\n#&gt;   a b          c\n#&gt; 1 1 a -0.2564503\n#&gt; 2 2 b -0.3925500\n\nSubsetting specific rows (not range as above):\n\ndf[1:3,\"b\"]\n#&gt; [1] \"a\" \"b\" \"c\"\ndf[c(1,3),\"b\"]\n#&gt; [1] \"a\" \"c\"\n\nEach dataframe column must have the same number of elements.\n(Enjoy this condition after thinking of how many times you had misaligned and columns of different lengths in Ms Exc..)\n\nz&lt;-c(\"Mom\",\"Dad\",3) #remember each vector as only one type of data, so this is coerced to character\n\ndf$z&lt;-z #Should not work becouse of different length\n#&gt; Error in `$&lt;-.data.frame`(`*tmp*`, z, value = c(\"Mom\", \"Dad\", \"3\")): replacement has 3 rows, data has 5\n\n\nlength(df$a)\n#&gt; [1] 5\nlength(z)\n#&gt; [1] 3\n\n#Beware. Length applied to the whole df means the number of columns!\nlength(df) #i.e. a b and c\n#&gt; [1] 3\n\nWhile length is general, for a data frame you can rather compute number of rows and columns this way\n\nnrow(df)\n#&gt; [1] 5\nncol(df)\n#&gt; [1] 3\ndim(df)\n#&gt; [1] 5 3\nnrow(df)==dim(df)[2] #What do you think?\n#&gt; [1] FALSE\n\n#But these won't work for a vector:\ndim(df$a)\n#&gt; NULL\nnrow(df$a)\n#&gt; NULL\n\nOften times data frames have more complicated column names. It is useful to access those directly.\nThis is also the way you change the name of a column:\n\nnames(df)[2]&lt;-\"blabla\"\ndf\n#&gt;   a blabla          c\n#&gt; 1 1      a -0.2564503\n#&gt; 2 2      b -0.3925500\n#&gt; 3 3      c  0.6861389\n#&gt; 4 4      d  0.8513324\n#&gt; 5 5      e -0.7379774",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#lists",
    "href": "023_df_lst.html#lists",
    "title": "7  Data frames and lists",
    "section": "7.2 Lists",
    "text": "7.2 Lists\nA List is a more general object than a dataframe.\nAll “columns” are not necessarily\n\nof the same length\nnor of the same class\n\n\nmylist&lt;-list(a = 1:5,\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3 4 5\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1]  0.625490436 -0.356154445 -0.648515418 -0.006052866  0.052289969\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 5      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nmylist&lt;-list(a = 1:3,#we removed 2 elements here\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -0.4304375  1.0036636  0.8927526  0.5275093 -1.3772948\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 3      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nlength(mylist) #number of elements in list\n#&gt; [1] 3\nlength(mylist$b) #number of objects within that element of the list\n#&gt; [1] 5\n\n\n7.2.1 Into the lists: [[ ]] vs [ ]\n\nmylist[2] # getting the list element displayed\n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[2])  # you see it is a list element\n#&gt; [1] \"list\"\nmylist[[2]] # getting the corresponding vector\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[[2]]) # you now see it is a character vector\n#&gt; [1] \"character\"\n\nAnd now subsetting:\n\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -0.4304375  1.0036636  0.8927526  0.5275093 -1.3772948\nmylist[[2]][1] #1st element of the 2nd element of the list\n#&gt; [1] \"a\"\nmylist[[1]][2] #2nd element of the 1st element of the list\n#&gt; [1] 2\nM&lt;-mylist[[3]]\nM[1] #Subsetting just as any vector\n#&gt; [1] -0.4304375",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html",
    "href": "024_df_functions.html",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "8.1 What is in a function? BMI example\npaste.heightweight&lt;-function(h,w){\n  print(paste(h,w))\n  }\npaste.heightweight(1.8,80) #you provide the 2 arguments and get the output\n#&gt; [1] \"1.8 80\"\nNow let’s do the computation with the BMI calculation with a new function\nbmi.calc&lt;-function(h,w){w/h^2}\nwhich we apply\nbmi.calc(1.8,80)\n#&gt; [1] 24.69136\nA function can take a sequence of processes (e.g compute, rounds, concatenate a sentence,…) and then returns the result of the last process.\nExample\nbmi.calc.text&lt;-function(h,w){\n  b&lt;-w/h^2\n  brounded&lt;-round(b)\n  paste(\"My BMI is\", brounded, \"kg/m2\")\n}\nbmi.calc.text(1.8,80)\n#&gt; [1] \"My BMI is 25 kg/m2\"\nFor clarity the outcome of the function can be put in a return()\nbmi.calc&lt;-function(h,w){\n  return(round(w/h^2))\n}\nbmi.calc(1.8,80)\n#&gt; [1] 25",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "href": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "Let’s create a BMI function\nFirst a simple function that simply prints a given height and weight",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "href": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "title": "8  Working with data frames and functions",
    "section": "8.2 Applying a function to a data frame column",
    "text": "8.2 Applying a function to a data frame column\nLet’s create a 2nd function to transfors degrees from Celsius to Fahrenheit\nSimpler with a single argument (x):\n\ncelsius2fahrenheit&lt;-function(x){round(32+(x*9/5))}\n\ncelsius2fahrenheit(25) #25 celsius degree is thus \n#&gt; [1] 77\n\nWhich we now apply to a series of values stored in a column within a data frame\n\nmytable&lt;-data.frame(A=c(21,22,23,24,25,26,27))\nmytable$F&lt;-celsius2fahrenheit(mytable[,\"A\"])\nmytable\n#&gt;    A  F\n#&gt; 1 21 70\n#&gt; 2 22 72\n#&gt; 3 23 73\n#&gt; 4 24 75\n#&gt; 5 25 77\n#&gt; 6 26 79\n#&gt; 7 27 81",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#data-frames-and-nas",
    "href": "024_df_functions.html#data-frames-and-nas",
    "title": "8  Working with data frames and functions",
    "section": "8.3 Data frames and NA’s",
    "text": "8.3 Data frames and NA’s\nComputation of a new column from columns of a dataframe\n\nmytable$G&lt;-mytable$A+mytable$F #note: adding C and F temperature is nonsensical though\nmytable$Gsquare&lt;-mytable$G^2 #note how you write an exponent \"^\" in R\nmytable$A*mytable$F # or a multiplication \"*\"\n#&gt; [1] 1470 1584 1679 1800 1925 2054 2187\n\nSimilarly we can apply our BMI computation to a data frame with heights and weights\n\nbmidf&lt;-data.frame(\n  h=c(1.8,1.7,2,1.9),\n  w=c(70,70,95,100))\n\nWe add the result of computing BMI directly as a new column “BMI” in our data.frame\n\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n\nNA is for unknowns !\nSuppose the 2nd person of our sample didn’t share his/her weight with us\n\nbmidf$w[2]&lt;-NA #NA is for unknowns\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n#You see the BMI could therefore not be computed\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nFor some functions you would still want to compute a value while ignoring the NA’s\nThe mean is a classical example\n\nmean(bmidf$h) #works\n#&gt; [1] 1.85\nmean(bmidf$w) #but returns NA because of one value not reported\n#&gt; [1] NA\n\nYou can explicitly ask to compute without the NA’s:\n\nmean(bmidf$w, na.rm=TRUE) #now works!\n#&gt; [1] 88.33333\n\nUsing complete cases\nFor some data frame made of surveyed values where different variables are filled in sparsely, it is important you get access only to entirely completed individuals\n\ncomplete.cases(bmidf) #returns a logical indicating whether the row \n#&gt; [1]  TRUE FALSE  TRUE  TRUE\n# has not a singleNA\nclass(complete.cases(bmidf))\n#&gt; [1] \"logical\"\n#Note that with logicals, TRUE is 1 and FALSE is zero. Thus\n\nsum(complete.cases(bmidf))\n#&gt; [1] 3\n\n#You can use this logical to subset the rows\n# and have a \"clean\" df\nbmidf2&lt;-bmidf[complete.cases(bmidf),] #read this as \"select complete cases rows with all columns\nbmidf2\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#environment-listing-and-management",
    "href": "024_df_functions.html#environment-listing-and-management",
    "title": "8  Working with data frames and functions",
    "section": "8.4 Environment listing and management",
    "text": "8.4 Environment listing and management\nWe have now created a bunch of objects which we can see in the Environment window of RStudio.\nIn the console we can also see them with\n\nls()\n#&gt; [1] \"bmi.calc\"           \"bmi.calc.text\"      \"bmidf\"             \n#&gt; [4] \"bmidf2\"             \"celsius2fahrenheit\" \"mytable\"           \n#&gt; [7] \"paste.heightweight\"\n\nAnd any of these objects can be removed with\n\nrm()\n#for example\nrm(mytable)\n\nIn the environment window of RStudio you also see the structure of objects (when displayed as a list not a grid)\nFrom the console you use the structure function str() to get the same info\n\nstr(bmidf)\n#&gt; 'data.frame':    4 obs. of  3 variables:\n#&gt;  $ h  : num  1.8 1.7 2 1.9\n#&gt;  $ w  : num  70 NA 95 100\n#&gt;  $ BMI: num  22 NA 24 28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#viewing-data-frame",
    "href": "024_df_functions.html#viewing-data-frame",
    "title": "8  Working with data frames and functions",
    "section": "8.5 Viewing data frame",
    "text": "8.5 Viewing data frame\n\nView(bmidf)\n\nis the most pleasant interactive way to view a data frame\nBut be careful if many rows or columns !\nThe classical console way is simply\n\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nIn case of a large vector or df there will be a limited display of 1000 values (default) in console\nSuppose you have a vector of 1002 values\n\nmydf&lt;-data.frame(z=1:502, zrev=502:1)\nmydf\n#&gt;       z zrev\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; 4     4  499\n#&gt; 5     5  498\n#&gt; 6     6  497\n#&gt; 7     7  496\n#&gt; 8     8  495\n#&gt; 9     9  494\n#&gt; 10   10  493\n#&gt; 11   11  492\n#&gt; 12   12  491\n#&gt; 13   13  490\n#&gt; 14   14  489\n#&gt; 15   15  488\n#&gt; 16   16  487\n#&gt; 17   17  486\n#&gt; 18   18  485\n#&gt; 19   19  484\n#&gt; 20   20  483\n#&gt; 21   21  482\n#&gt; 22   22  481\n#&gt; 23   23  480\n#&gt; 24   24  479\n#&gt; 25   25  478\n#&gt; 26   26  477\n#&gt; 27   27  476\n#&gt; 28   28  475\n#&gt; 29   29  474\n#&gt; 30   30  473\n#&gt; 31   31  472\n#&gt; 32   32  471\n#&gt; 33   33  470\n#&gt; 34   34  469\n#&gt; 35   35  468\n#&gt; 36   36  467\n#&gt; 37   37  466\n#&gt; 38   38  465\n#&gt; 39   39  464\n#&gt; 40   40  463\n#&gt; 41   41  462\n#&gt; 42   42  461\n#&gt; 43   43  460\n#&gt; 44   44  459\n#&gt; 45   45  458\n#&gt; 46   46  457\n#&gt; 47   47  456\n#&gt; 48   48  455\n#&gt; 49   49  454\n#&gt; 50   50  453\n#&gt; 51   51  452\n#&gt; 52   52  451\n#&gt; 53   53  450\n#&gt; 54   54  449\n#&gt; 55   55  448\n#&gt; 56   56  447\n#&gt; 57   57  446\n#&gt; 58   58  445\n#&gt; 59   59  444\n#&gt; 60   60  443\n#&gt; 61   61  442\n#&gt; 62   62  441\n#&gt; 63   63  440\n#&gt; 64   64  439\n#&gt; 65   65  438\n#&gt; 66   66  437\n#&gt; 67   67  436\n#&gt; 68   68  435\n#&gt; 69   69  434\n#&gt; 70   70  433\n#&gt; 71   71  432\n#&gt; 72   72  431\n#&gt; 73   73  430\n#&gt; 74   74  429\n#&gt; 75   75  428\n#&gt; 76   76  427\n#&gt; 77   77  426\n#&gt; 78   78  425\n#&gt; 79   79  424\n#&gt; 80   80  423\n#&gt; 81   81  422\n#&gt; 82   82  421\n#&gt; 83   83  420\n#&gt; 84   84  419\n#&gt; 85   85  418\n#&gt; 86   86  417\n#&gt; 87   87  416\n#&gt; 88   88  415\n#&gt; 89   89  414\n#&gt; 90   90  413\n#&gt; 91   91  412\n#&gt; 92   92  411\n#&gt; 93   93  410\n#&gt; 94   94  409\n#&gt; 95   95  408\n#&gt; 96   96  407\n#&gt; 97   97  406\n#&gt; 98   98  405\n#&gt; 99   99  404\n#&gt; 100 100  403\n#&gt; 101 101  402\n#&gt; 102 102  401\n#&gt; 103 103  400\n#&gt; 104 104  399\n#&gt; 105 105  398\n#&gt; 106 106  397\n#&gt; 107 107  396\n#&gt; 108 108  395\n#&gt; 109 109  394\n#&gt; 110 110  393\n#&gt; 111 111  392\n#&gt; 112 112  391\n#&gt; 113 113  390\n#&gt; 114 114  389\n#&gt; 115 115  388\n#&gt; 116 116  387\n#&gt; 117 117  386\n#&gt; 118 118  385\n#&gt; 119 119  384\n#&gt; 120 120  383\n#&gt; 121 121  382\n#&gt; 122 122  381\n#&gt; 123 123  380\n#&gt; 124 124  379\n#&gt; 125 125  378\n#&gt; 126 126  377\n#&gt; 127 127  376\n#&gt; 128 128  375\n#&gt; 129 129  374\n#&gt; 130 130  373\n#&gt; 131 131  372\n#&gt; 132 132  371\n#&gt; 133 133  370\n#&gt; 134 134  369\n#&gt; 135 135  368\n#&gt; 136 136  367\n#&gt; 137 137  366\n#&gt; 138 138  365\n#&gt; 139 139  364\n#&gt; 140 140  363\n#&gt; 141 141  362\n#&gt; 142 142  361\n#&gt; 143 143  360\n#&gt; 144 144  359\n#&gt; 145 145  358\n#&gt; 146 146  357\n#&gt; 147 147  356\n#&gt; 148 148  355\n#&gt; 149 149  354\n#&gt; 150 150  353\n#&gt; 151 151  352\n#&gt; 152 152  351\n#&gt; 153 153  350\n#&gt; 154 154  349\n#&gt; 155 155  348\n#&gt; 156 156  347\n#&gt; 157 157  346\n#&gt; 158 158  345\n#&gt; 159 159  344\n#&gt; 160 160  343\n#&gt; 161 161  342\n#&gt; 162 162  341\n#&gt; 163 163  340\n#&gt; 164 164  339\n#&gt; 165 165  338\n#&gt; 166 166  337\n#&gt; 167 167  336\n#&gt; 168 168  335\n#&gt; 169 169  334\n#&gt; 170 170  333\n#&gt; 171 171  332\n#&gt; 172 172  331\n#&gt; 173 173  330\n#&gt; 174 174  329\n#&gt; 175 175  328\n#&gt; 176 176  327\n#&gt; 177 177  326\n#&gt; 178 178  325\n#&gt; 179 179  324\n#&gt; 180 180  323\n#&gt; 181 181  322\n#&gt; 182 182  321\n#&gt; 183 183  320\n#&gt; 184 184  319\n#&gt; 185 185  318\n#&gt; 186 186  317\n#&gt; 187 187  316\n#&gt; 188 188  315\n#&gt; 189 189  314\n#&gt; 190 190  313\n#&gt; 191 191  312\n#&gt; 192 192  311\n#&gt; 193 193  310\n#&gt; 194 194  309\n#&gt; 195 195  308\n#&gt; 196 196  307\n#&gt; 197 197  306\n#&gt; 198 198  305\n#&gt; 199 199  304\n#&gt; 200 200  303\n#&gt; 201 201  302\n#&gt; 202 202  301\n#&gt; 203 203  300\n#&gt; 204 204  299\n#&gt; 205 205  298\n#&gt; 206 206  297\n#&gt; 207 207  296\n#&gt; 208 208  295\n#&gt; 209 209  294\n#&gt; 210 210  293\n#&gt; 211 211  292\n#&gt; 212 212  291\n#&gt; 213 213  290\n#&gt; 214 214  289\n#&gt; 215 215  288\n#&gt; 216 216  287\n#&gt; 217 217  286\n#&gt; 218 218  285\n#&gt; 219 219  284\n#&gt; 220 220  283\n#&gt; 221 221  282\n#&gt; 222 222  281\n#&gt; 223 223  280\n#&gt; 224 224  279\n#&gt; 225 225  278\n#&gt; 226 226  277\n#&gt; 227 227  276\n#&gt; 228 228  275\n#&gt; 229 229  274\n#&gt; 230 230  273\n#&gt; 231 231  272\n#&gt; 232 232  271\n#&gt; 233 233  270\n#&gt; 234 234  269\n#&gt; 235 235  268\n#&gt; 236 236  267\n#&gt; 237 237  266\n#&gt; 238 238  265\n#&gt; 239 239  264\n#&gt; 240 240  263\n#&gt; 241 241  262\n#&gt; 242 242  261\n#&gt; 243 243  260\n#&gt; 244 244  259\n#&gt; 245 245  258\n#&gt; 246 246  257\n#&gt; 247 247  256\n#&gt; 248 248  255\n#&gt; 249 249  254\n#&gt; 250 250  253\n#&gt; 251 251  252\n#&gt; 252 252  251\n#&gt; 253 253  250\n#&gt; 254 254  249\n#&gt; 255 255  248\n#&gt; 256 256  247\n#&gt; 257 257  246\n#&gt; 258 258  245\n#&gt; 259 259  244\n#&gt; 260 260  243\n#&gt; 261 261  242\n#&gt; 262 262  241\n#&gt; 263 263  240\n#&gt; 264 264  239\n#&gt; 265 265  238\n#&gt; 266 266  237\n#&gt; 267 267  236\n#&gt; 268 268  235\n#&gt; 269 269  234\n#&gt; 270 270  233\n#&gt; 271 271  232\n#&gt; 272 272  231\n#&gt; 273 273  230\n#&gt; 274 274  229\n#&gt; 275 275  228\n#&gt; 276 276  227\n#&gt; 277 277  226\n#&gt; 278 278  225\n#&gt; 279 279  224\n#&gt; 280 280  223\n#&gt; 281 281  222\n#&gt; 282 282  221\n#&gt; 283 283  220\n#&gt; 284 284  219\n#&gt; 285 285  218\n#&gt; 286 286  217\n#&gt; 287 287  216\n#&gt; 288 288  215\n#&gt; 289 289  214\n#&gt; 290 290  213\n#&gt; 291 291  212\n#&gt; 292 292  211\n#&gt; 293 293  210\n#&gt; 294 294  209\n#&gt; 295 295  208\n#&gt; 296 296  207\n#&gt; 297 297  206\n#&gt; 298 298  205\n#&gt; 299 299  204\n#&gt; 300 300  203\n#&gt; 301 301  202\n#&gt; 302 302  201\n#&gt; 303 303  200\n#&gt; 304 304  199\n#&gt; 305 305  198\n#&gt; 306 306  197\n#&gt; 307 307  196\n#&gt; 308 308  195\n#&gt; 309 309  194\n#&gt; 310 310  193\n#&gt; 311 311  192\n#&gt; 312 312  191\n#&gt; 313 313  190\n#&gt; 314 314  189\n#&gt; 315 315  188\n#&gt; 316 316  187\n#&gt; 317 317  186\n#&gt; 318 318  185\n#&gt; 319 319  184\n#&gt; 320 320  183\n#&gt; 321 321  182\n#&gt; 322 322  181\n#&gt; 323 323  180\n#&gt; 324 324  179\n#&gt; 325 325  178\n#&gt; 326 326  177\n#&gt; 327 327  176\n#&gt; 328 328  175\n#&gt; 329 329  174\n#&gt; 330 330  173\n#&gt; 331 331  172\n#&gt; 332 332  171\n#&gt; 333 333  170\n#&gt; 334 334  169\n#&gt; 335 335  168\n#&gt; 336 336  167\n#&gt; 337 337  166\n#&gt; 338 338  165\n#&gt; 339 339  164\n#&gt; 340 340  163\n#&gt; 341 341  162\n#&gt; 342 342  161\n#&gt; 343 343  160\n#&gt; 344 344  159\n#&gt; 345 345  158\n#&gt; 346 346  157\n#&gt; 347 347  156\n#&gt; 348 348  155\n#&gt; 349 349  154\n#&gt; 350 350  153\n#&gt; 351 351  152\n#&gt; 352 352  151\n#&gt; 353 353  150\n#&gt; 354 354  149\n#&gt; 355 355  148\n#&gt; 356 356  147\n#&gt; 357 357  146\n#&gt; 358 358  145\n#&gt; 359 359  144\n#&gt; 360 360  143\n#&gt; 361 361  142\n#&gt; 362 362  141\n#&gt; 363 363  140\n#&gt; 364 364  139\n#&gt; 365 365  138\n#&gt; 366 366  137\n#&gt; 367 367  136\n#&gt; 368 368  135\n#&gt; 369 369  134\n#&gt; 370 370  133\n#&gt; 371 371  132\n#&gt; 372 372  131\n#&gt; 373 373  130\n#&gt; 374 374  129\n#&gt; 375 375  128\n#&gt; 376 376  127\n#&gt; 377 377  126\n#&gt; 378 378  125\n#&gt; 379 379  124\n#&gt; 380 380  123\n#&gt; 381 381  122\n#&gt; 382 382  121\n#&gt; 383 383  120\n#&gt; 384 384  119\n#&gt; 385 385  118\n#&gt; 386 386  117\n#&gt; 387 387  116\n#&gt; 388 388  115\n#&gt; 389 389  114\n#&gt; 390 390  113\n#&gt; 391 391  112\n#&gt; 392 392  111\n#&gt; 393 393  110\n#&gt; 394 394  109\n#&gt; 395 395  108\n#&gt; 396 396  107\n#&gt; 397 397  106\n#&gt; 398 398  105\n#&gt; 399 399  104\n#&gt; 400 400  103\n#&gt; 401 401  102\n#&gt; 402 402  101\n#&gt; 403 403  100\n#&gt; 404 404   99\n#&gt; 405 405   98\n#&gt; 406 406   97\n#&gt; 407 407   96\n#&gt; 408 408   95\n#&gt; 409 409   94\n#&gt; 410 410   93\n#&gt; 411 411   92\n#&gt; 412 412   91\n#&gt; 413 413   90\n#&gt; 414 414   89\n#&gt; 415 415   88\n#&gt; 416 416   87\n#&gt; 417 417   86\n#&gt; 418 418   85\n#&gt; 419 419   84\n#&gt; 420 420   83\n#&gt; 421 421   82\n#&gt; 422 422   81\n#&gt; 423 423   80\n#&gt; 424 424   79\n#&gt; 425 425   78\n#&gt; 426 426   77\n#&gt; 427 427   76\n#&gt; 428 428   75\n#&gt; 429 429   74\n#&gt; 430 430   73\n#&gt; 431 431   72\n#&gt; 432 432   71\n#&gt; 433 433   70\n#&gt; 434 434   69\n#&gt; 435 435   68\n#&gt; 436 436   67\n#&gt; 437 437   66\n#&gt; 438 438   65\n#&gt; 439 439   64\n#&gt; 440 440   63\n#&gt; 441 441   62\n#&gt; 442 442   61\n#&gt; 443 443   60\n#&gt; 444 444   59\n#&gt; 445 445   58\n#&gt; 446 446   57\n#&gt; 447 447   56\n#&gt; 448 448   55\n#&gt; 449 449   54\n#&gt; 450 450   53\n#&gt; 451 451   52\n#&gt; 452 452   51\n#&gt; 453 453   50\n#&gt; 454 454   49\n#&gt; 455 455   48\n#&gt; 456 456   47\n#&gt; 457 457   46\n#&gt; 458 458   45\n#&gt; 459 459   44\n#&gt; 460 460   43\n#&gt; 461 461   42\n#&gt; 462 462   41\n#&gt; 463 463   40\n#&gt; 464 464   39\n#&gt; 465 465   38\n#&gt; 466 466   37\n#&gt; 467 467   36\n#&gt; 468 468   35\n#&gt; 469 469   34\n#&gt; 470 470   33\n#&gt; 471 471   32\n#&gt; 472 472   31\n#&gt; 473 473   30\n#&gt; 474 474   29\n#&gt; 475 475   28\n#&gt; 476 476   27\n#&gt; 477 477   26\n#&gt; 478 478   25\n#&gt; 479 479   24\n#&gt; 480 480   23\n#&gt; 481 481   22\n#&gt; 482 482   21\n#&gt; 483 483   20\n#&gt; 484 484   19\n#&gt; 485 485   18\n#&gt; 486 486   17\n#&gt; 487 487   16\n#&gt; 488 488   15\n#&gt; 489 489   14\n#&gt; 490 490   13\n#&gt; 491 491   12\n#&gt; 492 492   11\n#&gt; 493 493   10\n#&gt; 494 494    9\n#&gt; 495 495    8\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n#[ reached getOption(\"max.print\") -- omitted 2 entries ]\n\nYou can change the default using\n\n options(max.print=1500)\n\nbut one rarely does this\nMost of the times you want a sneak preview in your data from the top\n\n# head() returns the first rows (5 default) rows:\nhead(mydf)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\nhead(mydf,8)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\n#&gt; 7 7  496\n#&gt; 8 8  495\n\nor the bottom:\n\n# tail() the last ones :\ntail(mydf)\n#&gt;       z zrev\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\ntail(mydf, 7)\n#&gt;       z zrev\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\nor some random records if you load the car package (Package related to book “Companion to Applied Regression” by Fox, J and Weisber, S (2024) )\n\ncar::some(mydf)\n#&gt;       z zrev\n#&gt; 8     8  495\n#&gt; 22   22  481\n#&gt; 37   37  466\n#&gt; 96   96  407\n#&gt; 163 163  340\n#&gt; 193 193  310\n#&gt; 259 259  244\n#&gt; 298 298  205\n#&gt; 357 357  146\n#&gt; 471 471   32\n\nor a brief:\n\ncar::brief(mydf)\n#&gt; 502 x 2 data.frame (497 rows omitted)\n#&gt;       z zrev\n#&gt;     [i]  [i]\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; . . .            \n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied Regression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "025_read_write.html",
    "href": "025_read_write.html",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "9.1 Reading delimited files\nText files and delimited files can easily be imported in R using the function read.table(). You can play with the values of the different arguments to adapt to the format of your file. You have the possibility to load a file\n?read.table\n\nheartSA &lt;- read.table(\"data/SAheart/SAheart.txt\", header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(file.choose(), header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", \n                      sep=\",\", head=T, row.names=1)\nThere exist also some preset functions to read tables with some specific formats. Here again you can play we the values of the different arguments to load your database in an accurate way.\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n          dec = \",\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n            dec = \",\", fill = TRUE, comment.char = \"\", ...)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#reading-delimited-files",
    "href": "025_read_write.html#reading-delimited-files",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "stored locally: writing the path to reach it\nchosen interactively and stored locally\nfrom the web using its url.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#files-form-specific-software",
    "href": "025_read_write.html#files-form-specific-software",
    "title": "9  Reading and writing data to and from R",
    "section": "9.2 Files form specific software",
    "text": "9.2 Files form specific software\nSome databases are exported from other software and have atypical formats.\n\nYou first need to install (once per machine) and load the R package foreign.\n\n\n# install.packages('foreign')\nlibrary(foreign)\n\n\nThen you can load the following different formats\n\n\n# Read the SPSS data\nread.spss(\"example.sav\")\n\n# Read Stata data into R\nread.dta(\"c:/mydata.dta\") \n\nlibrary(xlsx)\n\n# first row contains variable names\nread.xlsx(\"c:/myexcel.xlsx\", 1)\n\n# read in the worksheet named mysheet\nread.xlsx(\"c:/myexcel.xlsx\", sheetName = \"mysheet\") \n\n# The package `readxl` can also be helpful.\n\nlibrary(sas7bdat)\n\n# Read in the SAS data\nmySASData &lt;- read.sas7bdat(\"example.sas7bdat\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#writing-a-dataframe-to-a-file",
    "href": "025_read_write.html#writing-a-dataframe-to-a-file",
    "title": "9  Reading and writing data to and from R",
    "section": "9.3 Writing a dataframe to a file",
    "text": "9.3 Writing a dataframe to a file\nYou can also export a dataset\n\nwrite.table(heartSA, file = \"data/SAheart/NewSAheart.txt\")\nwrite.csv(heartSA, file = \"data/SAheart/NewSAheart.csv\")\nwrite.csv2(heartSA, file = \"data/SAheart/NewSAheart.csv\")\n\nHowever, in case it is for further use within R, we recommend you use the RDS format, which works with any type of R object and is often very effective in terms of file size.\n\nsaveRDS(heartSA,\"data/SAheart/NewSAheart.rds\")\nNewSAheart&lt;-readRDS(\"data/SAheart/NewSAheart.rds\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html",
    "href": "031_sample_population.html",
    "title": "10  Sample and population",
    "section": "",
    "text": "10.1 Definitions\nSurvey\nElement, record, individual\nPopulation\nTarget population\nSampling units\nFrame\nSample",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#definitions",
    "href": "031_sample_population.html#definitions",
    "title": "10  Sample and population",
    "section": "",
    "text": "Any activity that collects information in an organised and methodical manner about characteristics of interest from some or all units of a population using well-defined concepts, methods and procedures and compiles such information into a useful summary form.\n\n\n\nAn object on which a measurement is taken.\n\n\n\nA collection of elements.\n\n\n\nThe population for which information is required.\n\n\n\nNon-overlapping collection of elements from the population that covers the entire population.\n\n\n\nThe device which delimits, identifies, and allows access to the elements of the target population. The frame is also called the survey frame or the sampling frame.\n\n\n\nA collection of sampling units drawn from a frame.",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#usual-denotations",
    "href": "031_sample_population.html#usual-denotations",
    "title": "10  Sample and population",
    "section": "10.2 Usual denotations",
    "text": "10.2 Usual denotations\nPopulation observations are usually denoted with capital letters: \\[X = \\{X_1, X_2,..., X_N\\}\\]\nTheir parameters are denoted with Greek letters, capital letters or a mix: \\[\\mu, \\mu_X, \\sigma, \\sigma_X, \\sigma_{X,Y}, E(X), Var(X), Cov(X,Y), \\rho_{X,Y}\\]\nSample observations are expressed with small letters: \\[x = (x_1, x_2,..., x_n)\\]\nTheir estimators associated are also expressed using small letters, i.e. \\[\\bar{x}, s, s_x, s_{x,y}, var(x), cov(x,y), r_{x,y}\\] …",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and population</span>"
    ]
  },
  {
    "objectID": "032_univariate.html",
    "href": "032_univariate.html",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1 Measures of Center",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-center",
    "href": "032_univariate.html#measures-of-center",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1.1 Mean\nThe arithmetic mean of a vector x having n observations: x = (x1, x2,… , xi, …, xn) is given by the following formulae for the empirical and theoretical means:\n\\[\\bar{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\] \\[\\mu_X = E(X) = \\dfrac{1}{N} \\sum_{i=1}^N X_i\\]\nwith \\(n\\) the size of the sample and \\(N\\) the size of the population.\nLet’s take a variable with the ages of some individuals:\n\nAge &lt;- c (25, 27, 28, 23, 52, 27, 27, 26, 25, 30)\n\nThe empirical mean of this variable is given by:\n\\((1 / 10) * (25 + 27 + 28 + 23 + 52 + 27 + 27 + 26 + 25 + 30) = 290 / 10 = 29\\)\nYou can compute the mean of a vector as follows in R:\n\nsum(Age) / length(Age)\n#&gt; [1] 29\nmean(Age)\n#&gt; [1] 29\n\nBut in some cases, the mean is not a good indicator of the central value of a distribution. For the above variable, we can see that one individual is 52 years old. The mean is sensitive to extreme values or outliers.\n\n\n11.1.2 Median\nThe median corresponds to the value such that 50% of the individuals have a smaller or equal value and 50% of the individuals have a larger or equal value. It is also called the 50th percentile. We consider the variable Age and a second variable Age2 where the value 52 has been removed. They have respectively 10 and 9 elements.\n\nAge2 &lt;- c (25, 27, 28, 23, 27, 27, 26, 25, 30)\n\nFirst, sort the values of the vector considered from the smallest to the largest. For Age: \\({23, 25, 25, 26, 27, 27, 27, 28, 30, 52}\\)\nand for Age2:\n\\({23, 25, 25, 26, 27, 27, 27, 28, 30}\\)\nWe know already how to do this in R:\n\nsort(Age)\n#&gt;  [1] 23 25 25 26 27 27 27 28 30 52\nsort(Age2)\n#&gt; [1] 23 25 25 26 27 27 27 28 30\n\nSecond,\n\nFor an odd set of numbers (Age2), find the number in the middle of the vector, this is the empirical median. The number to take is also given by: \\((n + 1) / 2 = 10 / 2\\) , i.e. the 5th value, 27\n\n\nsort(Age2)[5]\n#&gt; [1] 27\n\n\nFor an even set of numbers (Age), find the two numbers in the middle and compute their average value, this is the empirical median, i.e. \\((27 + 27) / 2 = 27\\)\n\n\nsort(Age)[c(5,6)]\n#&gt; [1] 27 27\nsum(sort(Age)[c(5,6)])/2\n#&gt; [1] 27\n\nWe can see that the median is much less sensitive to extreme values. In both cases, the median is 27. Using R built-in functions, the median is :\n\nmedian(Age)\n#&gt; [1] 27\nmedian(Age2)\n#&gt; [1] 27\n\n\n\n11.1.3 Mode\nThe mode (not to be mistaken with R mode for vectors) corresponds to the value(s) which appears the most often. A vector can have 0, 1 or many modes. For our variable Age, the mode is 27 which appears 3 times.\n\ntable(Age)\n#&gt; Age\n#&gt; 23 25 26 27 28 30 52 \n#&gt;  1  2  1  3  1  1  1\nsort(table(Age), decreasing = TRUE)\n#&gt; Age\n#&gt; 27 25 23 26 28 30 52 \n#&gt;  3  2  1  1  1  1  1\n\nThe mode is an immediate output in R. But we can write what we have just done and extract the first value after sorting, i.e.\n\nsort(table(Age),decreasing = TRUE)[1]\n#&gt; 27 \n#&gt;  3\n\nThe output however is the frequency (3), not the number we look for (27). So we should get the name and turn it into a numeric for obtaining the mode as a numeric:\n\nas.numeric(names(sort(table(Age),decreasing = TRUE)[1]))\n#&gt; [1] 27\n\nWhile this requires sorting (which can be long), an alternative would be to use the which.max() function, but still requires to get the value from name as a numeric:\n\nwhich.max(table(Age)) #returning the position of the max, i.e. 4th position here\n#&gt; 27 \n#&gt;  4\ntable(Age)[which.max(table(Age))] #then using that position into the original vector\n#&gt; 27 \n#&gt;  3\nas.numeric(names(table(Age)[which.max(table(Age))]))\n#&gt; [1] 27\n\nWe can also look at the mode for qualitative / categorical variables. If we take the example of the variable score, the mode is the value “C”.\n\nscore &lt;- as.factor ( c (\"C\",\"C\",\"A\",\"B\",\"A\",\"C\",\"B\",\"B\",\"A\",\"C\"))\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4\nnames(which.max(table(score)))\n#&gt; [1] \"C\"\n\nNote that, that the numeric case being a little complicated, we can see from the help that table() is based on tabulate() and find the following which is more direct for numeric examples:\n\nwhich.max(tabulate(Age))\n#&gt; [1] 27\n\nWe can assemble both factor and numeric cases in a single function, for example:\n\nmymode&lt;-function(x){\n  ifelse(is.factor(x)==TRUE,\n         names(which.max(table(x))),\n         which.max(tabulate(x)))\n}\nmymode(Age)\n#&gt; [1] 27\nmymode(score)\n#&gt; [1] \"C\"",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-dispersion",
    "href": "032_univariate.html#measures-of-dispersion",
    "title": "11  Univariate statistics",
    "section": "11.2 Measures of Dispersion",
    "text": "11.2 Measures of Dispersion\n\n11.2.1 Range\nRange is the simplest measure of the spread of a distribution and corresponds to the difference between the maximum and the minimum values: \\[Max(x) - Min(x)\\]\nFor the variable Age the minimum being 23 and the maximum 52, the range is: \\(52 - 23 = 29\\).\nIn R the function range returns the two extrema, not the difference, see\n\nmax(Age) - min(Age)\n#&gt; [1] 29\nrange(Age)\n#&gt; [1] 23 52\n\n\n\n11.2.2 Quantiles.\nExtending the concept of a median, quantiles (percentiles, deciles, quartiles,…) divide the distribution into equal slices. The i-th percentile corresponds to the value at which i% of the distribution is below that value. The median is when \\(i=50%\\), i.e. the ditribution is split into 2 half parts so that the probability of drawing a number below the median is 50%.\nPercentiles divide the distribution into 100 slices (probability = \\({0.01, 0.02, 0.03, ..., 1}\\)) ; deciles into 10 (probability = \\({0.1, 0.2, 0.3, ..., 1}\\)) ; quartiles into 4 (probability = \\({0.25, 0.5, 0.75, 1}\\)). You obtain all of these using the same function quantile() and the corresponding probability of picking up a number below:\n\n# quartiles are the default\nquantile(Age)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 23.00 25.25 27.00 27.75 52.00\n#deciles\nquantile(Age, probs = seq(0, 1, 0.1))\n#&gt;   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#&gt; 23.0 24.8 25.0 25.7 26.6 27.0 27.0 27.3 28.4 32.2 52.0\n\n#Suppose a larger set of 100000 values \"normally\" distributed around the mean 0\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n#the 1st , 5th, 95th and 99th % and some others\nquantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99))\n#&gt;         1%         5%        16%        84%        95%        99% \n#&gt; -2.3333179 -1.6488276 -0.9924832  1.0009836  1.6512973  2.3372571\n\n\nhist(x, breaks = 100)\nabline(v=quantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99)), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Inter-quartile range (IQR)\nIt is simply the difference between the 3rd and the 1st quartiles: \\[IQR = Q3(x) - Q1(x)\\].\nAgain for the variable Age, it equals: \\(27.75 - 25.25 = 2.5\\)\n\nquantile(Age, probs = 0.75) - quantile(Age, probs = 0.25)\n#&gt; 75% \n#&gt; 2.5\n\n#or\nIQR(Age)\n#&gt; [1] 2.5\n\n\n\n11.2.4 Variance and Standard Deviation\nAlthough quantiles and plots are very much in use to describe the spread of a distribution, statistical analysis relies most heavily on the notion of variance.\nFirst, think about the simplest way you can measure how a given observation is far from, (i.e. spread out of) a general expected value. A pretty effective way is to measure the difference between that observation and the mean of observations.\nThe Deviation to the mean for an individual i is \\[v_i=x_i-\\bar{x}\\]\nIt is then very tempting to say that the general dispersion of a variable is simply the sum of all those values. In order for the number not to grow with the number of observations, we then compute an average deviation by dividing by \\(n\\)\n\\[\\Sigma_i(x_i-\\bar{x})/n\\]\nBut is this a good idea?\nTake the Age example:\n\nv&lt;- Age-mean(Age) #set of deviations to the mean\nsum(v)/length(v)\n#&gt; [1] 0\n\nIt seems there is no “spreading” ? In fact, all the negative deviations compensate (here exactly) the positive deviations.\nWe can rather remove the signs and use the absolute value of each deviation, sum them up and divide by \\(n\\).\nThis is called MAD, the Mean Absolute Deviation and is quite easy to interpret indeed.\n\nabs_v&lt;- abs(Age-mean(Age)) #set of deviations to the mean\nmean(abs_v)\n#&gt; [1] 4.8\n\nYet, one could argue that large deviations to the mean are more important than the smaller ones to describe the pattern of deviations, especially since in a normal population there are more values closer to the mean than farther.\nRather than using absolute deviations, (most of) statisticians have therefore opted for squaring the deviations, which is still symmetrical and has the same characteristic of turning every negative value into a positive one.\nWe therefore usually consider the sum of squared deviations to the mean:\n\\[\\Sigma_i^n(x_i-\\bar{x})^2\\] which, we then divide by the number of observations to avoid the value to grow with the number of observations, thus allowing comparisons. It is then called the variance. More precisely, if we use a sample, we still need to use one of our observation in order to estimate the mean, hence we are left with \\(n-1\\) degrees of freedom.\nThe empirical variance is then given by:\n\\[var_x = s^2_x = \\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2\\]\nand the theoretical variance:\n\\[var_X = \\sigma^2_X = \\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2\\] \\[= E(X-E(X))^2 = E(X-\\mu)^2 \\]\nwhere \\(E(X)\\) is the expected mean of the population (or “Esperance”).\nThe variance is thus a single number that gives insight on how the variable is spread around the mean value. A small value (close to 0) indicates a small variability: values are not very different from the mean value. A high value indicated a strong variability.\nThe variance cannot be negative.\nIn R, we use the var() functio, which we here first reconstruct:\n\n(Age-mean(Age))^2 #squared deviations to the mean\n#&gt;  [1]  16   4   1  36 529   4   4   9  16   1\nsum((Age-mean(Age))^2) #sum of squared deviations to the mean\n#&gt; [1] 620\nsum((Age-mean(Age))^2)/(length(Age)-1) #...divided by n-1\n#&gt; [1] 68.88889\n\nvar(Age)\n#&gt; [1] 68.88889\nvar(Age2)\n#&gt; [1] 4.027778\n\nWe see that the default in R for var() is to divide by \\(n-1\\), i.e. the sample variance.\nFinally, we like the “dispersion” to be expressed in the same units as the original variable, i.e. years in this case. It is already the case for the Mean Absolute Deviation. We need to take the square root of the variance to obtain a “standard deviation”:\nThe standard deviations corresponding to the sample and population variance are then given by:\n\\[s_x = \\sqrt{\\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2}\\]\n\\[\\sigma_X = \\sqrt{\\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2}\\]\nAnd can be computed using:\n\nsqrt(var(Age))\n#&gt; [1] 8.299933\nsqrt(var(Age2))\n#&gt; [1] 2.006932\n#or simply\nsd(Age) #again remember it is the sample sd\n#&gt; [1] 8.299933\nsd(Age2)\n#&gt; [1] 2.006932",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#visualize-a-distribution",
    "href": "032_univariate.html#visualize-a-distribution",
    "title": "11  Univariate statistics",
    "section": "11.3 Visualize a distribution",
    "text": "11.3 Visualize a distribution\nVisualizing a distribution with graphics is important for both supporting the analysis and dissemination.\nWe have seen some graphics already above and we are going to produce improved graphics with ggplot later on. Without spending much time on design, the purpose here is show how graphics accompany the univariate statistics we introduced.\n\n11.3.1 Boxplots\nA boxplot visually provides a number of information about the distribution of a variable\n\nthe median value (thick black line),\nthe inter-quartile range (IQR) (the black box),\nthe minimum and maximum values or 1.5 times the IQR (horizontal lines),\nthe outlier(s) (dots out of the whiskers).\n\nValues are considered outliers when they fall outside the whiskers, that is outside a distance of 1.5 times the IQR. In absence of such outliers the horizontal lines show the extrema (min and max).\nWe have added the mean as a red point to clarify here the difference between the mean and median.\nExamine the difference again between Age and Age2\n\npar (mfrow = c(1, 2)) # to display multiple plots at once\nboxplot(Age, ylab = \"Age\", main = \"Boxplot of Age\")\npoints(mean(Age), col = 2, pch = 18)\n\nboxplot(Age2, ylab = \"Age2\", main = \"Boxplot of Age2\")\npoints(mean(Age2), col = 2, pch = 18)\n\n\n\n\n\n\n\n\n\n\n11.3.2 Stem and leaf\nProbably less in use nowadayd for visual purpose and reporting, a stem and leaf graph is a very effective way to look into the distribution of a variable while you are exploring, analyzing your data in the console.\nIt is not actually a plot but a presentation of the values into a “stem”, which is made of the values that are present across all cases and then a “leaf” where the remaining parts of each numbers is shown and concatenated, thus showing a kind of frequency together with the values:\nExamine the case for Age and Age2:\n\nstem(Age)\n#&gt; \n#&gt;   The decimal point is 1 digit(s) to the right of the |\n#&gt; \n#&gt;   2 | 35567778\n#&gt;   3 | 0\n#&gt;   4 | \n#&gt;   5 | 2\nstem(Age2)\n#&gt; \n#&gt;   The decimal point is at the |\n#&gt; \n#&gt;   22 | 0\n#&gt;   24 | 00\n#&gt;   26 | 0000\n#&gt;   28 | 0\n#&gt;   30 | 0\n\n\n\n11.3.3 Histogram\nHistograms are probably the first go to graphic in order to visualize a distribution\nLet’s reuse the x normal variable we created earlier and plot both its boxplot and the histogram\n\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n\nsummary(x)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; -4.127442 -0.668690  0.004522  0.003651  0.682819  4.323841\nboxplot(x, main = \"Boxplot of a random variable following N(0,1) with n = 100,000\")\n\n\n\n\n\n\n\nhist(x)\n\n\n\n\n\n\n\n\nA histogram is more detailed than a boxplot because it shows every data but does not provide a central or dispersion measure. Key to using a histogram is to play with the number of bars, otherwise some information, gaps, or multimodalities may be not be seen. You adapt the number of bars using the option “breaks”\n\npar(mfrow=c(1,2))\nhist(x, breaks=5)\nhist(x, breaks=100)\n\n\n\n\n\n\n\n\nFor a categorical variable (factor), the function plot() gives the counts of each category (level). We have worked an example using Le Tour de France data earlier in the course. It is similar to a visualisation of the table() output and is equivalent to the function hist() for quantitative variables.\n\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4\nplot(score, main = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nFor a numeric variable, a call to plot, shows values along the vertical axis and the index of the rows along the horizontal axis, which is rarely a useful information.\n\nplot(x)",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#the-shape-of-distributions-skewness-and-kurtosis",
    "href": "032_univariate.html#the-shape-of-distributions-skewness-and-kurtosis",
    "title": "11  Univariate statistics",
    "section": "11.4 The shape of distributions: Skewness and Kurtosis",
    "text": "11.4 The shape of distributions: Skewness and Kurtosis\n\n11.4.1 First and second moments:\nWe have seen earlier that the very first way to characterise a distribution is to use its mean and that the second way is to use the variance (or the square root of the variance, i.e. the standard deviation).\nThe mean and the variance are also named, respectively, the first and second moment of a distribution\nIndeed, for discrete data, the mean (or expectation) is calculated as:\n\\[\\mu = E[X] = \\sum_{i} x_i p(x_i)\\] where \\(x_i\\) are the values and \\(p(x_i)\\) are their probabilities.\nNote 1: for continuous variables, we should in fact use an integral rather that the sum symbol. Note 2, the “zeroth” moment of the distribution is in fact the sum of probabilities (x_i), i.e. the total mass (the concept is borrowed from physics), i.e. 1.\nThe variance is the second order moment, measuring the spread of the distribution around the mean. We have seen it is defined in difference to the mean with a square exponent:\n\\[\\sigma^2 = E[(X - \\mu)^2] = \\sum_{i} (x_i - \\mu)^2 p(x_i)\\] We can actually go on with higher exponents and use higher level moments to describe the shape of a distribution.\n\n\n11.4.2 Third moment: Skewness\nThe skewness is the third order moment of a distribution. The skewness measures the asymmetry of the distribution around the mean. It is is calculated as:\n\\[\\gamma_1 = E\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right] = \\sum_{i} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^3 p(x_i)\\]\nIn addition to the cubic exponent applied to the deviation to the mean, you notice we also divide by the standard deviation \\(\\sigma\\) to make the measure dimensionless, i.e. comparable across cases with different units. The skewness is not influenced by the scale of the data.\nIn practice, the skewness is computed as follows:\n\\[\\dfrac {\\sum_{i=1}^{n} (x_i - \\bar{x})^3} {n s^3}\\]\n\nset.seed(101)\nx&lt;-rnorm(1000, mean=50,sd=5)\nmean(x)\n#&gt; [1] 49.82569\nmedian(x)\n#&gt; [1] 49.72804\ne1071::skewness(x)\n#&gt; [1] -0.004246246\ne1071::skewness((x-100)/1000) #with this you see it is not influenced by any rescaling\n#&gt; [1] -0.004246246\nsum((x-mean(x))^3)/(sd(x)^3*length(x)) #manual computation\n#&gt; [1] -0.004246246\n\nWhen the skewness is \\(&gt;0\\), the tail of the distribution is heavier on the right side. This means there are more extreme values on the higher end. The mean is greater than the median.\nWhen the skewness is \\(&lt;0\\), the tail of the distribution is heavier on the left side. This means there are more extreme values on the lower end. The mean is lower than the median.\nA value around zero indicates a symmetric distribution.\n\nset.seed(101)\nright_skewed &lt;- x + rexp(1000, rate = 0.1) #we add an exp to the previous x for generating a right skewed distribution\nleft_skewed &lt;- rnorm(1000, mean = 50, sd = 5) - rexp(100, rate = 0.1) #left skewed\n\nhist(x, main = \"Symmetric\", col = \"blue\", breaks = 20)\n\n\n\n\n\n\n\nhist(right_skewed, main = \"Right-skewed\",col = \"green\", breaks = 20)\n\n\n\n\n\n\n\nhist(left_skewed, main = \"Left-skewed\", col = \"red\", breaks = 20)\n\n\n\n\n\n\n\n\ne1071::skewness(x)\n#&gt; [1] -0.004246246\ne1071::skewness(right_skewed)\n#&gt; [1] 1.350504\ne1071::skewness(left_skewed)\n#&gt; [1] -0.5836373\n\n\n\n11.4.3 Fourth moment: Kurtosis\nThe Kurtosis is the fourth order moment of a distribution. The Kurtosis measures the peakness of the distribution and is calculated as:\n\\[\\gamma_2 = E\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^4\\right] - 3 = \\sum_{i} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^4 p(x_i) - 3\\]\nYou notice the exponent to the level 4 applied to the deviations to the mean and that similarly to the skewness, we also divide by the standard deviation \\(\\sigma\\) to make the measure dimensionless, i.e. comparable across cases with different units. The Kurtosis is not influenced by the scale of the data.\nIn practice in R, the Kurtosis is similar to the skewness described above:\n\\[\\dfrac {\\sum_{i=1}^{n} (x_i - \\bar{x})^4} {n s^4}\\] However it is adjusted with some correction for small sample sizes and for comparison to a normal distribution for which the result would be 3. See the help for how it is computed by default\n\nset.seed(101)\nx&lt;-rnorm(1000, mean=50,sd=5)\ne1071::kurtosis(x)\n#&gt; [1] -0.119472\ne1071::kurtosis((x-100)/1000) #with this you see it is not dependent on scale \n#&gt; [1] -0.119472\n\nUsing this implementation, a Kurtosis close to 0 then indicates a distribution similar in shape to a normal (bell-shaped) distribution. A positive kurtosis indicates a more peaked distribution, also named leptokurtic.\nA negative kurtosis indicates a less peaked shape, named platykurtic.\nas an example, we can add some extreme values to the normal values we created earlier in order to higher peak look or trim the extremes of a normal distribution to have a flatter one:\n\nset.seed(1010)\nx&lt;-rnorm(1000, mean=50,sd=5)\nx_peaker&lt;-c(x, rnorm(300, mean = 50, sd = 20))\nx_flatter&lt;-x[abs(x) &lt; 60]\n\nhist(x, main = \"Normal\", col = \"blue\", breaks = 20)\n\n\n\n\n\n\n\nhist(x_peaker, main = \"Higher peak\",col = \"green\", breaks = 20)\n\n\n\n\n\n\n\nhist(x_flatter, main = \"Flatter\", col = \"red\", breaks = 20)\n\n\n\n\n\n\n\n\ne1071::kurtosis(x)\n#&gt; [1] 0.07423458\ne1071::kurtosis(x_peaker)\n#&gt; [1] 5.789643\ne1071::kurtosis(x_flatter)\n#&gt; [1] -0.1829135",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "033_discretisation.html",
    "href": "033_discretisation.html",
    "title": "12  Discretisation",
    "section": "",
    "text": "Geographers, maybe more than others because they like to produce maps, are often tempted to cut a numerical vector into a set classes.\nA typical use is the cartography of a continuous variable in a set of 5, 6 or 7 groups, knowing the human eye has difficulties to disentangle more colours. GIS and mapping software all have a menu where “discretisation” is made using a number of manually defined limits of preset algorithms, such as “Natural breaks”, “quantiles”, etc.\nIn R, the function cut()` is the base function to divide a range of numeric values x into intervals by a number of break values. It outputs a new factor where each value x is given a level according to which interval they fall in. The breaks are either\n\na scalar greater or equal to 2 for the range to be cut into equal length pieces.\na set of breaks to be used as the upper and lower limits of each discrete category\n\n\nset.seed(101)\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf4&lt;-cut(x, breaks=4)\nhead(xf4)\n#&gt; [1] (94.1,98.4] (98.4,103]  (94.1,98.4] (98.4,103]  (98.4,103]  (103,107]  \n#&gt; Levels: (89.7,94.1] (94.1,98.4] (98.4,103] (103,107]\ntable(xf4)\n#&gt; xf4\n#&gt; (89.7,94.1] (94.1,98.4]  (98.4,103]   (103,107] \n#&gt;           2           5           9           4\n\nNotice how the label clearly indicates the (default) closing on the right of each interval\n\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf6&lt;-cut(x, breaks=c(min(x),-90,95,100,105,110, max(x)))\nhead(xf6)\n#&gt; [1] (95,100]  (100,105] (95,100]  (89.6,95] (100,105] (89.6,95]\n#&gt; Levels: (-90,89.6] (89.6,95] (95,100] (100,105] (105,106] (106,110]\ntable(xf6)\n#&gt; xf6\n#&gt; (-90,89.6]  (89.6,95]   (95,100]  (100,105]  (105,106]  (106,110] \n#&gt;          1          3          4         10          2          0\n\nSince we can choose any breaks, it is pretty easy to adapt and use any discretisation method one would find elsewhere, e.g. in mapping packages.\nThere is a wonderful package, classInt, that does so and where you can simply choose the discretisation methodology.\nLet’s explore!\nhttps://cran.r-project.org/web/packages/classInt/classInt.pdf\nWe refer to the help of the package and specifically the function classIntervals()to find out about the available methods\n\nx_quantile_5&lt;-classInt::classIntervals(x, n=5, style=\"quantile\") #Default style\nx_quantile_5\n#&gt; style: quantile\n#&gt;   one of 3,876 possible partitions of this variable into 5 classes\n#&gt; [89.63447,95.69211) [95.69211,100.2653) [100.2653,102.3357) [102.3357,103.8727) \n#&gt;                   4                   4                   4                   4 \n#&gt; [103.8727,105.9493] \n#&gt;                   4\n\nThe classIntervals() output has its own class and specific plotting method that works with a given colour palette\n\nclass(x_quantile_5)\n#&gt; [1] \"classIntervals\"\nmycolors&lt;-c(\"darkgreen\",\"lightgreen\",\"lightyellow\", \"orange\", \"orangered\")\nplot(x_quantile_5, pal=mycolors)\n\n\n\n\n\n\n\n\nGiven the normality of the distribution, and the use of quantiles, the central class logically needs a smaller range to host the same number of values.\nBelow another split based on standard deviations:\n\nmean(x)\n#&gt; [1] 99.97489\nsd(x)\n#&gt; [1] 4.865144\nx_sd_5&lt;-classInt::classIntervals(x, n=5, style=\"sd\")\nx_sd_5\n#&gt; style: sd\n#&gt;   one of 50,388 possible partitions of this variable into 8 classes\n#&gt;  [87.81203,90.2446)  [90.2446,92.67717) [92.67717,95.10974) [95.10974,97.54231) \n#&gt;                   1                   1                   2                   1 \n#&gt; [97.54231,99.97489) [99.97489,102.4075)   [102.4075,104.84)   [104.84,107.2726] \n#&gt;                   3                   5                   5                   2\nplot(x_sd_5, pal=mycolors)\n\n\n\n\n\n\n\n\nAnd with 7 classes using the “Jenks” method, similar to the one we find within Esri ArcGIS:\n\nx_jenks_7&lt;-classInt::classIntervals(x, n=7, style=\"jenks\")\nx_jenks_7\n#&gt; style: jenks\n#&gt;   one of 27,132 possible partitions of this variable into 7 classes\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2\nplot(x_jenks_7, pal=mycolors)\n\n\n\n\n\n\n\n\nNotice that, ahead of plotting, classInt` expanded the number of colours, which we provided. In fact, 2 would be enough:\n\nplot(x_jenks_7, pal=c(\"yellow\",\"red\"))\n\n\n\n\n\n\n\n\nInterestingly, rather that specifying a number of classes, one could also use the same breaks as a standard boxplot:\n\nx_box&lt;-classInt::classIntervals(x,  style=\"box\")\nx_box\n#&gt; style: box\n#&gt;   one of 11,628 possible partitions of this variable into 6 classes\n#&gt; [89.63447,89.84276) [89.84276,98.08961) [98.08961,101.8191) [101.8191,103.5875) \n#&gt;                   1                   4                   5                   5 \n#&gt; [103.5875,111.8343)      [111.8343,Inf] \n#&gt;                   5                   0\nquantile(x,probs=c(0.25,0.5,0.75))\n#&gt;       25%       50%       75% \n#&gt;  98.08961 101.81905 103.58750\nc(quantile(x,probs=0.25)-1.5*IQR(x),\n  quantile(x,probs=0.75)+1.5*IQR(x))\n#&gt;       25%       75% \n#&gt;  89.84276 111.83435\nplot(x_box, pal=mycolors)\n\n\n\n\n\n\n\n\nOne then retrieves a vector of the categories in which each values fall using findCols(), which we can easily add to a dataframe as a new column, or even a vector of colours for use anywhere else using findColours().\nThis is shown below with our Jenks example:\n\nclassInt::findCols(x_jenks_7)\n#&gt;  [1] 4 6 4 2 6 2 5 4 5 5 6 5 7 1 7 3 5 6 2 5\nclassInt::findColours(x_jenks_7, pal=c(\"yellow\",\"red\"))\n#&gt;  [1] \"#FF7F00\" \"#FF2A00\" \"#FF7F00\" \"#FFD400\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt;  [8] \"#FF7F00\" \"#FF5500\" \"#FF5500\" \"#FF2A00\" \"#FF5500\" \"#FF0000\" \"#FFFF00\"\n#&gt; [15] \"#FF0000\" \"#FFAA00\" \"#FF5500\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt; attr(,\"palette\")\n#&gt; [1] \"#FFFF00\" \"#FFD400\" \"#FFAA00\" \"#FF7F00\" \"#FF5500\" \"#FF2A00\" \"#FF0000\"\n#&gt; attr(,\"table\")\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2\n\nFinally,\nFinally, you may find more convenient to use classify_intervals(), a wrapper for the sequence classIntervals() and findCols(), in order to issue directly the factor levels, as in cut(), rather than a more complex classIntervals object.\n\nk4&lt;-classInt::classify_intervals(x, n=4, style=\"kmeans\")\nk4\n#&gt;  [1] [94.66309,100.1217) [103.0166,105.9493] [94.66309,100.1217)\n#&gt;  [4] [89.63447,94.66309) [103.0166,105.9493] [89.63447,94.66309)\n#&gt;  [7] [100.1217,103.0166) [94.66309,100.1217) [100.1217,103.0166)\n#&gt; [10] [100.1217,103.0166) [103.0166,105.9493] [100.1217,103.0166)\n#&gt; [13] [103.0166,105.9493] [89.63447,94.66309) [103.0166,105.9493]\n#&gt; [16] [94.66309,100.1217) [100.1217,103.0166) [103.0166,105.9493]\n#&gt; [19] [89.63447,94.66309) [100.1217,103.0166)\n#&gt; 4 Levels: [89.63447,94.66309) [94.66309,100.1217) ... [103.0166,105.9493]\nclass(k4)\n#&gt; [1] \"factor\"",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discretisation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html",
    "href": "034_cross_tabulation.html",
    "title": "13  Cross-tabulation",
    "section": "",
    "text": "13.1 Case of 2 or several factors (contingency tables)\nSuppose a first data.frame is made with a single factor:\nD1&lt;-data.frame(\nScore1=factor(c(\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\",\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\"))\n)\nThe function table() returns counts, i.e. frequencies:\ntable(D1)\n#&gt; Score1\n#&gt; A B C D \n#&gt; 4 4 8 2\nLet’s add a second factor to this data-frame, we see the function table now returns a cross-tabulation, which we alsso call a contingency table (and to which we will later add significance tests)\nD2&lt;-D1\nD2[,\"Gender\"]&lt;-factor(c(\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\"))\ntable(D2)\n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 2 2\n#&gt;      B 2 2\n#&gt;      C 5 3\n#&gt;      D 1 1\nWhat happens if there are 3 and more factors?\nD3&lt;-D2\nD3[,\"Score2\"]&lt;-factor(c(\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\",\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\", \"A\",\"B\"))\nD3[,\"Country\"]=factor(c(\"LU\",\"DE\",\"DE\",\"DE\",\"DE\",\"FR\",\"DE\",\"LU\",\"DE\",\"BE\",\"DE\",\"FR\",\"BE\",\"FR\",\"LU\",\"LU\",\"FR\",\"DE\"))\n\ntable(D3)\n#&gt; , , Score2 = A, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 1 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 1\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 1 0\n#&gt; \n#&gt; , , Score2 = B, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 1 1\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 1\n#&gt; \n#&gt; , , Score2 = C, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 1 1\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 1 0\n#&gt;      C 2 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 1\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\nThe same cross-tabulation is undertaken (counts) but now for each level of the third one.",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html#case-of-2-or-several-factors-contingency-tables",
    "href": "034_cross_tabulation.html#case-of-2-or-several-factors-contingency-tables",
    "title": "13  Cross-tabulation",
    "section": "",
    "text": "13.1.1 Margins\nA table object is supposed to store frequencies. In many cases one will need to also compute vertical and horizontal totals. This is doen by done by applying the addmargins() function to a table object. By default both margins are added\n\naddmargins(table(D2))\n#&gt;       Gender\n#&gt; Score1  F  M Sum\n#&gt;    A    2  2   4\n#&gt;    B    2  2   4\n#&gt;    C    5  3   8\n#&gt;    D    1  1   2\n#&gt;    Sum 10  8  18\naddmargins(table(D2), margin = 1)\n#&gt;       Gender\n#&gt; Score1  F  M\n#&gt;    A    2  2\n#&gt;    B    2  2\n#&gt;    C    5  3\n#&gt;    D    1  1\n#&gt;    Sum 10  8\naddmargins(table(D2), margin = 2)\n#&gt;       Gender\n#&gt; Score1 F M Sum\n#&gt;      A 2 2   4\n#&gt;      B 2 2   4\n#&gt;      C 5 3   8\n#&gt;      D 1 1   2\n\nSimilar functions are rowSums and rowCols. However, although they are more general as applicable to any data.frame, not just tables, they don’t assemble the table with margins, they are simply vectors\n\nrowSums(table(D2))\n#&gt; A B C D \n#&gt; 4 4 8 2\ncolSums(table(D2))\n#&gt;  F  M \n#&gt; 10  8\nclass (rowSums)\n#&gt; [1] \"function\"\nclass (addmargins(table(D2)))\n#&gt; [1] \"table\"  \"matrix\" \"array\"\n\n\n\n13.1.2 Proportions\nIn many instances as well, one needs to compute proportions rather than counts:\nThe prop.table() function does it by deafutl across all the two dimensions of the table:\n\nprop.table(table(D2))\n#&gt;       Gender\n#&gt; Score1          F          M\n#&gt;      A 0.11111111 0.11111111\n#&gt;      B 0.11111111 0.11111111\n#&gt;      C 0.27777778 0.16666667\n#&gt;      D 0.05555556 0.05555556\naddmargins(prop.table(table(D2)))\n#&gt;       Gender\n#&gt; Score1          F          M        Sum\n#&gt;    A   0.11111111 0.11111111 0.22222222\n#&gt;    B   0.11111111 0.11111111 0.22222222\n#&gt;    C   0.27777778 0.16666667 0.44444444\n#&gt;    D   0.05555556 0.05555556 0.11111111\n#&gt;    Sum 0.55555556 0.44444444 1.00000000\n\nWhile you may need the proportions of columns or rows only:\n\nprop.table(table(D2),margin = 1)\n#&gt;       Gender\n#&gt; Score1     F     M\n#&gt;      A 0.500 0.500\n#&gt;      B 0.500 0.500\n#&gt;      C 0.625 0.375\n#&gt;      D 0.500 0.500\nprop.table(table(D2),margin = 2)\n#&gt;       Gender\n#&gt; Score1     F     M\n#&gt;      A 0.200 0.250\n#&gt;      B 0.200 0.250\n#&gt;      C 0.500 0.375\n#&gt;      D 0.100 0.125\n\nSince they are table output you can also add margins to the proportions and make sure how it sums to 1:\n\naddmargins(prop.table(table(D2)))\n#&gt;       Gender\n#&gt; Score1          F          M        Sum\n#&gt;    A   0.11111111 0.11111111 0.22222222\n#&gt;    B   0.11111111 0.11111111 0.22222222\n#&gt;    C   0.27777778 0.16666667 0.44444444\n#&gt;    D   0.05555556 0.05555556 0.11111111\n#&gt;    Sum 0.55555556 0.44444444 1.00000000\naddmargins(prop.table(table(D2),margin = 1))\n#&gt;       Gender\n#&gt; Score1     F     M   Sum\n#&gt;    A   0.500 0.500 1.000\n#&gt;    B   0.500 0.500 1.000\n#&gt;    C   0.625 0.375 1.000\n#&gt;    D   0.500 0.500 1.000\n#&gt;    Sum 2.125 1.875 4.000\naddmargins(prop.table(table(D2),margin = 2))\n#&gt;       Gender\n#&gt; Score1     F     M   Sum\n#&gt;    A   0.200 0.250 0.450\n#&gt;    B   0.200 0.250 0.450\n#&gt;    C   0.500 0.375 0.875\n#&gt;    D   0.100 0.125 0.225\n#&gt;    Sum 1.000 1.000 2.000",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html#case-of-a-numeric-vector",
    "href": "034_cross_tabulation.html#case-of-a-numeric-vector",
    "title": "13  Cross-tabulation",
    "section": "13.2 Case of a numeric vector",
    "text": "13.2 Case of a numeric vector\nLet’s add a numeric vector to our data-frame:\n\nD3[,\"Q\"]&lt;-c(rnorm(18, mean=12, sd=2))\n\nTable is useless in this case\n\ntable(D3[,c(\"Q\")])\n#&gt; \n#&gt; 8.29780201374557 8.44703715621246 8.90306580027945 9.64846397994692 \n#&gt;                1                1                1                1 \n#&gt; 9.76698746882718 10.3795913968578 10.7347142830984 10.8401289210835 \n#&gt;                1                1                1                1 \n#&gt; 11.0008071176547 11.7732188234035 11.8372918029665 12.4793368215967 \n#&gt;                1                1                1                1 \n#&gt; 12.5791406335374 13.0996643734177 13.6297320069097  13.660649107861 \n#&gt;                1                1                1                1 \n#&gt; 13.7142020113952 14.2955329263167 \n#&gt;                1                1\ntable(D3[,c(\"Country\", \"Q\")])\n#&gt;        Q\n#&gt; Country 8.29780201374557 8.44703715621246 8.90306580027945 9.64846397994692\n#&gt;      BE                0                0                0                0\n#&gt;      DE                0                0                1                0\n#&gt;      FR                1                1                0                1\n#&gt;      LU                0                0                0                0\n#&gt;        Q\n#&gt; Country 9.76698746882718 10.3795913968578 10.7347142830984 10.8401289210835\n#&gt;      BE                0                0                0                0\n#&gt;      DE                0                1                0                1\n#&gt;      FR                1                0                0                0\n#&gt;      LU                0                0                1                0\n#&gt;        Q\n#&gt; Country 11.0008071176547 11.7732188234035 11.8372918029665 12.4793368215967\n#&gt;      BE                0                1                0                0\n#&gt;      DE                1                0                1                1\n#&gt;      FR                0                0                0                0\n#&gt;      LU                0                0                0                0\n#&gt;        Q\n#&gt; Country 12.5791406335374 13.0996643734177 13.6297320069097 13.660649107861\n#&gt;      BE                0                0                0               0\n#&gt;      DE                1                0                0               0\n#&gt;      FR                0                0                0               0\n#&gt;      LU                0                1                1               1\n#&gt;        Q\n#&gt; Country 13.7142020113952 14.2955329263167\n#&gt;      BE                0                1\n#&gt;      DE                1                0\n#&gt;      FR                0                0\n#&gt;      LU                0                0\n\nAggregation of a numeric vector over factor levels require a certain statistcis to be computed, for example a center or dispersion indicator. We use the function aggregate for that purpose.\n\nmeanCountry&lt;-aggregate(D3[\"Q\"], by=D3[\"Country\"], FUN=\"mean\")\nsdCountry&lt;-aggregate(D3[\"Q\"], by=D3[\"Country\"], FUN=\"sd\")\ncbind(meanCountry, sdCountry)\n#&gt;   Country         Q Country         Q\n#&gt; 1      BE 13.034376      BE 1.7835454\n#&gt; 2      DE 11.466696      DE 1.5031032\n#&gt; 3      FR  9.040073      FR 0.7748552\n#&gt; 4      LU 12.781190      LU 1.3883996\n\naggregate() essentially splits the data into subsets, and computes the requested summary statistics (FUN) for each.\nor even across several factors using a formula and the data argument rather than by\n\nmeanCountry2&lt;-aggregate(Q ~ Country, data=D3, FUN=\"mean\")\nmeanCountry2\n#&gt;   Country         Q\n#&gt; 1      BE 13.034376\n#&gt; 2      DE 11.466696\n#&gt; 3      FR  9.040073\n#&gt; 4      LU 12.781190\nmeanCountryGender&lt;-aggregate(Q ~ Country + Gender, data=D3, FUN=\"mean\")\nmeanCountryGender\n#&gt;   Country Gender         Q\n#&gt; 1      BE      F 14.295533\n#&gt; 2      DE      F 12.130541\n#&gt; 3      FR      F  9.040073\n#&gt; 4      LU      F 13.380157\n#&gt; 5      BE      M 11.773219\n#&gt; 6      DE      M 11.068388\n#&gt; 7      LU      M 12.182223",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html",
    "href": "040_graphics_ggplot.html",
    "title": "14  ggplot graphics",
    "section": "",
    "text": "14.1 Library\nHere, we simply load the various packages that will allow us to manipulate data tables and create figures:\nIn ggplot2, a geom (short for “geometric object”) is a layer that defines how data points are visually represented in a plot. Each type of plot, such as points, lines, bars, etc., is created using a specific geom function. Here are some common examples:\nThese functions are added to a basic ggplot object using the + operator.\nExample of code :\nggplot() +\ngeom_…………(data, aes(x = x, y = y, fill = color)) +\nlabs(title = “title”, x = “abs”, y = “ord”) +\ntheme_bw()\nlibrary(ggplot2)\nlibrary(agridat) # oats data\nlibrary(questionr) # insee data\nlibrary(dplyr) \n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nlibrary(RColorBrewer)\nlibrary(readr) \nlibrary(viridis)\n#&gt; Loading required package: viridisLite\nlibrary(tidyr)\n\ndisplay.brewer.all()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#library",
    "href": "040_graphics_ggplot.html#library",
    "title": "14  ggplot graphics",
    "section": "",
    "text": "tidyverse is a package that contains several packages designed for data manipulation in R (stringr, dplyr, ggplot2)\nggplot2 which allows you to create complex graphics and observing data with visuals\n\n\n\ngeom_point(): for scatter plots.\ngeom_bar(): for bar charts.\ngeom_histogram(): for histograms.\ngeom_boxplot(): for box plots.",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#data",
    "href": "040_graphics_ggplot.html#data",
    "title": "14  ggplot graphics",
    "section": "14.2 Data",
    "text": "14.2 Data\nWe will present different code examples with 2 datasets to show that the methods are applicable to all datasets. Therefore, it is important to save our codes properly so that we can copy them for other data. Here are the two datasets :\nThe table below comes from the library ‘agridat’ shows the grain yield with 3 types of species. The ‘yield’ column represents the yield in quarter-pounds (lbs), and the ‘grain’ column represents the yield in pounds (lbs), (1lbs = 453 g). You can display the first lines of the table using the head() function.\n\nOat yield (grain, straw)\nNitrogen dose\nGenotype (variety)\nBlocks defined for experimentation\n\nThe file ‘rp2018’ contains, for all the municipalities in France in 2018, the following variables.\n\npop_tot (population)\netud (student)\ncadres (senior executive)\nlocataire (tenant)\n\n\n\ndata(yates.oats)\nhead(yates.oats)\n#&gt;   row col yield nitro        gen block grain straw\n#&gt; 1  16   3    80     0 GoldenRain    B1 20.00 28.00\n#&gt; 2  12   4    60     0 GoldenRain    B2 15.00 25.00\n#&gt; 3   3   3    89     0 GoldenRain    B3 22.25 40.50\n#&gt; 4  14   1   117     0 GoldenRain    B4 29.25 28.75\n#&gt; 5   8   2    64     0 GoldenRain    B5 16.00 32.00\n#&gt; 6   5   2    70     0 GoldenRain    B6 17.50 27.25\n\ndata(rp2018)\nhead(rp2018)\n#&gt; # A tibble: 6 × 62\n#&gt;   code_insee commune     code_region region code_departement departement pop_tot\n#&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 01004      Ambérieu-e… 84          Auver… 01               Ain          14204 \n#&gt; 2 01007      Ambronay    84          Auver… 01               Ain           2763 \n#&gt; 3 01014      Arbent      84          Auver… 01               Ain           3356 \n#&gt; 4 01024      Attignat    84          Auver… 01               Ain           3196 \n#&gt; 5 01025      Bâgé-Domma… 84          Auver… 01               Ain           4078.\n#&gt; 6 01027      Balan       84          Auver… 01               Ain           2513 \n#&gt; # ℹ 55 more variables: pop_cl &lt;fct&gt;, pop_0_14 &lt;dbl&gt;, pop_15_29 &lt;dbl&gt;,\n#&gt; #   pop_18_24 &lt;dbl&gt;, pop_75p &lt;dbl&gt;, pop_femmes &lt;dbl&gt;, pop_act_15p &lt;dbl&gt;,\n#&gt; #   pop_chom &lt;dbl&gt;, pop_agric &lt;dbl&gt;, pop_indep &lt;dbl&gt;, pop_cadres &lt;dbl&gt;,\n#&gt; #   pop_interm &lt;dbl&gt;, pop_empl &lt;dbl&gt;, pop_ouvr &lt;dbl&gt;, pop_scol_18_24 &lt;dbl&gt;,\n#&gt; #   pop_non_scol_15p &lt;dbl&gt;, pop_dipl_aucun &lt;dbl&gt;, pop_dipl_bepc &lt;dbl&gt;,\n#&gt; #   pop_dipl_capbep &lt;dbl&gt;, pop_dipl_bac &lt;dbl&gt;, pop_dipl_sup2 &lt;dbl&gt;,\n#&gt; #   pop_dipl_sup34 &lt;dbl&gt;, pop_dipl_sup &lt;dbl&gt;, log_rp &lt;dbl&gt;, …",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#basic-plot---ggplot",
    "href": "040_graphics_ggplot.html#basic-plot---ggplot",
    "title": "14  ggplot graphics",
    "section": "14.3 Basic plot - ggplot",
    "text": "14.3 Basic plot - ggplot\nHere, we indicate to R that we want to plot a scatterplot, so we need to specify a “x” and “y” for our graph. Here the example of the basic R plot, and the ggplot plot.\n\n\nplot(yates.oats$grain, yates.oats$straw) # plot(x, y)\n\n\n\n\n\n\n\n\nggplot() + \n  geom_point(data = yates.oats, aes(x = grain, y = straw))",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#histogram",
    "href": "040_graphics_ggplot.html#histogram",
    "title": "14  ggplot graphics",
    "section": "14.4 Histogram",
    "text": "14.4 Histogram\nUsually, when you’re interested in a variable, you look at its distribution. First, a classic basic histogram: you specify the data table in the ggplot() function, and the components of the table that will be used to create the plot in the aes() function. For example, we need to specify which column of the table will be represented on the x-axis, on the y-axis, or which column will allow us to color certain elements or modify the shape of the points, for instance. We then add successive layers to the graph using the + sign to achieve the desired rendering.\nHere, we indicate to R that we want to plot a histogram, so we don’t need to specify a “y” for our graph:\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThe problem is that the figure is quite unattractive, isn’t it? So, we will modify it with options: fill for the bar fill, color for the border, and alpha for transparency. The theme allows us to change the overall appearance of the figure. Here you can see all theme : https://ggplot2.tidyverse.org/reference/ggtheme.html\nBelow, geom_histogram() is used to plot a histogram.\n\nWe can also modify the number of classes represented by the histogram with the bins option.\nTo modify the size of the classes represented by the histogram, we use the binwidth option.\n\nTry both codes below:\n\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_histogram(fill = \"lightblue\",\n                 color = \"black\",\n                 bins = 10, # bar numbers (number of classes)\n                 alpha = 0.5) + \n  labs(title = \"Grain yield\", \n       x = \"Grain (lbs)\", \n       y = \"Count\") + \n  theme_bw() \n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud)) +\n  geom_histogram(fill = \"red\",\n                 color = \"black\",\n                 binwidth = 5, # reduce or increase the width of the bar (number of classes)\n                 alpha = 0.5) + \n  labs(title = \"Student proportion in France per municipalities\", \n       x = \"Student (%)\", \n       y = \"Count\") + \n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#density",
    "href": "040_graphics_ggplot.html#density",
    "title": "14  ggplot graphics",
    "section": "14.5 Density",
    "text": "14.5 Density\ngeom_density() is used to plot the density curve.\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_density(fill = \"lightblue\",\n                 color = \"black\",\n                 alpha = 0.5) +\n  labs(title = \"Grain yield\", \n       x = \"lbs\", \n       y = \"Count\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud)) +\n  geom_density(fill = \"red\",\n                 color = \"black\",\n                 alpha = 0.5) + \n  labs(title = \"Student proportion in France per municipalities\", \n       x = \"Student (%)\", \n       y = \"Count\") + \n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#qqplot",
    "href": "040_graphics_ggplot.html#qqplot",
    "title": "14  ggplot graphics",
    "section": "14.6 QQplot",
    "text": "14.6 QQplot\nA QQ plot indicates a normal distribution when the points closely follow a straight line, suggesting that the quantiles of the data match the quantiles of a normal distribution. Here is a good QQ-plot reference : https://www.tjmahr.com/quantile-quantile-plots-from-scratch/\n\n\nggplot(yates.oats, aes(sample = grain)) +\n  geom_qq(color = \"blue\") +\n  labs(x = \"yield (lbs)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(sample = etud)) +\n  geom_qq(color = \"red\") +\n  labs(x = \"Student (%)\") +\n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#boxplot",
    "href": "040_graphics_ggplot.html#boxplot",
    "title": "14  ggplot graphics",
    "section": "14.7 Boxplot",
    "text": "14.7 Boxplot\nLet’s look at the yields in the form of boxplots for each variety. To do this, we use the geom_boxplot() function. Please note that the color of boxplots and barplots is managed not with color but with fill (points and lines are managed with ‘color’). Other palettes are available at the following address: https://bookdown.org/rdpeng/exdata/plotting-and-color-in-r.html\n\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") + \n  scale_fill_brewer(palette = \"Set1\") + # color\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = rp2018, aes(x = code_region, y = etud, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Proportion of students per region\", x = \"Region\", y = \"Student(%)\", fill = \"Region\") + \n  scale_fill_brewer(palette = \"Set1\") + # color\n  theme_bw() \n#&gt; Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9\n#&gt; Returning the palette you asked for with that many colors\n\n\n\n\n\n\n\n\nThe colorRampPalette() function in manner similar to colorRamp((), however the function that it returns gives you a fixed number of colors that interpolate the palette. Again we have a function pal() that was returned by colorRampPalette(), this time interpolating a palette containing the colors red and yellow. But now, the pal() function takes an integer argument specifing the number of interpolated colors to return.\nThe reference for the packages legocolors : https://cran.r-project.org/web/packages/legocolors/readme/README.html\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") + \n  scale_fill_manual(values = c(\"black\", \"red\", \"yellow\"),\n                    limits = c(\"GoldenRain\", \"Marvellous\", \"Victory\")) + # color\n  theme_bw()\n\n\n\n\n\n\n\n\npal1 &lt;- colorRampPalette(c(\"lightblue\", \"purple\", \"red\", \"green\"))\npal2 &lt;- colorRampPalette(brewer.pal(9, \"Set1\")) # 9 to use all colors\n\nlibrary(legocolors)\n#&gt; Warning: package 'legocolors' was built under R version 4.4.1\npal3 &lt;- colorRampPalette(legoCols$hex[2:13])\n\n# length(rp2018$code_region)\n# length(unique(rp2018$code_region))\n\nggplot(data = rp2018, aes(x = code_region, y = etud, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Proportion of students per region\", x = \"Region\", y = \"Student(%)\", fill = \"Region\") + \n  scale_fill_manual(values = pal3(17)) + # you can change the pal() here (see above)\n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#boxplot-with-point",
    "href": "040_graphics_ggplot.html#boxplot-with-point",
    "title": "14  ggplot graphics",
    "section": "14.8 Boxplot with point",
    "text": "14.8 Boxplot with point\nTo display points on a boxplot, you can use the geom_jitter() function, which displays the points by offsetting them from each other to avoid overlap. When displaying the points, you should add the 'outlier.shape = NA' option in the geom_boxplot()function to prevent displaying the same points twice.\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot(outlier.shape = NA) + #exclude point outlier\n  geom_jitter(width = 0.1, color = \"black\") + # jitter allows to arrange the place of the point comparaing to the geom_point\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#multi-boxplot",
    "href": "040_graphics_ggplot.html#multi-boxplot",
    "title": "14  ggplot graphics",
    "section": "14.9 Multi-boxplot",
    "text": "14.9 Multi-boxplot\nWe could also put the blocks in x, and color according to the varieties. Or conversely, the varieties on the x-axis and the blocks in color.\n\n\nggplot(yates.oats, aes(x = block, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Blocks\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = block)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Blocks\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor the ‘rp2018’ data, we can create categories based on regions or departments; however, this still results in a large number. In the example below, I still made the boxplot graph but removed the legend. This is an example of data observation to see a marked trend at first glance, but it is rarely used for reports.\n\nggplot(rp2018, aes(x = code_region, y = etud, fill = code_departement)) +\n  geom_boxplot(outlier.shape = NA) + \n  labs(title = \"Department in the region\", x = \"Region\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# 44 Grand-Est, 76 Occitanie, 53 Bretagne, 28 Normandie\n\nrp2018_reg44 &lt;- rp2018[rp2018$code_region == \"44\",]\n\ng44 &lt;- ggplot(rp2018_reg44, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Grand Est\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg76 &lt;- rp2018[rp2018$code_region == \"76\",]\n\ng76 &lt;- ggplot(rp2018_reg76, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Occitanie\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg53 &lt;- rp2018[rp2018$code_region == \"53\",]\n\ng53 &lt;- ggplot(rp2018_reg53, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Bretagne\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg28 &lt;- rp2018[rp2018$code_region == \"28\",]\n\ng28 &lt;- ggplot(rp2018_reg28, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Normandie\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nlibrary(gridExtra)\n#&gt; \n#&gt; Attaching package: 'gridExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\ngrid.arrange(g44, g76, g53, g28,\n          nrow = 2, ncol = 2)",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#violinplot",
    "href": "040_graphics_ggplot.html#violinplot",
    "title": "14  ggplot graphics",
    "section": "14.10 Violinplot",
    "text": "14.10 Violinplot\nOne last one, the violin plot, which shows us the distribution curve. We use the geom_violin() function for this. On it, we could display the mean and the standard deviation, for example. For this, we use the stat_summary() function, in which we specify the function used (fun.data), the number of standard deviations represented by the error bars (fun.args), the geometry of the representation (geom), and the color, of course.\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_violin() +\n  geom_jitter(width = .1) + \n    stat_summary(fun.data = \"mean_sdl\", # mean representation + standard deviation\n               fun.args = list(mult = 1), # number of standard deviation\n               geom = \"pointrange\", # geometry \n               color = \"grey\") +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#scatterplot",
    "href": "040_graphics_ggplot.html#scatterplot",
    "title": "14  ggplot graphics",
    "section": "14.11 Scatterplot",
    "text": "14.11 Scatterplot\nWe can also create a scatterplot (a graph with points) between straw yield and grain yield, or etud and cadres for example. The geom_point() option allows us to display points (with continuous data, for instance).\nThe scale_color_gradient() here allows us to directly detect the numerical values of the variables and produce a continuous color palette for data visualization. This allows us to integrate another variable on the points of the graph, and we can also apply it to the size to compare the x-axis and y-axis of the data to find the effect of other variables. For example, we can see that the higher the proportion of students, the more apartment rentals we observe. The scale_color_gradientn()here allows to\n\nggplot(yates.oats, aes(x = straw, y = grain, color = yield)) + # the color is set with 'color' \n  geom_point() + \n  scale_color_gradient(\"yield\", low = \"white\", high = \"blue\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nggplot(yates.oats, aes(x = straw, y = grain, color = yield)) +  \n  geom_point() + \n  scale_color_gradientn(colors = c(\"white\", \"lightblue\", \"blue\", \"black\"), \n                        limits = c(min(yates.oats$yield), max(yates.oats$yield)),\n                        values = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud, y = cadres, color = locataire, size = pop_tot)) + \n  geom_point() + \n  scale_color_gradient(\"locataire\", low = \"white\", high = \"blue\") +\n  theme_classic()",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#scatterplot-and-regression-line",
    "href": "040_graphics_ggplot.html#scatterplot-and-regression-line",
    "title": "14  ggplot graphics",
    "section": "14.12 Scatterplot and regression line",
    "text": "14.12 Scatterplot and regression line\nWe can also try to put the color of the points according to the variable (to color points, use color), and we add regression lines with the geom_smooth() for each genotype, also for the students and the cadres. The scale_color_brewer() function defines the color palette to color the points.\n\nggplot(yates.oats, aes(x = straw, y = grain, color = gen)) +\n    geom_point(shape = 1) + # ajust the size and choose the shape\n    geom_smooth(method = \"lm\", se = F) + #regression line\n    scale_fill_viridis() + # ajust a linear color with numerics values\n    theme_bw() +\n    labs(title = \"Grain vs Straw\", x = \"Straw yield (lbs)\", y = \"Grain yield (lbs)\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud, y = cadres, color = locataire)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) +\n  scale_color_gradient(\"locataire\", low = \"white\", high = \"blue\") +\n  theme_classic()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; Warning: The following aesthetics were dropped during statistical transformation:\n#&gt; colour.\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#barplot",
    "href": "040_graphics_ggplot.html#barplot",
    "title": "14  ggplot graphics",
    "section": "14.13 Barplot",
    "text": "14.13 Barplot\nHere’s a quick example to understand the proportion of forest area in each canton of Luxembourg:\nFor instance, you can create a bar chart where the x-axis represents the cantons and the y-axis represents the proportion of forest area. Using a function like geom_bar() to create a barplot.\nTo specify that we want the height of the barplot to represent the value indicated in our data frame, we need to add stat = \"identity in the geom_bar() function. Normally, geom_bar() counts the number of cases at each x position (as an histogram), but with stat = \"identity\", it uses the values in the data directly. The ‘width’ option allows you to adjust the width of the barplot.\n\nforest_area &lt;- read.csv2(\"data/statec/forest_area_canton.csv\", sep = \",\") \n\nggplot(forest_area, aes(x = GEO..Géographie, y = OBS_VALUE, fill = GEO..Géographie )) +\n  geom_bar(stat = \"identity\",\n           width = 0.5) + # width of the bar\n  theme_bw() +\n  labs(title = \"Area of the afforestation rate\", x = \"Cantons\", y = \"Afforestation rate\") +\n  scale_fill_manual(values = pal2(17)) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n\nggplot(forest_area, aes(x = GEO..Géographie, y = OBS_VALUE, fill = GEO..Géographie )) +\n  geom_bar(stat = \"identity\",\n           width = 0.5) + # width of the bar\n  theme_bw() +\n  labs(title = \"Area of the afforestation rate\", x = \"Cantons\", y = \"Afforestation rate\") +\n  scale_fill_manual(values = pal2(17)) +\n  theme(legend.position = \"none\") +\n  coord_flip() # allow to keep the real name of the canton",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#to-help-you",
    "href": "040_graphics_ggplot.html#to-help-you",
    "title": "14  ggplot graphics",
    "section": "14.14 To help you",
    "text": "14.14 To help you\ndata-to-viz\nr-graph-gallery\ncookbook-r\n\nData of the classes :\nlibrary(agridat) -&gt; yates.oats\nlibrary(questionr) -&gt; rp2018\nurl air bnb data -&gt; githubusercontent.com\nForest area by canton and commune -&gt; lustat.statec.lu",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "040_graphics_ggplot.html#your-turn",
    "href": "040_graphics_ggplot.html#your-turn",
    "title": "14  ggplot graphics",
    "section": "14.15 Your turn!",
    "text": "14.15 Your turn!\n\nGo take the data on land use proportions for some cities in the ouest of Europe : land_use_EU_ouest.rds\nCreate a few plots with a ‘ggplot2’ style.\n\nThe data comes from the urban atlas 2018 shows the proportion and the area of different classes of land use per city.\n\np_…. is the proportion in %\narea_….. is the surface in m²\npop_tot of the city\nperimeter_tot and area_tot of the city",
    "crumbs": [
      "Part IV - Graphics with ggplot",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "051_distributions_unif_normal.html",
    "href": "051_distributions_unif_normal.html",
    "title": "15  Uniform and normal distributions",
    "section": "",
    "text": "15.1 The uniform distribution\nThe uniform distribution is one of the simplest distribution of a continuous (ratio or interval) variable. Although it is rare in practice that each value along a continuum has the same chance of occurring than all others, it is a base distribution to know of and the source of generating random numbers.\nEach value has the same probability of occurrence. It is a simple case through which we show how we can compute density, probabilities and cumulative distributions functions in R.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "051_distributions_unif_normal.html#the-uniform-distribution",
    "href": "051_distributions_unif_normal.html#the-uniform-distribution",
    "title": "15  Uniform and normal distributions",
    "section": "",
    "text": "15.1.1 Definition and key characteristics\nA random variable \\(X \\in [a;b]\\) following an uniform distribution, can be expressed as: \\(X \\sim U[a;b]\\), with expectation \\(E(X) = \\dfrac{a+b}{2}\\) and variance \\(Var(X) = \\dfrac{(b-a)^2}{12}\\).\nMath note: The denominator, 12, of the variance may seem a little surprising, but results from an integration. Indeed, the variance is the expected value of the squared deviations to the mean. You thus integrate the squared difference between each point \\(X\\) and the mean, which itself is \\((a+b)/2\\), over the interval \\([a, b]\\), that is integrating \\(((X-(a+b)/2)^2)/(b-a)\\), leading to the above defined \\(Var(X)\\).\nTo generate empirically a uniform function, we feed the runif() function with a number of observations and a range, i.e. a minimum value (default \\(a=0\\)) and a maximum value (default \\(b=1\\)).\n\nset.seed(101)\nu&lt;-runif(1000, min=10, max=30)\nhist(u)\n\n\n\n\n\n\n\n\nThe histogram shows that each value (interval of values) is similarly frequent.\n\n\n15.1.2 Density\nInstead of a histogram, or on top of it, we often plot densities, which is a smoother representation than the bars. The computation uses a local density (kernel density i.e. using a bandwidth instead of the strict silos of the histogram). Beyond the visual, the notion of density is important because, once it is scaled so that the entire area under the curve sums to 1, it is interpretable as a probability.\nIn R in order to compute the density for an empirical distribution, we use the density() function and use its returned values for plotting. Note that we don’t have frequencies anymore along the y-axis, but values below 1, i.e. probabilities. This is also why plotting the density on top of a histogram requires further fine tuning (which we leave out for now).\n\ndensity(u)\n#&gt; \n#&gt; Call:\n#&gt;  density.default(x = u)\n#&gt; \n#&gt; Data: u (1000 obs.); Bandwidth 'bw' = 1.316\n#&gt; \n#&gt;        x               y            \n#&gt;  Min.   : 6.06   Min.   :7.237e-05  \n#&gt;  1st Qu.:13.03   1st Qu.:1.876e-02  \n#&gt;  Median :20.00   Median :4.731e-02  \n#&gt;  Mean   :20.00   Mean   :3.581e-02  \n#&gt;  3rd Qu.:26.96   3rd Qu.:5.010e-02  \n#&gt;  Max.   :33.93   Max.   :5.278e-02\nplot(density(u))\n\n\n\n\n\n\n\n\nI order to obtaine the corresponding continuous - not numerically simulated - density, using the same definition (i.e. min=10 and max=30) and a similar range for display (5 to 35), we can use dunif() within the curve() function:\n\ncurve(dunif(x,min=10,max=30),5,35)\n\n\n\n\n\n\n\n\nwhere we see more clearly that the probability to obtain a particular value with a uniform distribution is constant over the defined range and is zero otherwise.\nThe previous plot based on a numerical empirical generation runif() is of course less regular, and there was some smoothing at the borders of the graph due to the density being computed within a kernel (bandwidth). Yet, the kernel is necessary in practice for having some increment. In theory, the density can be defined over a point, i.e. for an infinitesimal delta of x (\\(\\delta x\\)), but in practice you need a discrete interval (\\(dx\\)), hence the different ‘look’ of our two density plots: the continuous theoretical one using d...() and the numerical empirical one using density().\nMathematically, the density of a uniform distribution is given by (see help(dunif)) :\n\\[\n\\begin{align}\nf_X(x) &= \\dfrac{1}{b-a} &\\text{ for } a \\le x \\le b \\\\\nf_X(x) &= 0 &\\text{ otherwise}\n\\end{align}\n\\] where \\(b\\) is the maximum and \\(a\\) the minimum whereby the distribution is defined.\nIn our example we now see why the constant was at 0.05, i.e. (\\(1/(30-10)\\))\nAnd can check it for any point x fed into dunif()\n\ndunif(c(2, 10, 15, 20, 30, 55),min=10, max=30)\n#&gt; [1] 0.00 0.05 0.05 0.05 0.05 0.00\n\n\n\n15.1.3 Cumulated probabilities\nFor any continuous distribution, it is interesting to accumulate the probabilities along the x values so that this cumulative sum indicates the probability of randomly drawing a number that would fall below any given value. This is called the cumulative density function, aka cdf.\nSimilar to the density, there is both a numerical empirical way and a continuous theoretical way to get the cumulative density function in R, using respectively the ecdf() function or the p...() function corresponding to the distribution of interest.\nWith our empirically generated (sample) uniform distribution \\(u\\), we compute the empirical cumulative distribution function (ecdf) as follows:\n\necdf(u)\n#&gt; Empirical CDF \n#&gt; Call: ecdf(u)\n#&gt;  x[1:1000] = 10.007, 10.015, 10.048,  ..., 29.977, 29.986\nplot(ecdf(u))\n\n\n\n\n\n\n\n\nWe can see that it is (almost) a straight line. Its slope is of course the density we have seen above. density() is the derivative of the ecdf() and the ecdf() the integral (cumulative sum) of density(). For every increment of x, we increase the probability of drawing a number below x by 0.05, up until the max (30) is reached.\nInstead of simulating numbers or using an empirical sample, we can use the theoretical punif() function in this case to obtain the theoretical cumulative density function corresponding to our parameters.\n\ncurve(punif(x,min=10, max=30),5,35)\n\n\n\n\n\n\n\n\nWe see it is the theoretical continuous version of ecdf() applied to our vector \\(u\\).\nMathematically, the cumulative distribution function of the uniform is defined by: \\[\\begin{align}\nF_X(x) &= 0 &\\text{ for }& x &lt; a \\\\\nF_X(x) &= \\dfrac{x-a}{b-a} &\\text{ for }& a \\le x \\le b \\\\\nF_X(x) &= 1 &\\text{ for }& x &gt; b\n\\end{align}\n\\]\nInstead of using the p...() function (similar for d...()) with a general x variable, we can supply a quantile (or a set of quantiles), in order to obtain the probability of drawing a number below this quantile.\nWith the uniform distribution and punif() it is very straightforward because the cumulative probability simply is the quantile as shown below\n\npunif(q=c(0.01,0.25,0.5,0.75,0.99))\n#&gt; [1] 0.01 0.25 0.50 0.75 0.99\npunif(min=50, max=200, q=c(100)) #the probability of drawing a number below 100 knowing the min and max are 50 and 200 and the distribution uniform is 1/3 (the range being 150 and 100 being 50 beyond the max)\n#&gt; [1] 0.3333333\n\nThe last of the 4 R functions for distributions is the reverse of p...(), i.e. q...(), which provides the corresponding quantile for any given probability \\(p\\). It is again quite straightforward for a uniform distribution defined over the range 0 to 1 since the quantile is then the probability, and proportionally within the defined range for other uniform distributions.\n\ncurve(qunif(x,min=1000,max=10000))\n\n\n\n\n\n\n\nqunif(p=0.77,min=1000,max=10000)\n#&gt; [1] 7930",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "051_distributions_unif_normal.html#the-normal-distribution",
    "href": "051_distributions_unif_normal.html#the-normal-distribution",
    "title": "15  Uniform and normal distributions",
    "section": "15.2 The Normal distribution",
    "text": "15.2 The Normal distribution\nThe Normal or Gaussian distribution is the workhorse of statistics and the most used distribution. We have seen its general bell shape earlier.\n\n15.2.1 Definition and key characteristics:\nA normal distribution is entirely defined by its mean and standard deviation, which we thus need to provide for generating examples.\nA normal distribution is thus denoted by \\[X \\sim N(\\mu, \\sigma^2) \\ \\  \\  \\forall X \\in R\\]\nwhere \\(\\mu\\) and \\(\\sigma^2\\) are the real and unknown mean and variance of the studied population. It is defined over the entire set of real numbers R.\nLet’s generate a normal distribution and observe its histogram.\n\nset.seed(101)\nN&lt;-rnorm(100000,mean=5,sd=1)\nhist(N, breaks = 100)\n\n\n\n\n\n\n\nmean(N)\n#&gt; [1] 5.002881\nmedian(N)\n#&gt; [1] 5.00018\nsd(N)\n#&gt; [1] 0.9947701\n\n\nSymmetry\nWith a numeric example the characteristics are not exact, but the mean and standard deviations are close to those requested. We also see it is a symmetric distribution and the mean thus equals the median (approximately here). In other words half of the values are below the mean and the other half above.\nSymmetry also means that the skewness (3rd moment) is close to zero. Note that its 4th moment (Kurtosis) is theoretically 3, but R takes out this value in its computation so the kurtosis of the normal distribution is 0, which is used as a reference. Again it is approximately given this is only a numerical example of it.\n\nmean(N)\n#&gt; [1] 5.002881\nmedian(N)\n#&gt; [1] 5.00018\ne1071::skewness(N)\n#&gt; [1] -0.001284229\ne1071::kurtosis(N)\n#&gt; [1] 0.004644777\n\nWe can look at the impact of generating a smaller sample, to show how means, medians, skewness or kurtosis and the histogram vary quite a bit from the expected when the size of the sample decreases:\n\nN10000&lt;-rnorm(10000,mean=5,sd=1)\nN1000&lt;-rnorm(1000,mean=5,sd=1)\nN100&lt;-rnorm(100,mean=5,sd=1)\nN10&lt;-rnorm(10,mean=5,sd=1)\nhist(N10000, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N1000, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N100, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N10, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\n\n\n\nQuantiles and standard deviations\nA key feature of the normal distribution is that approximately 68% of its values fall within one standard deviation from the mean (both side as it is symmetrical), 95% within two standard deviations, and 99.7% within three standard deviations. This characteristic is heavily used to make probability tests.\n\n\n\n15.2.2 Density\nWe know compute the empirical density for our sample, showing the previous histogram with large sample size was indeed bell-shaped (but not so much the small sample examples)\n\ndensity(N)\n#&gt; \n#&gt; Call:\n#&gt;  density.default(x = N)\n#&gt; \n#&gt; Data: N (100000 obs.);   Bandwidth 'bw' = 0.08953\n#&gt; \n#&gt;        x                y            \n#&gt;  Min.   :0.5144   Min.   :0.0000007  \n#&gt;  1st Qu.:2.8273   1st Qu.:0.0011045  \n#&gt;  Median :5.1403   Median :0.0258991  \n#&gt;  Mean   :5.1403   Mean   :0.1078749  \n#&gt;  3rd Qu.:7.4533   3rd Qu.:0.2048571  \n#&gt;  Max.   :9.7663   Max.   :0.3981291\nplot(density(N)) #The integral of the density being 1, overlaying on top of the histogram doesn't work because it depends on the heights of the bar which itself depends on the number of bars. See Crawley, p215 for such a manipulation or later with ggplot\n\n\n\n\n\n\n\n\n\nplot(density(N10000), main=\"Varying sample size\", col=rgb(1, 0.5, 1, 0.5), lwd=5,xlab=\"x\")\nlines(density(N1000), col=rgb(1, 0.5, 1, 0.5),lwd=3)\nlines(density(N100), col=rgb(1, 0.5, 1, 0.5), lwd=2)\nlines(density(N10), col=rgb(1, 0.5, 1, 0.5), lwd=1)\n\n\n\n\n\n\n\n\nTo obtain the corresponding theoretical function of the density, using the same mean and standard deviation, and along the same range of x values (0 to 10) for the graph, we can use dnorm() instead of the empirical function density(). Observe also that the density of a normal distribution at its peak, i.e. at the mean or median, is around 0.39.\n\ncurve(dnorm(x,mean=5,sd=1),0,10)\n\n\n\n\n\n\n\ndnorm(5, mean=5,sd=1)\n#&gt; [1] 0.3989423\n\nMathematically, the density of a norma distribution is given by the following, which is used within dnorm() (see help(dnorm)) :\n\\[f(x) = \\dfrac {1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp{(- \\dfrac{1}{2} \\dfrac{(x-\\mu)^2}{\\sigma^2})}\\]\nIn the following, we play with the two defining parameters, mean and standard deviation, to show how each impact the shape of the distribution while keeping the second parameter constant:\n\ncurve(dnorm(x,mean=0,sd=1),-5,5, lty=1)\ncurve(dnorm(x,mean=-2,sd=1),-5,5, lty=2, add=TRUE)\ncurve(dnorm(x,mean=1,sd=1),-5,5, lty=3, add=TRUE)\n\n\n\n\n\n\n\n\ncurve(dnorm(x,mean=0,sd=1),-5,5, lty=1, col=\"blue\")\ncurve(dnorm(x,mean=0,sd=0.7),-5,5, lty=2, col=\"blue\", add=TRUE)\ncurve(dnorm(x,mean=0,sd=2),-5,5, lty=3, col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\n\nThe general shape is not influenced by the mean as the distribution is simply translated along x. When the standard deviation increases, as expected, the distribution is flatter but still around the same mean.\nIf we generate two new (large) samples with a different standard deviation, we see the flattening. However, the shape is still a bell shape and the kurtosis will not pick the difference!. The peakness referred to by the kurtosis is one that is relative to the corresponding normal distribution (thus knowing its variance).\n\nset.seed(101)\nZ&lt;-rnorm(1000000,mean=0,sd=1)\nFlat&lt;-rnorm(1000000,mean=0,sd=3)\nplot(density(Z))\nlines(density(Flat))\n\n\n\n\n\n\n\ne1071::kurtosis(Z)\n#&gt; [1] -0.005819773\ne1071::kurtosis(Flat)\n#&gt; [1] -0.002299496\n\nWe know explore graphically and with the function dnorm() the other property of the normal distribution by which we know that in a normal distribution, the range of values situated - + - 1 standard deviation from the mean represents 68% of the data - + - 2 standard deviations from the mean represents 95% of the data - + - 3 standard deviations from the mean represents 99.7% of the data\n\ncurve(dnorm(x, 0, 1), col = 'green', lwd = 8, xlim = c(-3, 3), main = 'Density function of X ~ N(0,1)\\n Intervals', ylab = 'f(x)')\ncurve(dnorm(x, 0, 1), add = TRUE, col = 'gold', lwd = 5, xlim = c(-2, 2))\ncurve(dnorm(x, 0, 1), add = TRUE, col = \"red\", lwd = 2, xlim = c(-1, 1))\nabline(v = 0, col = \"grey\", lty = 1)\nabline(v = c(-1, 1), col = \"red\", lty = 3)\nabline(v = c(-2, 2), col = \"gold\", lty = 3)\nabline(v = c(-3, 3), col = \"green\", lty = 3)\nlegend(\"topleft\", lwd = c(2,5,8),\n       legend = c(\"68% in [ mu +/- sigma ]\", \n                  \"95% in [ mu +/- 2*sigma ]\", \n                  \"99.7% in [ mu +/- 3*sigma ]\"),\n       col = c(\"red\", 'gold', 'green'))\n\n\n\n\n\n\n\n\n\n\ncurve(dnorm(x, 0, 1), col = 'green', lwd = 8, xlim = c(-2.58, 2.58), main = 'Density function of X ~ N(0,1)\\n Intervals', ylab = 'f(x)')\ncurve(dnorm(x, 0, 1), add = TRUE, col = 'gold', lwd = 5, xlim = c(-1.96, 1.96))\ncurve(dnorm(x, 0, 1), add = TRUE, col = \"red\", lwd = 2, xlim = c(-1.645, 1.645))\nabline(v = 0, col = \"grey\", lty = 1)\nabline(v = c(-1.645, 1.645), col = \"red\", lty = 3)\nabline(v = c(-1.96, 1.96), col = 'gold', lty = 3)\nabline(v = c(-2.58, 2.58), col = 'green', lty = 3)\nlegend(\"topleft\", lty = 1, lwd = c(2,5,8),\n       legend = c(\"90% in [ +/- 1.645 ]\", \n                  \"95% in [ +/- 1.96 ]\", \n                  \"99% in [ +/- 2.58 ]\"),\n       col = c(\"red\", 'gold', 'green'))\n\n\n\n\n\n\n\n\n\n\n15.2.3 Cumulated probabilities\nSimilar to the case of the uniform distribution, we can also look at the cumulative probability. Theoretically we use pnorm(), which it the equivalent of the uniform punif(). Empirically we can use the ecdf() function again since it makes no assumption on the distribution (for it is used to explore empirical material).\n\ncurve(pnorm(x),-4,4) #theoretical (unit normal)\n\n\n\n\n\n\n\n\nN&lt;-rnorm(100)\nplot(ecdf(N)) #sample\n\n\n\n\n\n\n\n\nWe see that the bell shape of the density (from dnorm()) is the derivative of an S-shaped function. The probability of drawing a number below a certain value is not constantly increasing as in the case of the uniform distribution. Rather, the probability grows slowly for lower values, then very quickly around the mean where the mass of data is found, then slows downs and plateau again for higher values.\nMathematically, the cumulative distribution function of the standard normal distribution is the integral of the probability distribution \\(f(x)\\) defined earlier. It is often denoted by \\(\\Phi(x)\\) but has no closed solution and thus statistical software compute values numerically.\n\ncurve(pnorm(x, 0, 1),-6,6, type='l', col=2, ylab = 'F(x)', \n     main = 'Cumulative Distribution Function of N(mu ; 1)')\ncurve(pnorm(x, 2, 1), col='darkorange', add=TRUE)\ncurve(pnorm(x, 4, 1), col='gold', add=TRUE)\nlegend(\"topleft\", lty = 1,\n       legend = c(\"mu = 0\", \"mu = 2\", \"mu = 4\"),\n       col = c(2, 'darkorange', 'gold'))\n\n\n\n\n\n\n\n\n\ncurve(pnorm(x, 0, 1),-6,6, type='l', col=2, ylab = 'F(x)', \n     main = 'Cumulative Distribution Function of N(0 ; sigma^2)')\ncurve(pnorm(x, 0, 2), col='purple', add=TRUE)\ncurve(pnorm(x, 0, 4), col='blue', add=TRUE)\nlegend(\"topleft\", lty = 1,\n       legend = c(\"sigma = 1\", \"sigma = 2\", \"sigma = 4\"),\n       col = c(2, 'purple', 'blue'))\n\n\n\n\n\n\n\n\nWe can also use pnorm()to verify the key property of the normal distribution, that 68 % of the values fall in between -1 and +1 standard deviation from the mean, and respectively 95% and 98.7% for 2 and 3 standard deviations.\n\n1-(pnorm(-1)+(1-pnorm(1))) #1 minus the sum of the probability of being below -1 and being above 1)\n#&gt; [1] 0.6826895\npnorm(1)-pnorm(-1) #or simply :-) \n#&gt; [1] 0.6826895\n\n1-(pnorm(-2)+(1-pnorm(2))) \n#&gt; [1] 0.9544997\n\n1-(pnorm(-3)+(1-pnorm(3))) \n#&gt; [1] 0.9973002\n\nOf course pnorm()can be interrogated for any normal distribution, i.e. with other means and standard deviations. For example, suppose we know the mean of GDP /capita for all countries in Europe is 40 000 euro on average with a standard deviation of 20 000 and we have indication that the distribution is normal. What is the probability of finding a country below the GDP of Latvia, i.e. 22 000 euro per capita or Luxembourg 100 000 euro per capita?\n\npnorm(22000,mean=40000,sd = 20000)\n#&gt; [1] 0.1840601\npnorm(100000,mean=40000,sd = 20000)\n#&gt; [1] 0.9986501\n\nWe see that about 18.4% of the countries will have a GDP lower than Latvia and 99.9% will be lower than Luxembourg.\nThe other way around, we can ask what would be the GDP/capita for a country to be in the top 1% or top 10% using qnorm()\n\nqnorm(0.99,mean=40000,sd = 20000)\n#&gt; [1] 86526.96\nqnorm(0.90,mean=40000,sd = 20000)\n#&gt; [1] 65631.03\nqnorm(0.5,mean=40000,sd = 20000)\n#&gt; [1] 40000",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "052_CLT_Standardisation.html",
    "href": "052_CLT_Standardisation.html",
    "title": "16  Central Limit Theorem and standardisation",
    "section": "",
    "text": "16.1 Central Limit Theorem\nThe reason for the success of the normal distribution is not only due to its key features (symmetry and the linkage between quantiles and standard deviations) but is also explained by the Central Limit Theorem:\nLet’s draw 5 times a 100 numbers from a uniform distribution within the interval 0 to 10. For each of the five cases, the average should be close to 5.\nmean(runif(100)*10)\n#&gt; [1] 4.913308\nmean(runif(100)*10)\n#&gt; [1] 4.916601\nmean(runif(100)*10)\n#&gt; [1] 5.035741\nmean(runif(100)*10)\n#&gt; [1] 5.379667\nmean(runif(100)*10)\n#&gt; [1] 4.923971\nWe can write this in a loop and produce a histogram of the means:\nn&lt;-5\nmeans&lt;-numeric(n)\nfor (i in 1:n){\n  means[i]&lt;-mean(runif(100)*10)\n}\nmeans\n#&gt; [1] 5.567078 5.064164 4.763338 5.049475 5.488653\nhist(means)\nIt is not quite impressive at this stage, but if we repeat, say 10000 times the experiment rather than 5, the histogram tends towards the shape of a normal distribution!\nn&lt;-10000\nmeans&lt;-numeric(n)\nfor (i in 1:n){\n  means[i]&lt;-mean(runif(100)*10)\n}\nhead(means)\n#&gt; [1] 5.409663 5.171069 4.651183 5.210977 5.042082 5.082752\nhist(means, breaks=100)\nAnd the median, mean and standard deviation of the “means” are\nmedian(means)\n#&gt; [1] 4.990204\nmean(means)\n#&gt; [1] 4.993733\nsd(means)\n#&gt; [1] 0.2909366",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "052_CLT_Standardisation.html#central-limit-theorem",
    "href": "052_CLT_Standardisation.html#central-limit-theorem",
    "title": "16  Central Limit Theorem and standardisation",
    "section": "",
    "text": "If you take repeated samples from a population with finite variance and calculate their averages, then the averages will be normally distributed (Crawley (2012), p213)",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "052_CLT_Standardisation.html#standardization",
    "href": "052_CLT_Standardisation.html#standardization",
    "title": "16  Central Limit Theorem and standardisation",
    "section": "16.2 Standardization",
    "text": "16.2 Standardization\nIn order to visually appreciate how well the obtained distribution compares to a corresponding normal distribution, we can generate a normal distribution with the same parameters (mean and variance). Alternatively, we can also standardize the outcome in order to fit a “Unit” normal distribution, i.e. one where the mean is 0 and the standard deviation is 1.\nThis process is called Standardization. It involves centering an empirical variable, i.e. computing deviations to the mean, so that the mean is set to 0, and reducing by dividing the centered by the standard deviation. Hence making the new standard deviation equal to 1. One often name a standradized variable Z. The built in function for standardization is scale(). It centers and reduces by default, but you can toggle one or the other on or off.\nImportant: Since standardization involves a division by a number (standard deviation) with the same units as the original variable, a standardized variable has no unit ! This is a property you may like for comparing with other variables that have different original units.\n\nCentered&lt;-means-mean(means) #centering\nZ&lt;-Centered/sd(means) #reducing /scaling\n#or simply \nZ&lt;-scale(means, center = TRUE, scale = TRUE)\n\n#Compare\ncbind(mean(means), mean(Z))\n#&gt;          [,1]         [,2]\n#&gt; [1,] 4.993733 1.478957e-15\ncbind(sd(means), sd(Z))\n#&gt;           [,1] [,2]\n#&gt; [1,] 0.2909366    1\n\n\n16.2.1 Standardized density from many repetitions\nWe can now compare our empirical distribution density (blue) to the theoretical density of a unit normal distribution (black). The overlay effectively proves the Central Limit Theorem, showing that even from a very simple distribution like the uniform distribution, the mean over repeated samples is distributed normally.\n\ncurve(dnorm,-4,4) #Density of theoretical normal distribution\nlines(density(Z), col=\"blue\") #Density of our empirical distribution after n repetitions\n\n\n\n\n\n\n\n\n\n\n16.2.2 QQplot\nAnother classical way to know whether an empirical distribution corresponds well to a given theoretical one is to use a quantile-quantile plot, aka qqplot, where empirical quantiles are plotted along the y-axis and theoretical ones (e.g. normal) along the x-axis.\nWe expect a straight line at least in a good range of values around the mean. Towards the extreme there are less points and the variability is thus greater. Given we look at the shape of the curve, not the values, we don’t need to standardize the data ahead this time (but of course then the values on the two axes will differ). We verify visually that the qqplot of our mean of means is close to a straight line.\n\nqqnorm(y=means)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "053_distributions_other_continuous.html",
    "href": "053_distributions_other_continuous.html",
    "title": "17  Continuous distributions",
    "section": "",
    "text": "17.1 Exponential\nA random variable \\(X \\in [0;+\\infty[\\) following an exponential distribution with parameter \\(\\lambda &gt; 0\\) can be expressed as: \\(X \\sim \\epsilon(\\lambda)\\) with expectation \\(E(X) = 1/ \\lambda\\) and variance \\(Var(X) = 1/ \\lambda^2\\).\nIts density function can be written as:\n\\[f(x) = λ {e}^{- λ x} \\text{ for } x \\geq 0\\] \\[f(x) = 0 \\text{ for } x &lt; 0\\]\nIts cumulative distribution function is defined by:\n\\[F_X(x) = 1 - \\exp{(- \\lambda x)} \\text{ for } x \\geq 0\\] \\[F_X(x) = 0 \\text{ for } x &lt; 0\\]\nx &lt;- seq(0,8,0.01)\ndExp.5 &lt;- dexp(x, .5)\ndExp1 &lt;- dexp(x, 1)\ndExp2 &lt;- dexp(x, 2)\ndExp3 &lt;- dexp(x, 3)\nplot(x, dExp3, type = 'l', col = 'green',\n     main = 'Density function of X~Exp(lambda)', ylab = 'f(x)')\nlines(x, dExp2, col = 'gold')\nlines(x, dExp1, col = 'darkorange')\nlines(x, dExp.5, col = 2)\nlegend(\"topright\", lty = 1,\n       legend = c(\"lambda = 0.5\", \"lambda = 1\", \n                  \"lambda = 2\", \"lambda = 3\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\nplot(x, pexp(x, 3), type = 'l', col = 'green',\n     main = 'Cumulative Distribution Function of X~Exp(lambda)', ylab = 'F(x)')\nlines(x, pexp(x, 2), col='gold')\nlines(x, pexp(x, 1), col='darkorange')\nlines(x, pexp(x, .5), col=2)\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"lambda = 0.5\", \"lambda = 1\", \"lambda = 2\", \"lambda = 3\"),\n       col = c(2, 'darkorange', 'gold', 'green'))",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous distributions</span>"
    ]
  },
  {
    "objectID": "053_distributions_other_continuous.html#chi-square",
    "href": "053_distributions_other_continuous.html#chi-square",
    "title": "17  Continuous distributions",
    "section": "17.2 Chi-square",
    "text": "17.2 Chi-square\nA random variable \\(X\\) following a chi-square (or khi-square) distribution can be expressed as: \\(X \\sim \\chi^2(k)\\) with parameter \\(k&gt; 0\\), expectation \\(E(X) = k\\), variance \\(Var(X) = 2k\\).\n\\(k\\) represents the degree of freedom.\nThe density function is\n\\[f_X(x) = \\dfrac {1} {2^{k/2} \\gamma(k/2)} x^{(k/2)-1} \\exp{(\\dfrac{-x}{2})} \\text{ for } x \\geq 0\\]\n\\[f_X(x) = 0 \\text{ for } x &lt; 0\\]\n\nx &lt;- seq(0,8,0.01)\ndChiSq1 &lt;- dchisq(x, df=1)\ndChiSq2 &lt;- dchisq(x, df=2)\ndChiSq3 &lt;- dchisq(x, df=3)\ndChiSq6 &lt;- dchisq(x, df=6)\n\nWe see (and know from the CLT) that when the degrees of freedom increases, the function gets closer to a normal distribution. However it is not symmetrical and the right hand side stays longer than in the normal distribution for quite some time.\n\nplot(x, dChiSq1, type = 'l', col = 2, ylim = c(0, 1.25),\n     main = 'Density function of X~Chi^2(p)', ylab = 'f(x)')\nlines(x, dChiSq2, col = 'darkorange')\nlines(x, dChiSq3, col = 'gold')\nlines(x, dChiSq6, col = 'green')\nlegend(\"topright\", lty = 1,\n       legend = c(\"k = 1\", \"k = 2\", \"k = 3\", \"k = 6\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nThe cumulative density is as follows:\n\nplot(x, pchisq(x, df=1), type = 'l', col = 2,\n     main = 'Cumulative Distribution Function of X~Chi2(p)', ylab = 'F(x)')\nlines(x, pchisq(x, df=2), col='darkorange')\nlines(x, pchisq(x, df=3), col='gold')\nlines(x, pchisq(x, df=6), col='green')\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"k = 1\", \"k = 2\", \"k = 3\", \"k = 6\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nThe chi-square distribution is typically used to test the independence of two variables. It is typically applied to a categorical or ordinal metric along two categories, e.g. the level of education of Luxembourg natives vs Luxembourg migrants or the counts of votes for Kamala Harris among young vs elders population or among urban vs rural population. It is thus often applied to contingency tables after the cross-tabulation of two factors in order to test if observed frequencies differ from expected frequencies.\nThe df considered are usually low since it corresponds to the number of categories-1 (e.g. urban+rural, i.e. 2 -1=1) times the types of output -1, (e.g. vote for Harris vs Trump, i.e. 2-1=1). Thus far from a normal distribution.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous distributions</span>"
    ]
  },
  {
    "objectID": "053_distributions_other_continuous.html#student-t-distribution",
    "href": "053_distributions_other_continuous.html#student-t-distribution",
    "title": "17  Continuous distributions",
    "section": "17.3 Student t-distribution",
    "text": "17.3 Student t-distribution\nA random variable \\(X\\) following a student distribution, can be expressed as: \\(X \\sim t(n)\\) with parameter \\(n\\) degrees of freedom.\nIts density function is\n\\[f(x) = \\dfrac{1}{\\sqrt{n\\pi}} \\dfrac {\\gamma{(\\dfrac{n+1}{2})}} {\\gamma{(\\dfrac{n}{2})}} \\dfrac {1} {(1+\\dfrac{x^2}{n})^{(n+1)/2}} \\text{ for all } x\\in R\\] We have: \\[E(X) = 0 \\text{ for } n \\ge 2\\] \\[Var(X) = \\dfrac{n}{n-2} \\text{ for } n \\ge 3\\]\nIf we have a random variable \\(U \\sim N (0, 1)\\) and a random variable \\(X \\sim \\chi^2(n)\\) which are independent, then the random variable \\(T_n = \\dfrac{U}{\\sqrt{X/n}}\\) follows a student distribution \\(t(n)\\)\n\nx &lt;- seq(-8, 8, 0.01)\ndStudent1 &lt;- dt(x, 1)\ndStudent2 &lt;- dt(x, 2)\ndStudent4 &lt;- dt(x, 4)\n\n\nplot(x, dnorm(x), type = 'l', col = 'darkgray', xlim = c(-4, 4), lty = 3,\n     main = 'Density function of X~t(n)', ylab = 'f(x)')\nlines(x, dStudent4, col = 'gold')\nlines(x, dStudent2, col = 'darkorange')\nlines(x, dStudent1, col = 2)\nlegend(\"topright\", lty = c(rep(1, 3), 3),\n       legend = c(\"t(n = 1)\", \"t(n = 2)\", \"t(n = 4)\", \"N(0,1)\"),\n       col = c(2, 'darkorange', 'gold', 'darkgray'))\n\n\n\n\n\n\n\n\n\nplot(x, pnorm(x), type = 'l', col = 'darkgray', lty = 3,\n     main = 'Cumulative Distribution Function of X~t(n)', ylab = 'F(x)')\nlines(x, pt(x, 2), col='gold')\nlines(x, pt(x, 3), col='darkorange')\nlines(x, pt(x, 6), col=2)\nlegend(\"bottomright\", lty = c(rep(1, 3), 3),\n       legend = c(\"t(n = 1)\", \"t(n = 2)\", \"t(n = 4)\", \"N(0,1)\"),\n       col = c(2, 'darkorange', 'gold', 'darkgray'))\n\n\n\n\n\n\n\n\nCompared to the normal distribution, we can see the t-distribution depends solely on \\(n\\). It is used for continuous variables in the non-rare cases where the sample size is small and the variance of the population is unknown.\nt-Student’s tests are typically used to test whether two samples have different means or if a sample mean differ from a given value.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous distributions</span>"
    ]
  },
  {
    "objectID": "053_distributions_other_continuous.html#fisher-snedecor",
    "href": "053_distributions_other_continuous.html#fisher-snedecor",
    "title": "17  Continuous distributions",
    "section": "17.4 Fisher-Snedecor",
    "text": "17.4 Fisher-Snedecor\nA random variable \\(X\\) following a Fisher-Snedecor distribution can be expressed as \\(X \\sim F(n, p)\\) with parameters \\(n\\) and \\(p\\) degrees of freedom.\nIts density function is\n\\[f_X(x) = \\dfrac {\\gamma(\\dfrac{n+p}{2})} {\\gamma(\\dfrac{n}{2}) \\gamma(\\dfrac{p}{2})} (\\dfrac{n}{p})^{(n/2)} \\dfrac{x^{\\dfrac{n-2}{2}}}{(1 + \\dfrac{n}{p}x)^{\\dfrac{n-2}{2}}} \\text{ for } x \\geq 0\\] \\[f_X(x) = 0 \\text{ for } x &lt; 0\\]\nWe have:\n\\[E(X) = \\dfrac{p}{p-2} \\text{ for } p \\ge 3\\] \\[Var(X) = \\dfrac{2p^2(n+p-2)}{n(p-2)^2(p-4)} \\text{ for } p \\ge 5\\]\nIf we have two random variables \\(X\\) and \\(Y\\) independent such that \\(X \\sim \\chi^2(n)\\) and \\(Y \\sim \\chi^2(p)\\), then the random variable \\(\\dfrac{X/n}{Y/p}\\) follows a Fisher distribution \\(F(n, p)\\)\n\nx &lt;- seq(0, 3, 0.01)\ndF1 &lt;- df(x, 1, 1)\ndF10 &lt;- df(x, 10, 10)\ndF20 &lt;- df(x, 20, 20)\ndF50 &lt;- df(x, 50, 50)\n\n\nplot(x, dF1, type = 'l', col = 2, \n     main = 'Density function of X ~ F(n, p)', ylab = 'f(x)')\nlines(x, dF10, col = 'darkorange')\nlines(x, dF20, col = 'gold')\nlines(x, dF50, col = 'green')\nlegend(\"topright\", lty = 1,\n       legend = c(\"F(n = p = 1)\", \"F(n = p = 10)\", \"F(n = p = 20)\", \"F(n = p = 50)\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\n\nplot(x, pf(x, 50, 50), type = 'l', col = 'green',\n     main = 'Cumulative Distribution Function of X ~ F(n, p)', ylab = 'F(x)')\nlines(x, pf(x, 20, 20), col='gold')\nlines(x, pf(x, 10, 10), col='darkorange')\nlines(x, pf(x, 1, 1), col=2)\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"F(n = p = 1)\", \"F(n = p = 10)\", \"F(n = p = 20)\", \"F(n = p = 50)\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nYou will mostly encounter F-tests in association with the goodness of fit of a regression analysis. It is used to test whether a linear model better fits the data than a ‘intercept only’ model, i.e. one that contains no independent variables, i.e. whether your model provides any useful information for prediction.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous distributions</span>"
    ]
  },
  {
    "objectID": "054_distributions_discrete.html",
    "href": "054_distributions_discrete.html",
    "title": "18  Discrete distributions",
    "section": "",
    "text": "18.1 Discrete uniform distribution\n(to be done)",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discrete distributions</span>"
    ]
  },
  {
    "objectID": "054_distributions_discrete.html#discrete-binomial-distribution",
    "href": "054_distributions_discrete.html#discrete-binomial-distribution",
    "title": "18  Discrete distributions",
    "section": "18.2 Discrete binomial distribution",
    "text": "18.2 Discrete binomial distribution\nThrowing a coin holds two possible values and generates a so called Bernoulli random variable. We can represent the outcome by a 0 or 1. Let’s suppose 1 (whichever side you choose) is what we consider a success. This is the usual convention.\nA binomial distribution is the sum of independent and identically distributed Bernoulli random variables.\nWhen there are \\(n\\) Bernoulli trials (all independent), then the sum of those trials, each with the same probability of success \\(p\\)) is binomially distributed, with parameters \\(n\\) and \\(p\\). It is defined with the two parameters: \\(B(n, p)\\) with parameter \\(p \\in [0; 1]\\), the probability that the event of interest (success) happens\nA Bernoulli random variable \\(X = \\{0 ;1\\}\\) can thus be expressed as a binomial distribution with a single toss \\(n=1\\), \\(X \\sim B(1, p)\\)\n    X        1         0\n-------- --------- ---------\n$P(X=i)$    $p$    $q = 1-p$\nThe probability that \\(X\\) takes any other value is null (unless the coin stays on its edge…)\nIn a binomial distribution, the expectation is \\(E(X) = np\\) and the variance is \\(Var(X) = npq\\).\nThe probability that one obtains \\(k\\) successes over \\(n\\) repetitions (experiences), is then given by \\(P(X=k) = (n|k) p^k q^{n-k}\\).\nFor an intuition we can simulate 3 trials and produce a tree of probabilities. Where we see that the probability of 3 successes is one of 8 possible outcomes,\n     1st      2nd     3rd         Total\n     result   result  result\n     \n                     --1--        3 successes\n              --1---|\n             |       --0--        2 successes\n     --1-----|\n    |        |       --1--        2 successes\n    |         --0---|\n    |                --0--        1 success\n----|\n    |                --1--        2 successes\n    |         --1---|\n    |        |       --0--        1 success\n     --0-----|\n             |       --1--        1 success\n              --0---|\n                     --0--        0 success\nwhich in R is:\n\ndbinom(3, size=3, prob=0.5)\n#&gt; [1] 0.125\n\nOther example: suppose we run a trial 10 times, knowing the probability of success is 0.5, what is the probability that the total number of successes will be exactly 3? or exactly 5 ? or 10?\n\ndbinom(3, size=10, prob=0.5)\n#&gt; [1] 0.1171875\ndbinom(5, size=10, prob=0.5)\n#&gt; [1] 0.2460938\ndbinom(10, size=10, prob=0.5)\n#&gt; [1] 0.0009765625\n\nLet’s generalize a little for values from 1 to 100 and different probabilities:\n\nx &lt;- 1:100\ndBer1 &lt;- dbinom(x, size=100, prob=0.1)\ndBer2 &lt;- dbinom(x, size=100, prob=0.25)\ndBer3 &lt;- dbinom(x, size=100, prob=0.5)\ndBer4 &lt;- dbinom(x, size=100, prob=0.75)\ndBer5 &lt;- dbinom(x, size=100, prob=0.9)\n\nWith 100 trials, you can see the density ‘curve’ is symmetrical but shifted towards higher total success when the probability of success gets higher, and is flatter when closer to 0.5, (and symmetrical behaviour for cases for \\(p\\) and \\(q=1-p\\))\n\nplot(x, dBer1,main = 'Distribution of B(100, p)', ylab = 'f(x)', type=\"h\", col = 'darkgreen')\npoints(x, dBer2, col = 'lightgreen', type=\"h\")\npoints(x, dBer3, col = 'gold', type=\"h\")\npoints(x, dBer4, col = 'orange', type=\"h\")\npoints(x, dBer5, col = 'darkred', type=\"h\")\nlegend(\"top\",pch=1,\n       legend = c(\"p = 0.10\", \"p = 0.25\", \"p = 0.50\",\"p = 0.75\", \"p = 0.90\"),\n       col = c(\"darkgreen\",\"lightgreen\", 'gold', 'orange',\"darkred\"))\n\n\n\n\n\n\n\n\nBack to using \\(p=0.5\\), we now look at the effect of sample size to see how from right-skewed it shifts to a normal distribution:\n\ndn1 &lt;- dbinom(x, size=5, prob=0.5)\ndn2 &lt;- dbinom(x, size=10, prob=0.5)\ndn3 &lt;- dbinom(x, size=50, prob=0.5)\ndn4 &lt;- dbinom(x, size=100, prob=0.5)\n\n\nplot(x, dn1, col=\"lightblue\", type=\"h\",\n     main = 'Distribution of B(n, 0.5)', ylab = 'f(x)')\npoints(x, dn2, col = 'blue', type=\"h\")\npoints(x, dn3, col = 'blue4', type=\"h\")\npoints(x, dn4, col = 'purple',  type=\"h\")\nlegend(\"top\", pch = 1,\n       legend = c(\"n = 5\", \"n = 10\", \"n = 50\", \"n=100\"),\n       col = c(\"lightblue\", 'blue',\"blue4\", 'purple'))\n\n\n\n\n\n\n\n\nWhile we have looked at the distribution of the probabilities for a total outcome after repetitions to be of a given “exact” value using the density function dbinom(), we can use the cumulative form pbinom() to know where the total outcome is smaller or equal to a given value. For example, after 100 experiments with probability 0.5, what is the probability I obtain less than 50 successes? Or \\(n/2\\) successes depending on \\(n\\) ?\n\npbinom(50, size=100, prob=0.5)\n#&gt; [1] 0.5397946\npbinom(5, size=10, prob=0.5)\n#&gt; [1] 0.6230469\npbinom(5000000, size=10000000, prob=0.5)\n#&gt; [1] 0.5001262\n\nThe cumulative density function can be plotted as follows for different probabilities and a given size:\n\nplot(pbinom(1:100,size=100,prob=0.10), main = 'CDF of B(100, p)', ylab = 'f(x)', type=\"s\", col = 'darkgreen')\nlines(pbinom(1:100,size=100,prob=0.25), type=\"s\", col = 'lightgreen')\nlines(pbinom(1:100,size=100,prob=0.5), type=\"s\", col = 'gold')\nlines(pbinom(1:100,size=100,prob=0.75), type=\"s\", col = 'orange')\nlines(pbinom(1:100,size=100,prob=0.9), type=\"s\", col = 'darkred')\nlegend(\"bottomright\",pch=1,\n       legend = c(\"p = 0.10\", \"p = 0.25\", \"p = 0.50\",\"p = 0.75\", \"p = 0.90\"),\n       col = c(\"darkgreen\",\"lightgreen\", 'gold', 'orange',\"darkred\"))\n\n\n\n\n\n\n\n\nThe binomial distribution is used in many cases where there are two potential outcomes that are mutually exclusive. For example when we try to explain why some plots of land are developed or not or why people use the car rather than an active mode of transport.",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discrete distributions</span>"
    ]
  },
  {
    "objectID": "054_distributions_discrete.html#discrete-poisson-distribution",
    "href": "054_distributions_discrete.html#discrete-poisson-distribution",
    "title": "18  Discrete distributions",
    "section": "18.3 Discrete Poisson distribution",
    "text": "18.3 Discrete Poisson distribution\nThe Poisson distribution is for rare discrete occurrence events. It is used when counting the occurrence of a certain event that appears randomly but at a known rate or density. The main statistical property of the Poisson distribution is that its variance equals its mean\nThere are many uses in geography, transport or planning, such as the counting of cars passing a rural road segment over a certain time, or the distribution of points (trees, bees, houses…) over a homogeneous set of spatial polygons (grid cells)\nSuppose there are 100 houses or trees over 100 grid cells. The overall density (\\(\\lambda\\)) is 1. What is the probability that a cell does not receive any single house? Or in other words what will be the proportion of cells without a single house or tree?\n\nppois(lambda = 1,q=0)\n#&gt; [1] 0.3678794\n\nHow does this change when the overall density is even higher or lower? i.e. with 150 houses/trees or only 25?\n\nppois(lambda = 1.5,q=0)\n#&gt; [1] 0.2231302\nppois(lambda = 0.5,q=0)\n#&gt; [1] 0.6065307\nppois(lambda = 0.25,q=0)\n#&gt; [1] 0.7788008\n\nLet’s generate such cases and get the frequency of counts to “see” those occurrences:\n\npois025&lt;-rpois(100,0.25)\npois025\n#&gt;   [1] 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1\n#&gt;  [38] 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1\n#&gt;  [75] 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1\ntable(pois025)\n#&gt; pois025\n#&gt;  0  1  2 \n#&gt; 69 30  1\npois150&lt;-rpois(100,1.5)\npois150\n#&gt;   [1] 3 2 2 0 1 5 2 1 2 2 2 1 1 0 1 3 1 2 2 1 1 1 1 1 1 2 2 1 4 0 0 3 1 0 2 1 1\n#&gt;  [38] 2 1 3 1 0 1 2 0 2 2 3 0 0 1 0 1 0 1 1 2 2 2 3 3 0 0 0 2 1 1 2 3 2 1 2 4 0\n#&gt;  [75] 2 2 2 1 1 0 5 3 3 1 2 1 0 2 1 2 1 1 0 1 2 1 2 2 0 0\ntable(pois150)\n#&gt; pois150\n#&gt;  0  1  2  3  4  5 \n#&gt; 20 35 31 10  2  2\n\nWe examine how the probability of different counts (not just 0 or more) changes when lambda changes:\n\nlambdan&lt;-data.frame(n=rep(1:4,4),lambda=rep(seq(1,0.25,by=-0.25),each=4))\nlambdan$d&lt;-dpois(lambdan$n,lambdan$lambda)\nlambdan\n#&gt;    n lambda            d\n#&gt; 1  1   1.00 0.3678794412\n#&gt; 2  2   1.00 0.1839397206\n#&gt; 3  3   1.00 0.0613132402\n#&gt; 4  4   1.00 0.0153283100\n#&gt; 5  1   0.75 0.3542749146\n#&gt; 6  2   0.75 0.1328530930\n#&gt; 7  3   0.75 0.0332132732\n#&gt; 8  4   0.75 0.0062274887\n#&gt; 9  1   0.50 0.3032653299\n#&gt; 10 2   0.50 0.0758163325\n#&gt; 11 3   0.50 0.0126360554\n#&gt; 12 4   0.50 0.0015795069\n#&gt; 13 1   0.25 0.1947001958\n#&gt; 14 2   0.25 0.0243375245\n#&gt; 15 3   0.25 0.0020281270\n#&gt; 16 4   0.25 0.0001267579\n\nInteresingly, the distribution depends on the segments of observations or, in space, the rsolution of the grid, i.e. the modifiable areal unit problem (MAUP)\nConsider the following:\nLet’s define a grid of 100 (10x10) cells over a mixed forest. Suppose a poisson process with mean and variance = 1 gives the number of coniferous trees within that forest. We can expect around 37 % of cells to have at least a coniferous, right? (see above).\nNow suppose the mean and variance increase to 2, we are still in a poisson process because the number of events is still quite rare even there are less empty cells.\n\nppois(lambda=1, q=0)\n#&gt; [1] 0.3678794\nppois(lambda=2, q=0)\n#&gt; [1] 0.1353353\n\nBut if we now groups cells to make them larger, say divide the space into 25 cells (5 x 5) rather than 100. Then you see that lambda is multiplied by 4 and the probability of a zero count:\n\nppois(lambda=2*4, q=0)\n#&gt; [1] 0.0003354626\n\nAgain, following the the Central Limit Theorem, the higher will be the mean (λ) and thus the spatial aggregation, the closer the distribution of coniferous will be to a normal distribution.\n\nr2_100&lt;-rpois(lambda=2, n=100)\nr2_100\n#&gt;   [1] 2 1 3 0 1 0 3 1 0 0 2 1 4 3 3 5 0 2 4 5 0 2 1 1 2 3 2 2 0 2 1 0 4 4 0 1 4\n#&gt;  [38] 1 2 3 3 0 5 1 1 2 2 3 2 2 0 3 2 3 1 0 2 3 2 1 3 3 2 2 0 5 2 1 1 4 2 3 1 0\n#&gt;  [75] 2 3 1 7 1 3 1 3 5 2 0 2 2 2 0 2 2 3 2 2 1 5 1 2 2 2\npar(mfrow = c(1, 2))\nimage(matrix(r2_100, 10),asp=1)\nimage(matrix(r2_100, 10)&gt;1, asp=1)\n\n\n\n\n\n\n\n\nRe-aggregated:\n\nlarge&lt;-matrix(rep(1:5,each=20)*10+rep(rep(1:5, each=2),10),10)\nlarge\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]   11   11   21   21   31   31   41   41   51    51\n#&gt;  [2,]   11   11   21   21   31   31   41   41   51    51\n#&gt;  [3,]   12   12   22   22   32   32   42   42   52    52\n#&gt;  [4,]   12   12   22   22   32   32   42   42   52    52\n#&gt;  [5,]   13   13   23   23   33   33   43   43   53    53\n#&gt;  [6,]   13   13   23   23   33   33   43   43   53    53\n#&gt;  [7,]   14   14   24   24   34   34   44   44   54    54\n#&gt;  [8,]   14   14   24   24   34   34   44   44   54    54\n#&gt;  [9,]   15   15   25   25   35   35   45   45   55    55\n#&gt; [10,]   15   15   25   25   35   35   45   45   55    55\n\nsumbylarge&lt;-aggregate(r2_100, by=list(matrix(large)), FUN=sum)\n\nmatrix(sumbylarge$x,5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    6    3    6   11    9\n#&gt; [2,]   10   10   11    5   11\n#&gt; [3,]    9    6    4   10    8\n#&gt; [4,]    6    9   10   11    7\n#&gt; [5,]    9    7    7    9    6\n\npar(mfrow = c(1, 2))\nimage(matrix(sumbylarge$x,5))\nimage(matrix(sumbylarge$x,5)&gt;1)",
    "crumbs": [
      "Part V - Distributions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discrete distributions</span>"
    ]
  },
  {
    "objectID": "061_inference.html",
    "href": "061_inference.html",
    "title": "19  Statistical Inference",
    "section": "",
    "text": "19.1 Point Estimates\nTo estimate the population mean \\(\\bar{X}\\) (or other metrics such as variance or median) based on a sample, we typically use the sample mean:\n\\[\\bar{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\]\nThe sample mean, \\(\\bar{x}\\), is known as a point estimate of the population mean.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "061_inference.html#point-estimates",
    "href": "061_inference.html#point-estimates",
    "title": "19  Statistical Inference",
    "section": "",
    "text": "19.1.1 Problem\nIf we draw another sample from the same population and calculate the new sample mean, we are likely to obtain a different point estimate. This difference is referred to as sampling variation.\nIn other words, an estimate is close to the true value but not exactly equal to the actual (population) parameter value. Additionally, sampling variation is expected to decrease as the sample size increases. A point estimate converges to the population parameter value as the sample size grows and approaches the size of the entire population.\n\n\n19.1.2 Example\nWe suppose that the dataset census contains our target population. Our aim is to estimate the population mean for the variable age and the proportion of the different genders in the case of variable sex.\nWe look at the running mean of the variable age and at the running proportion of men for the variable sex.\nWe start by extracting from the total population a sample made of one observation: \\(n = 1\\). At each step, we extract randomly a new sample with one additional observation. We use a sampling with replacement, i.e. the previous sample is not simply augmented: we take a new sample (made of completely new individuals) each time. We stop when we reach the total population, i.e. when \\(n = 500\\) in our case.\nThe key function for sampling in R is sample(), which randomly selects a number (arg size) of records for a given vector (or, if none are specified, outputs a vector of \\(1\\) to \\(size\\) integers in a random order, see below for examples of the sample() function).\nNote also that we introduce here a “for loop” to iterate from 1 to \\(N=500\\) (our total population)\n\nN &lt;- nrow(census) #500\nmu_age &lt;- mean(census$age) #our population mean (usually unknown)\nmu_men &lt;- mean(census$sex == 'Male') #now our population proportion of males (usually unknown)\n\n#Prepare a vector of Means and Proportions\nrMeanAge = NULL\nrPropMale = NULL\n\nset.seed(201292)\n\nfor (i in 1:N) {\n  sample &lt;- census[sample(N, i), ]\n  rMeanAge[i] &lt;- mean(sample$age)\n  rPropMale[i] &lt;- mean(sample$sex == \"Male\")\n}\nrm(sample) #we don't keep the last sample\n\nplot(rMeanAge, type = 'l', col = 2, \n     xlab = 'Sample size', ylab = 'age', \n     main = 'Running mean')\nabline(h = mu_age, col = 'darkgray')\ntext(500, mu_age + 1.5, mu_age, col = 'darkgray')\n\n\n\n\n\n\n\n\nplot(rPropMale, type = 'l', col = 4, \n     xlab = 'Sample size', ylab = 'men', \n     main = 'Running proportion')\nabline(h = mu_men, col = 'darkgray')\ntext(500, mu_men + .05, mu_men, col = 'darkgray')\n\n\n\n\n\n\n\n\n\n\n19.1.3 Standard Error\nTo quantify the uncertainty of a point estimate, we use its standard error. The standard error of an estimate (e.g. the mean) is the standard deviation of its sampling distribution.\nThe notion is not to be confused with the standard deviation of the sample: - the standard error of the sample mean indicates how far the sample mean is from the population mean. - the standard deviation of the sample indicates how far each individual data within the sample is from the sample mean.\nThe exact value of the standard error of the mean (or of the proportion) is \\[SE_{\\bar{x}} = \\dfrac{\\sigma}{\\sqrt{n}}\\] which in our case can be computed, given we know the population:\n\nsd(census$age)/sqrt(N)\n#&gt; [1] 0.9754678\nsd(census$sex == \"Male\")/sqrt(N)\n#&gt; [1] 0.02232498\n\nHowever, the standard deviation of the population is usually not known and actually we would not need a point estimate and a standard error if we were knowing the total population.\nThe standard error of the mean (or proportion) is thus estimated by replacing \\(\\sigma\\) in the definition with the standard deviation of the sample used to make the point estimate, i.e. \\(s\\)\n\\[SE_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\]\nThe denominator, \\(\\sqrt{n}\\), reflects how the variability of the sample mean decreases as the sample size increases. Also, due to the square root, we see that a willingness to reduce the error of the estimate by two requires four times as many observations.\nSuppose a sample of 25 or 100 observations, the standard error would be\n\nset.seed(101)\nsample_age25 &lt;- sample(census$age, 25)\nSEage25 &lt;- sd(sample_age25) / sqrt(25)\nSEage25\n#&gt; [1] 4.464631\n\nsample_age100 &lt;- sample(census$age, 100)\nSEage100 &lt;- sd(sample_age100) / sqrt(100)\nSEage100\n#&gt; [1] 2.174494\n\nsample_sex25 &lt;- sample(census$sex, 25)\nSEmen25 &lt;- sd(sample_sex25 == 'Male') / sqrt(25)\nSEmen25\n#&gt; [1] 0.09797959\n\nsample_sex100 &lt;- sample(census$sex, 100)\nSEmen100 &lt;- sd(sample_sex100 == 'Male') / sqrt(100)\nSEmen100\n#&gt; [1] 0.04988877\n\nwhere we see the reduction in error due to sample size. From 100 to 25, the error is roughly halved The standard error of the mean of the sample tends to zero when the sample size is increased. Indeed we have seen earlier with the running means and proportions that sample means get very close to the population mean when sample size is increased and approaches the population.\n\n\n19.1.4 Sampling Distribution\nLet’s now build the sampling distribution of the sample mean. So we can see the shape of that distribution and see that the standard error is indeed the standard deviation of the sampling distribution\nWe draw \\(K = 1000\\) samples of size \\(n = 50\\) from the population.\n\nK &lt;- 1000\nN&lt;-nrow(census)\nn&lt;-50\n\nmeanAge = NULL\npropMen = NULL\n\nset.seed(345)\nfor (i in 1:K) {\n  sample &lt;- census[sample(N, n), ]\n  meanAge[i] &lt;- mean(sample$age)\n  propMen[i] &lt;- mean(sample$sex == 'Male')\n}\nrm(sample)\n\nhist(meanAge, breaks = 25,\n     main = 'Sampling distribution of the mean age')\nabline(v = mu_age, col = 2)\ntext(38.5, 100, mu_age, col = 2)\n\n\n\n\n\n\n\n\nhist(propMen, breaks = 20,\n     main = 'Sampling distribution of the proportion of men')\nabline(v = mu_men, col = 2)\ntext(mu_men + .1, 100, mu_men, col = 2)\n\n\n\n\n\n\n\n\nIn our case, the distribution of the mean and of the proportion are symmetric and centered around the real value \\(\\mu = 35.298\\) (mu_age) and \\(\\mu = 0.536\\) (mu_men).\nFollowing the Central Limit Theorem, no matter whether the true population is normal or not, the sample mean is approximately normally distributed when sample size is large (we often see a value of \\(&gt; 30\\) to consider a sample is not small)\nSince we are lucky enough to have a large amount of samples, we can estimate the standard error directly form its definition, i.e. the standard deviation of the sample means.\nIn this case:\n\nsd(meanAge) #standard deviation of the sample means\n#&gt; [1] 2.970229\nsd(propMen) #standard deviation of the sample proportions\n#&gt; [1] 0.06858092",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "061_inference.html#confidence-interval-estimation",
    "href": "061_inference.html#confidence-interval-estimation",
    "title": "19  Statistical Inference",
    "section": "19.2 Confidence Interval Estimation",
    "text": "19.2 Confidence Interval Estimation\nDue to the error in point estimates, it is often relevant to look at intervals instead of just the exact points. The range of values of estimates for a given parameter is called a confidence interval (CI). To build a confidence interval we need three elements:\n\nA point estimate \\(\\hat{\\theta}\\) for the parameter of interest \\(\\theta\\)\nThe standard error associated with the point estimate \\(SE_{\\hat{\\theta}}\\)\nA confidence level \\(1 - \\alpha\\)\n\nWe know the first two elements. A confidence interval uses the information about the uncertainty of the point estimate to define the range of values such that we are confident at a level \\(1 - \\alpha\\) that it captures the true (population) parameter value. We call \\(1 - \\alpha\\) the confidence level.\nFor \\(\\alpha = 5\\% = 0.05\\), we can say that\n\n“We are 95% confident that the interval will capture the population parameter.”\n\nIn other words:\n\n“If we generate 100 samples, and compute their confidence interval, in 95 cases, the interval will contain the parameter value”.\n\nWhen a point estimate \\(\\hat{\\theta}\\) follows a normal distribution (case of a sufficiently sample size), its confidence interval is defined by: \\[[\\hat{\\theta} \\pm z_\\alpha SE]\\]\nHere, \\(z_\\alpha SE\\) is the margin of error.\nUnder a normal distribution:\n\n\\(z_{0.1} = 1.645\\): 90% confidence interval\n\\(z_{0.05} = 1.96\\): 95% confidence interval\n\\(z_{0.001} = 2.58\\): 99% confidence interval\n\n\n19.2.1 Example\nIn the case of the age variable of the census data, the intervals for each of our \\(K\\) samples are\n\nCI95 &lt;- data.frame(low = meanAge - 1.96 * sd(meanAge), \n                   point = meanAge,\n                   high = meanAge + 1.96 * sd(meanAge))\n\nhead(CI95)\n#&gt;        low point     high\n#&gt; 1 29.89835 35.72 41.54165\n#&gt; 2 25.25835 31.08 36.90165\n#&gt; 3 30.87835 36.70 42.52165\n#&gt; 4 22.39835 28.22 34.04165\n#&gt; 5 28.71835 34.54 40.36165\n#&gt; 6 29.07835 34.90 40.72165\n\nWhile in most cases, the interval is within the 95% confidence range around \\(\\mu = 35.298\\), the upper bounds of the confidence intervals for some of the samples can exceed \\(\\mu + 1.96 \\times SE_{\\text{age}}\\), indicating that these samples have higher or lower sample means and greater variability, thus suggesting they are less reliable as estimates of the population mean.\nWe can find out those cases for example as follow:\n\nabov&lt;-which(CI95$low&gt;mu_age)\nbelo&lt;-which(CI95$high&lt;mu_age)\nabov\n#&gt;  [1]  57  68 172 175 190 209 244 277 477 505 548 571 606 631 692 748 786 810 839\n#&gt; [20] 841 857 976 984\nbelo\n#&gt;  [1]   4  12  81  93 137 139 148 187 197 261 380 400 432 461 633 644 763 773 853\n#&gt; [20] 876 886\n\nInterestingly, but not surprisingly, we can see there are roughly 25 cases where the CI range is too high and 25 where the CI range is too low. Well that is 50 out 1000 samples and the exact meaning of \\(\\alpha=0.05\\):\n\nIf we generate 1000 samples, and compute their confidence interval, in 950 cases, the interval will contain the parameter value.\n\nWe show the confidence interval for 3 samples (out of the 95%) where the point estimate can be trusted to represent the population mean, followed by 3 of the too high cases and 3 of the too low cases.\n\na_subset&lt;-c(1:3,abov[1:3],belo[1:3])\nplot(CI95[a_subset, 'low'], col = 'darkgray', lwd = .3, \n     type = 'b', ylim = c(23,45), \n     xlab = 'sample', ylab = 'mean age',\n     main = 'Confidence Intervals at level 95%',\n     xaxt=\"n\")\nlines(CI95[a_subset, 'point'], col = 4, type = 'b')\nlines(CI95[a_subset, 'high'], col = 'darkgray', lwd = .75, , type = 'b')\nabline(h = mu_age, col = 2, lwd = 2)\nabline(v = 3.5, lwd = 0.5, col=\"grey10\",lty=2)\nabline(v = 6.5, lwd = 0.5, col=\"grey10\",lty=2)\naxis(1, at=1:length(a_subset), labels=a_subset,las=2)\n\n\n\n\nPopulation mean in red = 35.298 ; some of the point estimates in dark blue ; their corresponding confidence intervals at level 95% in gray\n\n\n\n\nUnfortunately, in the large majority of cases, we would not be able to compute the standard error this way as we would only have one or two samples. The only solution is then to use the standard deviation of the sample divided by the square root of the sample size (see earlier equations) as a best guess for the standard deviation of the distribution of sample means, i.e. to make up the standard error.\nSo far, we haven’t stored that information (i.e. standard deviation for each sample). We need to relaunch the exact same samples (luckily we used a seed). We do it for age only:\n\nK &lt;- 1000\nmeanAge = NULL\nsdAge = NULL\n\nset.seed(345)\nfor (i in 1:K) {\n  sample &lt;- census[sample(N, n), ]\n  meanAge[i] &lt;- mean(sample$age)\n  sdAge[i] &lt;- sd(sample$age)\n}\n\nFrom each standard deviation, we compute the standard error:\n\nseAge&lt;-sdAge/sqrt(n) #sample sd error \nhead(seAge)\n#&gt; [1] 3.112849 2.517504 3.298144 2.995001 3.229483 2.931775\nhead(seAge&gt;sd(meanAge)) #compare sample standard error and sampling distribution st dev\n#&gt; [1]  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n\nwhere we see there is variety which will also make the CI different (not constant) for each sample and thus change the decision on the relevance of each sample mean:\n\nCI95s &lt;- data.frame(low = meanAge - 1.96 * seAge, \n                   point = meanAge,\n                   high = meanAge + 1.96 * seAge)\n\n\nabovs&lt;-which(CI95s$low&gt;mu_age)\nbelos&lt;-which(CI95s$high&lt;mu_age)\nabovs\n#&gt;  [1]  57  68 172 175 190 209 277 477 505 548 571 606 748 786 792 810 813 839 857\n#&gt; [20] 976\nbelos\n#&gt;  [1]   4  12  80  81  93 117 137 139 148 187 197 261 278 380 400 432 461 471 633\n#&gt; [20] 638 644 705 723 763 773 853 876 886\n\n\na_subset&lt;-c(1:3,abovs[1:3],belos[1:3])\nplot(CI95s[a_subset, 'low'], col = 'darkgray', lwd = .3, \n     type = 'b', ylim = c(23,45), \n     xlab = 'sample', ylab = 'mean age',\n     main = 'Confidence Intervals at level 95%',\n     xaxt=\"n\")\nlines(CI95s[a_subset, 'point'], col = 4, type = 'b')\nlines(CI95s[a_subset, 'high'], col = 'darkgray', lwd = .75, , type = 'b')\nabline(h = mu_age, col = 2, lwd = 2)\nabline(v = 3.5, lwd = 0.5, col=\"grey10\",lty=2)\nabline(v = 6.5, lwd = 0.5, col=\"grey10\",lty=2)\naxis(1, at=1:length(a_subset), labels=a_subset,las=2)\n\n\n\n\nPopulation mean in red = 35.298 ; some of the point estimates in dark blue ; their corresponding confidence intervals at level 95% in gray\n\n\n\n\nThe decision would have changed for some of the samples, for example 723 and 984, knowing mu_age is 35.298.\n\nrbind(CI95s[c(723),],CI95[c(723),],CI95s[c(984),],CI95[c(984),])\n#&gt;           low point     high\n#&gt; 723  24.19980 29.72 35.24020\n#&gt; 7231 23.89835 29.72 35.54165\n#&gt; 984  35.09008 41.56 48.02992\n#&gt; 9841 35.73835 41.56 47.38165\n\n\n\n19.2.2 Note on the sample() function\nWhen applied to a vector x, sample() selects a set of values within the vector. The size of this set (i.e. the size of the sample) is given though the argument size. Although there is no explicit default to the argument (see help), it is inferred from the length of x is not given. Hence when size is missing sample(x) provides a random permutation of the vector x. Don’t forget to use set.seed() to keep a particular permutation\n\nset.seed(101)\nsample(c(\"A\",\"B\",\"C\",\"D\"))\n#&gt; [1] \"A\" \"D\" \"B\" \"C\"\nset.seed(102)\nsample(c(\"A\",\"B\",\"C\",\"D\"))\n#&gt; [1] \"C\" \"B\" \"A\" \"D\"\n\nWhen size is explicit, a random subset is returned. In the case of a random draw withour replacement (default) size cannot be smaller than length(x). It can if replacement is allowed:\n\nsample(c(\"A\",\"B\",\"C\",\"D\"),2)\n#&gt; [1] \"D\" \"C\"\n#sample(c(\"A\",\"B\",\"C\",\"D\"),6) error\nsample(c(\"A\",\"B\",\"C\",\"D\"),10, replace = TRUE)\n#&gt;  [1] \"A\" \"D\" \"B\" \"D\" \"C\" \"C\" \"C\" \"A\" \"C\" \"C\"\n\nWhen applied to a scalar x, sample returns a random permutation of values from 1 to x, or a subset of it if size is indicated:\n\nsample(7)\n#&gt; [1] 6 5 4 7 1 2 3\nsample(7)\n#&gt; [1] 6 2 7 3 4 1 5\nsample(7,size=3)\n#&gt; [1] 5 3 4\nsample(7,size=10, replace=TRUE)\n#&gt;  [1] 3 1 2 1 7 1 2 7 3 2\n\nIt is useful as such but also when applied to a data.frame.\nFor sampling a random set of rows over a data.frame you use sample with a scalar corresponding to the number of rows in the data.frame. This returns a random permutation of the identification of each row, which you can then use to reshuffle or subset the data set:\n\ndata&lt;-data.frame(Y=c(\"A\",\"B\",\"C\",\"D\"), Z=11:14)\ndata\n#&gt;   Y  Z\n#&gt; 1 A 11\n#&gt; 2 B 12\n#&gt; 3 C 13\n#&gt; 4 D 14\n\nset.seed(101)\nsample(nrow(data))\n#&gt; [1] 1 4 2 3\n\n#reshuffle (permutation)\nset.seed(101)\ndata[sample(nrow(data)),]\n#&gt;   Y  Z\n#&gt; 1 A 11\n#&gt; 4 D 14\n#&gt; 2 B 12\n#&gt; 3 C 13\n\n#random subset\nset.seed(101)\ndata[sample(nrow(data),size=2),]\n#&gt;   Y  Z\n#&gt; 1 A 11\n#&gt; 4 D 14\n\n\n\n19.2.3 Other example:\nWe have a sample of 100 trees with measurements of their diameter at breast height. We are interested in the mean value and standard error. The point estimate equals 164 with an empirical variance of 333221.\nWhat is the standard error of the sample mean?\n\nn &lt;- 100\nvariance &lt;- 333221\n# compute sample standard deviation\ns &lt;- sqrt(variance)\ns\n#&gt; [1] 577.253\n# compute standard error of the sample mean\nSE_mean &lt;- s / sqrt(n)\nSE_mean\n#&gt; [1] 57.7253\n\nWhat is the standard error if the sample was made of 1000 trees?\n\nSE_mean &lt;- s / sqrt(1000)\nSE_mean\n#&gt; [1] 18.25434",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "062_hypotheses_test.html",
    "href": "062_hypotheses_test.html",
    "title": "20  Hypotheses testing",
    "section": "",
    "text": "20.1 Concepts and Definitions",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypotheses testing</span>"
    ]
  },
  {
    "objectID": "062_hypotheses_test.html#concepts-and-definitions",
    "href": "062_hypotheses_test.html#concepts-and-definitions",
    "title": "20  Hypotheses testing",
    "section": "",
    "text": "20.1.1 Test\nA statistical test is a method used to determine whether there is a significant difference between an expected model (or hypothesis) and the observed data. It is used to assess whether observed differences are due to random chance or if they are statistically significant, thereby allowing to compare a hypothesis against the collected data.\nYou are interested in a particular question for which only two answers are possible: yes or no. Each outcome is expressed as an hypothesis: a null hypothesis \\(H_0\\) - it often represents the hypothesis of no change - and an alternative hypothesis \\(H_1\\) - it often represents the hypothesis of a change.\nYour sample is made of observations assumed to be random and described by a statistical model. The two possible answers to the question of interest are:\n\nThe null hypothesis is rejected: it means that the answer to the question is yes: the observed differences are significant and sufficiently large that they our hypothesis does not hold.\nThe null hypothesis is not rejected: it means that the answer to the question is no: the differences observed are due to randomness. With this data our hypothesis still holds.\n\n\n\n20.1.2 Example:\nYou want to answer the question: Does a given drug have an impact on a particular disease?\n\nThe null hypothesis (\\(H_0\\)) is: The drug has no effect; the patient does not feel better or worse with the treatment.\nThe alternative hypothesis (\\(H_1\\)) is: The drug has an effect (which can be either positive or negative).\n\n\n\n20.1.3 Uni- and Bi-lateral Tests\nThere are two types of tests: bilateral and unilateral.\n\nBilateral Test: This test checks for any difference from the null hypothesis, whether it is higher or lower. \\[H_0 = \\{ \\theta = \\theta_0 \\} \\text{ and } H_1 = \\{ \\theta \\neq \\theta_0 \\}\\]\nUnilateral Test: This test checks for a difference in a specific direction, either higher or lower.\n\nRight-tailed test: \\[H_0 = \\{ \\theta \\leq \\theta_0 \\} \\text{ and } H_1 = \\{ \\theta &gt; \\theta_0 \\}\\]\nLeft-tailed test: \\[H_0 = \\{ \\theta \\geq \\theta_0 \\} \\text{ and } H_1 = \\{ \\theta &lt; \\theta_0 \\}\\]\n\n\n\n\n20.1.4 Types of Errors\nDecision error terms help us understand the likelihood of making incorrect decisions. There are two types of errors:\n\n\n\n\n\\(H_0\\) true\n\\(H_1\\) true\n\n\n\n\n\\(H_0\\) not rejected\n\\(1 - \\alpha\\)\n\\(\\beta\\)\n\n\n\\(H_0\\) rejected\n\\(\\alpha\\)\n\\(1 - \\beta\\)\n\n\n\nIn the table above, rows represent the decision made, while columns represent the actual reality.\n\nType I Error (\\(\\alpha\\)):\n\nThis is the significance level of the test. It occurs when we reject the null hypothesis (\\(H_0\\)) when it is actually true. This error represents the probability of incorrectly concluding that there is an effect when there is none. Common significance levels are 1%, 5%, or 10%.\n\nType II Error (\\(\\beta\\)):\n\nThis occurs when we fail to reject the null hypothesis (\\(H_0\\)) when the alternative hypothesis (\\(H_1\\)) is actually true. This error represents the probability of incorrectly concluding that there is no effect when there is one.\n\n\n20.1.5 Power of a Test\nThe power of a test is given by \\(1 - \\beta\\).\nIt represents the probability of correctly rejecting the null hypothesis (\\(H_0\\)) when it is false. If the power of a test tends to 1 as the sample size increases to infinity, the test is said to be consistent.\n\n\n20.1.6 Critical Region: \\(C_\\alpha\\)\nTo test if a point estimate \\(\\hat{\\theta}\\) is significantly different from a null value \\(\\theta_0\\), we must consider uncertainty. We use the concept of the confidence interval (CI) introduced previously. The confidence interval is also known as the acceptance region. Any value outside this interval falls into the rejection region, which is called the critical region and can be expressed as:\n\\[CI_{1-\\alpha} = [\\theta_0 \\pm z_\\alpha * SE ] = [ \\theta_0 - z_{\\alpha/2} * SE ; \\theta_0 + z_{1-\\alpha/2} * SE ] \\]\n\\[C_\\alpha = [ min ; \\theta_0 - z_{\\alpha/2} * SE] \\cup [ \\theta_0 + z_{1-\\alpha/2} * SE ; max ]\\][^062_hypothesis_test-1]\nFrom \\(Z\\) the value of the observed statistic we can decide to reject or not the null hypothesis.\n\nIf \\(Z \\in C_\\alpha\\), we reject \\(H_0\\)\nIf \\(Z \\notin C_\\alpha\\), we do not reject \\(H_0\\)\n\n\n\n20.1.7 P-Value:\nAlso called the probability value, the p-value or \\(p\\) corresponds to the probability that a given test statistic exceeds the threshold value to fall within the rejection area when \\(H_0\\) is true. It helps us determine the strength of the evidence against the null hypothesis.\nA low p-value (close to 0) indicates that the observed data is unlikely under the null hypothesis, suggesting that we should reject \\(H_0\\). Conversely, a high p-value suggests that the observed data is consistent with \\(H_0\\), and we do not reject it.\nDecision Rule: - If \\(p.value &lt; \\alpha\\), we reject \\(H_0\\). - If \\(p.value &gt; \\alpha\\), we do not reject \\(H_0\\).",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypotheses testing</span>"
    ]
  },
  {
    "objectID": "062_hypotheses_test.html#steps-in-practice",
    "href": "062_hypotheses_test.html#steps-in-practice",
    "title": "20  Hypotheses testing",
    "section": "20.2 Steps in practice",
    "text": "20.2 Steps in practice\n\nChoose \\(H_0\\), \\(H_1\\), and \\(\\alpha\\).\nDefine the test statistic.\nUsing the Critical Region\n\n\nCompute the critical region \\(C_\\alpha\\) with respect to \\(\\alpha\\) and \\(H_0\\).\nCompute the value of the test statistic from the observed sample.\n\nor 3. Using the P-Value\n\nCompute the p-value from the sample.\n\n\nConclusion: Reject or do not reject \\(H_0\\) at the significance level \\(\\alpha\\).",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypotheses testing</span>"
    ]
  },
  {
    "objectID": "062_hypotheses_test.html#illustration",
    "href": "062_hypotheses_test.html#illustration",
    "title": "20  Hypotheses testing",
    "section": "20.3 Illustration",
    "text": "20.3 Illustration\nSuppose we have a parameter \\(\\theta\\), observed twice: blue \\(1.5\\) and green \\(2.5\\) Suppose the population is expected to follow a standard normal distribution\n\nplot(x, y, type='l', axes = F, xlab = '', ylab = '',\n     main = '')\naxis(1, mean(y), 'null\\n value', col = 'red', col.axis = 'red')\naxis(1, obs1, 'observed \\n value', col = 'blue', col.axis = 'blue')\naxis(1, obs2, 'observed\\n value', col = 'green', col.axis = 'green')\ntext(-3, .25, \"Distribution of the\\n parameter if H0 true\")\n\n\n\n\n\n\n\n\nand we want to test the hypothesis \\(H_0\\) that those observed values are not null, i.e. testing the null hypothesis: \\(H_0: \\theta = \\theta_0\\).\nWe choose a significance level \\(\\alpha = 5%\\)\nThe question is then: Are the observed value \\(\\theta\\) significantly different from the null value \\(\\theta_0\\) at level \\(\\alpha = 5\\%\\)?\nUsing the critical region we observe the position of each parameter against the values representing the limits of the critical region:\nWe know the region is here defined based on a normal distribution of mean 0 and standard deviation 1. So for \\(\\alpha = 5\\%\\), we have \\(z=-1.96\\) and \\(z=1.96\\). Or more exactly\n\nz &lt;- 1.96\nqnorm(p=0.05/2,mean = 0, sd=1,lower.tail = TRUE)\n#&gt; [1] -1.959964\nqnorm(p=0.05/2,mean = 0, sd=1,lower.tail = FALSE)\n#&gt; [1] 1.959964\n\n\nplot(x, y, type='l', axes = F, xlab = '', ylab = '',\n     main = 'alpha = 5%\\n Critical region')\naxis(1, mean(y), 'null\\n value', col = 'red', col.axis = 'red')\naxis(1, obs1, 'observed\\n value', col = 'blue', col.axis = 'blue')\naxis(1, obs2, 'observed\\n value', col = 'green', col.axis = 'green')\ntext(-3, .25, \"Distribution of the\\n parameter if H0 true\")\n\npolygon(c(z, x[x &gt;= z]), c(0, y[x &gt;= z]), col = 'purple', border = 'purple')\npolygon(c(-z, x[x &lt;= -z]), c(0, y[x &lt;= -z]), col = 'purple', border = 'purple')\ntext(3, .08, \"P(X &gt; 1.96) = 2.5%\\n = alpha/2\", col = 'purple')\ntext(-3, .08, \"P(X &lt; -1.96) = 2.5%\\n = alpha/2\", col = 'purple')\n\n\n\n\n\n\n\n\nNow we see that:\n\nthe green case is beyond the critical region for \\(\\theta\\). We thus reject the null hypothesis. The green \\(\\theta\\) is not zero.\nthe blue case is within the critical region for \\(\\theta\\). We can’t reject the null hypothesis. The blue \\(\\theta\\) could still be a zero.\n\nUsing the p-value, we compute the corresponding p-values:\n\np1&lt;-1-pnorm(q = obs1,mean = 0, sd=1)\np1\n#&gt; [1] 0.0668072\np2&lt;-1-pnorm(q = obs2,mean = 0, sd=1)\np2\n#&gt; [1] 0.006209665\n\n#the p-value we chose can also be obtained from same function\nqz&lt;-qnorm(p = 0.05/2, mean = 0, sd = 1, lower.tail = FALSE)\nqz\n#&gt; [1] 1.959964\npz&lt;-1-pnorm(q=qz , mean = 0, sd = 1)\npz\n#&gt; [1] 0.025\n\nWhere we see that - \\(p.value = 0.06  &gt; \\alpha\\) for the blue case, i.e. we reject \\(H_0\\). - \\(p.value = 0.006  &lt; \\alpha\\) for the green case, i.e. we don’t reject \\(H_0\\).\n\nplot(x, y, type='l', axes = F, xlab = '', ylab = '',\n     main = 'alpha = 5%\\n P-value')\npolygon(c(obs1, x[x &gt;= obs1]), c(0, y[x &gt;= obs1]), col = 'blue', border = 'blue')\ntext(obs1+1, .09, \"p.value\", col = 'blue')\npolygon(c(z, x[x &gt;= z]), c(0, y[x &gt;= z]), col = 'purple', border = 'purple')\ntext(z+.5, .06, \"alpha\", col = 'purple')\npolygon(c(obs2, x[x &gt;= obs2]), c(0, y[x &gt;= obs2]), col = 'green', border = 'green')\ntext(obs2+1, .09, \"p.value\", col = 'green')",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypotheses testing</span>"
    ]
  },
  {
    "objectID": "062_hypotheses_test.html#example-testing-the-average-height",
    "href": "062_hypotheses_test.html#example-testing-the-average-height",
    "title": "20  Hypotheses testing",
    "section": "20.4 Example: Testing the Average Height",
    "text": "20.4 Example: Testing the Average Height\nSuppose we have a sample of 30 individuals, and we want to test if their average height is significantly different from the known population mean height of 170 cm.\nHere is our sample (on purpose we generate it with an expected mean of 172 and quite a large variance)\n\nset.seed(123)\nh &lt;- rnorm(30, mean = 172, sd = 6)\nsummary(h)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   160.2   168.0   171.6   171.7   174.9   182.7\nmean(h)\n#&gt; [1] 171.7174\n\nWe know the population mean and consider a significance level of \\(0.05\\)\n\nbarH &lt;- 170\nalpha &lt;- 0.05\n\nWe would like to know if the sample mean is close to the population mean, but we have a small number of records and the variance of the population is not know, so we can’t relate to a known normal distribution but must consider a t-student distribution.\nOur t-statistic compares the sample mean to the population mean divided by the standard error:\n\nt_statistic &lt;- (mean(h) - barH) / (sd(h) / sqrt(length(h)))\nt_statistic\n#&gt; [1] 1.598058\n\nWe must compare this t-statistic to the value from a two-tailed t-test critical value (our mean could be below or above the mean) with 29 degrees of freedom:\n\ndf &lt;- length(h) - 1\nt_critical &lt;- qt(1 - alpha / 2, df)\nt_critical\n#&gt; [1] 2.04523\n\nUsing the critical region, we can check if the parameter is beyond the limits or not:\n\nabs(t_statistic) &gt; t_critical\n#&gt; [1] FALSE\n\n, which in this case is false hence the sample mean is within the acceptable range to correspond to the population mean. We can’t reject the null hypothesis.\nWe can also compute the p-value:\n\np_value &lt;- 2 * (1 - pt(abs(t_statistic), df))\np_value\n#&gt; [1] 0.1208704\n\n, which is above \\(\\alpha\\). Hence we can’t reject the null hypothesis. The sample mean is not different form the population mean.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hypotheses testing</span>"
    ]
  },
  {
    "objectID": "063_hypotheses_single_sample.html",
    "href": "063_hypotheses_single_sample.html",
    "title": "21  Hypotheses testing - One sample tests",
    "section": "",
    "text": "21.1 Tests about a Mean",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypotheses testing - One sample tests</span>"
    ]
  },
  {
    "objectID": "063_hypotheses_single_sample.html#tests-about-a-mean",
    "href": "063_hypotheses_single_sample.html#tests-about-a-mean",
    "title": "21  Hypotheses testing - One sample tests",
    "section": "",
    "text": "21.1.1 Mean of normal distribution with known variance (Z-Test)\nWe consider a random variable \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\mu\\) unknown but \\(\\sigma^2\\) being known - a very unrealistic case in practice. We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\).\n\nTesting: \\(H_0: \\mu = \\mu_0\\) against \\(H_1: \\mu \\neq \\mu_0\\).\nTest Statistic: random variable defined by \\[Z_n = \\dfrac{\\hat{\\mu_n} - \\mu_0}{\\sigma / \\sqrt{n}} \\sim N(0, 1)\\]\nThe critical values are read on a standard normal distribution table.\n\n\nExample (using summary description of sample):\nSuppose a population with the following characteristics:\n\nmu_0 &lt;- 50 #hypothesized population mean against which the test is made\nsigma &lt;- 10 #population sd\n\nAnd a sample of 25 records with a mean of 55\n\nn &lt;- 25 #sample size\nsample_mean &lt;- 55\n\nThe test statistic is\n\nZ_n &lt;- (sample_mean - mu_0) / (sigma / sqrt(n))\nZ_n\n#&gt; [1] 2.5\n\nWith a given significance level, you then decide to reject or not the null hypothesis (equal means)\n\nalpha &lt;- 0.05\n# Determine the critical value and decide:\nZ_alpha &lt;- qnorm(1 - alpha / 2)\nZ_alpha\n#&gt; [1] 1.959964\nreject_H0 &lt;- abs(Z_n) &gt; Z_alpha\nreject_H0\n#&gt; [1] TRUE\n\n# or Calculate the p-value and decide:\np_value &lt;- 2 * (1 - pnorm(abs(Z_n)))\np_value\n#&gt; [1] 0.01241933\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] TRUE\n\nSuppose another example with sample mean closer to the hypothesized population mean, then\n\nsample_mean &lt;- 51\nZ_n &lt;- (sample_mean - mu_0) / (sigma / sqrt(n))\np_value &lt;- 2 * (1 - pnorm(abs(Z_n)))\np_value\n#&gt; [1] 0.6170751\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] FALSE\n\n\n\n\n21.1.2 Mean of normal distribution with unknown variance (t-test)\nIn reality, it is far more likely you don’t know the population variance.\nWe consider a random variable \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\mu\\) and \\(\\sigma^2\\) unknown. We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\).\n\nTesting: \\(H_0: \\mu = \\mu_0\\) against \\(H_1: \\mu \\neq \\mu_0\\).\nTest Statistic: random variable defined by \\[T_{n-1} = \\dfrac{\\hat{\\mu_n} - \\mu_0}{S_{n,c} / \\sqrt{n}} \\sim t(n-1)\\]\nDecision rule: the critical value \\(c_\\alpha\\) is read on a student distribution table.\nThe critical values are read on a t-student distribution table, which we know doesn’t need a variance to be defined.\n\n\nExample (using a summary description of a sample)\nSuppose the same example as for the Z-test but this time we don’t know the population variance. We can’t use \\(\\sigma\\) but can still compute the sample variance (or standard deviation)\n\nsample_mean &lt;- 55\nsample_sd &lt;- 10\n\nand then the t-test:\n\nT_n &lt;- (sample_mean - mu_0) / (sample_sd / sqrt(n))\nT_n\n#&gt; [1] 2.5\n\nSo we can now decide (using the p-value)\n\np_value &lt;- 2 * (1 - pt(abs(T_n), df = n - 1))\np_value\n#&gt; [1] 0.01965418\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] TRUE\n\nAgain, if the sample mean was closer to the population one, for this same observed variance, we would have:\n\nsample_mean &lt;- 51\nT_n &lt;- (sample_mean - mu_0) / (sample_sd / sqrt(n))\np_value &lt;- 2 * (1 - pt(abs(T_n), df = n - 1))\np_value\n#&gt; [1] 0.6216287\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] FALSE\n\n\n\nExample (using the sample data itself)\nSuppose we have the actual sample data instead of just the summary statistics. We can perform the t-test using the t.test() function in R (still using the hypothesized mean of 50), with the advantage of not having to compute the sample mean and staandard deviation and a reporting of bot p-value and confidence interval\n\nsample_data &lt;- c(55, 51, 49, 52, 56, 54, 50, 53, 57, 58)\nt.test(sample_data, mu = mu_0)\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  sample_data\n#&gt; t = 3.6556, df = 9, p-value = 0.005271\n#&gt; alternative hypothesis: true mean is not equal to 50\n#&gt; 95 percent confidence interval:\n#&gt;  51.33415 55.66585\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;      53.5\n\nwhich holds the same result as our earlier “manual” method using the summary statistics of the sample, see:\n\nn&lt;-length(sample_data)\nsample_mean&lt;-mean(sample_data)\nsample_sd&lt;-sd(sample_data)\np_value &lt;- 2 * (1 - pt(abs((sample_mean - mu_0) / (sample_sd / sqrt(n))), df = n - 1))\np_value\n#&gt; [1] 0.005271186",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypotheses testing - One sample tests</span>"
    ]
  },
  {
    "objectID": "063_hypotheses_single_sample.html#tests-about-a-variance",
    "href": "063_hypotheses_single_sample.html#tests-about-a-variance",
    "title": "21  Hypotheses testing - One sample tests",
    "section": "21.2 Tests about a Variance",
    "text": "21.2 Tests about a Variance\n\n21.2.1 Variance of normal distribution with known mean (chi-squared test n df)\nWe consider a random variable \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\mu\\) known and \\(\\sigma^2\\) unknown - quite rare in practice. We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\).\n\nTesting: \\(H_0: \\sigma^2 = \\sigma^2_0\\) against \\(H_1: \\sigma^2 \\neq \\sigma^2_0\\).\nTest Statistic: random variable defined by \\[\\chi^2_{n} = \\dfrac{(n) \\hat{\\sigma^2_n}}{\\sigma^2} \\sim \\chi^2(n)\\]\nThe critical values are read on a chi-squared distribution table. After a sampling from a normal distribution, the distribution of the sample variance is known to follow a chi-squared distribution.\n\n\nExample:\nConsidering the same hypothetical population and a generated normal sample of 30 observations (for which the true standard deviation is higher, i.e. 15).\n\nset.seed(123)\nn&lt;-30\nx &lt;- rnorm(n, mean = 50, sd = 15)\nsample_variance &lt;- var(x)\nsd(x)\n#&gt; [1] 14.71546\n\n\nsigma_0&lt;-10\nchi_squared_stat &lt;- (n * sample_variance) / sigma_0^2\nchi_squared_stat\n#&gt; [1] 64.96343\n\n\nalpha&lt;-0.05\ncritical_value_lower &lt;- qchisq(alpha / 2, df = n)\ncritical_value_upper &lt;- qchisq(1 - alpha / 2, df = n)\nc(critical_value_lower,critical_value_upper)\n#&gt; [1] 16.79077 46.97924\n\n#or p-value\npchisq(chi_squared_stat, df = n)\n#&gt; [1] 0.9997783\np_value &lt;- 2 * min(pchisq(chi_squared_stat, df = n), 1 - pchisq(chi_squared_stat, df = n))\np_value\n#&gt; [1] 0.0004434497\n\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] TRUE\n\nSuppose another sample (generated with its true standard deviation, the same as the population’s)\n\nset.seed(101112)\nn&lt;-30\nx &lt;- rnorm(n, mean = 50, sd = 10)\nsample_variance &lt;- var(x)\nsd(x)\n#&gt; [1] 9.974145\n\n\nchi_squared_stat &lt;- (n * sample_variance) / sigma_0^2\nchi_squared_stat\n#&gt; [1] 29.84507\n\n\ncritical_value_lower &lt;- qchisq(alpha / 2, df = n)\ncritical_value_upper &lt;- qchisq(1 - alpha / 2, df = n)\nc(critical_value_lower,critical_value_upper)\n#&gt; [1] 16.79077 46.97924\n\n#or p-value\npchisq(chi_squared_stat, df = n - 1)\n#&gt; [1] 0.5782318\np_value &lt;- 2 * min(pchisq(chi_squared_stat, df = n), 1 - pchisq(chi_squared_stat, df = n))\np_value\n#&gt; [1] 0.9472181\n\nreject_H0 &lt;- p_value &lt; alpha\nreject_H0\n#&gt; [1] FALSE\n\nThis time the null hypothesis is not rejected. The sample variance cannot be said to differ from the population variance.\n\n\n\n21.2.2 Variance of normal distribution with unknown mean (chi-squared test n-1 df)\nWe consider a random variable \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\mu\\) and \\(\\sigma^2\\) unknown - more likely case. We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\).\n\nTesting: \\(H_0: \\sigma^2 = \\sigma^2_0\\) against \\(H_1: \\sigma^2 \\neq \\sigma^2_0\\).\nTest Statistic: random variable defined by \\[\\chi^2_{n-1,c} = \\dfrac{n S^2_{n,c}}{\\sigma^2} \\sim \\chi^2(n-1)\\]\nThe critical values are read on a chi-squared distribution table.\n\nThe test needs to include the fact we need a degree of freedom to estimate the mean:\n\nExample\n\nchi_squared_stat &lt;- ((n - 1) * sample_variance) / sigma_0^2\nchi_squared_stat\n#&gt; [1] 28.85023",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypotheses testing - One sample tests</span>"
    ]
  },
  {
    "objectID": "063_hypotheses_single_sample.html#tests-about-a-proportion",
    "href": "063_hypotheses_single_sample.html#tests-about-a-proportion",
    "title": "21  Hypotheses testing - One sample tests",
    "section": "21.3 Tests about a Proportion",
    "text": "21.3 Tests about a Proportion\n\n21.3.1 Proportion from a small samples (Binomial test)\nWe consider a random variable \\(X\\). We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\).\n\nTesting: \\(H_0: \\pi_A = \\pi_0\\) against \\(H_1: \\pi_A \\neq \\pi_0\\).\nTest Statistic: random variable defined by \\[n_A = n \\hat{\\pi_{n,A}} \\sim B(n, \\pi_0)\\]\nThe critical values are read on a binomial distribution table.\n\n\nExample\n\nn &lt;- 30\npi_0 &lt;- 0.5\nx1 &lt;- 20 # Observed successes\nx2 &lt;- 15 # Observed successes\nbinom_test1 &lt;- binom.test(x1, n, p = pi_0, alternative = \"two.sided\")\nbinom_test1\n#&gt; \n#&gt;  Exact binomial test\n#&gt; \n#&gt; data:  x1 and n\n#&gt; number of successes = 20, number of trials = 30, p-value = 0.09874\n#&gt; alternative hypothesis: true probability of success is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt;  0.4718800 0.8271258\n#&gt; sample estimates:\n#&gt; probability of success \n#&gt;              0.6666667\nbinom_test2 &lt;- binom.test(x2, n, p = pi_0, alternative = \"two.sided\")\nbinom_test2\n#&gt; \n#&gt;  Exact binomial test\n#&gt; \n#&gt; data:  x2 and n\n#&gt; number of successes = 15, number of trials = 30, p-value = 1\n#&gt; alternative hypothesis: true probability of success is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt;  0.3129703 0.6870297\n#&gt; sample estimates:\n#&gt; probability of success \n#&gt;                    0.5\n\nThe null hypothesis is rejected in the first case and thus the proportions are different to 0.5, i.e. there is not the same amount of successes and failures. It is not rejected in the second case.\n\n\n\n21.3.2 Proportion from a large sample (Normal approximation test)\nWe consider a random variable \\(X\\). We use a sample of size \\(n\\): \\(x = {x_1, x_2,..., x_n}\\) made of independent realizations of that random variable \\(X\\) such that the following inequalities are respected: \\(n \\ge 50\\), \\(n\\pi_0 \\ge 16\\) and \\(n(1-\\pi_0) \\ge 16\\).\n\nTesting: \\(H_0: \\pi_A = \\pi_0\\) against \\(H_1: \\pi_A \\neq \\pi_0\\).\nTest Statistic: random variable defined by \\[Z_n = \\dfrac{\\hat{\\pi_{n,A}} - \\pi_0}{\\sqrt{\\dfrac{\\pi_0 (1-\\pi_0)}{n}}} \\# N(0, 1)\\]\nThe critical values are read on a standard normal distribution table.\n\n\nExample\nIn practice, there are two ways for approximating the distribution of the test statistic. They lead to similar result but are not calculated the same way, leading to some difference in p-values.\n\nThe first method (the least practical) is to compute a z-test with the normal distribution directly.\n\n\nn &lt;- 100 #now sufficiently large\nx &lt;- 70 # Observed successes #70 to reject the null hypothesis, (try 51 and 60)\npi_0 &lt;- 0.5 #the expected proportion\nsample_prop&lt;-x / n #sample proportion\n\nztest &lt;- (sample_prop - pi_0) / sqrt(pi_0 * (1 - pi_0) / n)\n\np_value &lt;- 2 * (1 - pnorm(abs(ztest)))\np_value\n#&gt; [1] 6.334248e-05\n\nWith 70 successes out of 100, the p-value is very small and it is unlikely that the result is from a trial where there is a fifty-fifty chance of success, i.e. the null probability. (If it is a coin, check it is not biased ;-) )\n\nThe second method (recommended for ease of use) is to use a chi-square test within the prop.test() function. It is similar in structure to the binom.test() used when sample size is small. The chi-squared test within prop.test() uses (by default) the so-called Yates’ correction, which corrects for small values in the case of 2 x 2 matrices (e.g. success x failure and male x female, see later) where one cell of the matrix would have rare events and for which the continuous approximation would lead to overestimation. In our single sample case, we turn this default correction off:\n\n\nprop.test(x, n, p = pi_0, alternative = \"two.sided\", correct = FALSE)\n#&gt; \n#&gt;  1-sample proportions test without continuity correction\n#&gt; \n#&gt; data:  x out of n, null probability pi_0\n#&gt; X-squared = 16, df = 1, p-value = 6.334e-05\n#&gt; alternative hypothesis: true p is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt;  0.6041515 0.7810511\n#&gt; sample estimates:\n#&gt;   p \n#&gt; 0.7\n\nNote that in a proportion test, the expected probability does not need to be 0.5. Suppose there are 3 equally common species of the “tilia” tree genus in our cities (say cordata, platyphyllos and tomentosa for example). So if you count 25 with small leafs (cordata) out of 70 (the others you cannot recognize because their leafs are large) in a neighbourhood, is it surprising?\n\nprop.test(25, 70, p = 0.33, alternative = \"two.sided\", correct = FALSE)\n#&gt; \n#&gt;  1-sample proportions test without continuity correction\n#&gt; \n#&gt; data:  25 out of 70, null probability 0.33\n#&gt; X-squared = 0.23325, df = 1, p-value = 0.6291\n#&gt; alternative hypothesis: true p is not equal to 0.33\n#&gt; 95 percent confidence interval:\n#&gt;  0.2550334 0.4741161\n#&gt; sample estimates:\n#&gt;         p \n#&gt; 0.3571429\n\nYou can’t reject the null hypothesis. It is not really a surprise, i.e. an over or under planting of that species.\n\n\n\n21.3.3 Chi-squared test of goodness of fit\nWe can see the prop.test() above wraps a chi-squared test, which could also be directly computed if the counts of “yes” and “no” are provided as counts in a vector\nWith our earlier example, we obtain the same result by doing:\n\nn &lt;- 100 #now sufficiently large\nx &lt;- 70 # Observed successes #70 to reject the null hypothesis, (try 51 and 60)\npi_0 &lt;- 0.5 #the expected proportion\nchisq.test(c(x,n-x), correct = FALSE) \n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  c(x, n - x)\n#&gt; X-squared = 16, df = 1, p-value = 6.334e-05\n\nThis is the so-called Chi-squared test of goodness of fit (conformity/adequacy), which is more general than the two proportions’ test:\nFrom a random qualitative/factorial variable \\(X\\), we want to test if the frequencies are equally distributed or distributed along a known theoretical distribution function, i.e., if we have the same number of observations for each possible categories than expected from a given (including uniform) distribution.\nLet \\(X\\) having \\(I\\) categories \\(i\\) and \\(n\\) observations, and assume we expect a uniform distribution. We thus expect to see \\(obs_i = exp_i\\) individuals for each category (\\(\\forall i\\)) where \\(exp_i\\) are the so called theoretical frequencies.\nIn the uniform case, \\(exp_i\\) is simply \\(n p_i\\) with \\(p_i=\\dfrac{1}{I}\\) and so \\(exp_i=n/I\\).\nTesting:\n\\[H_0: obs_i = exp_i = n/I$ $\\forall i\\]\n\\[H_0 (\\text{uniform case}): obs_i = n/I$ $\\forall i\\]\nTest Statistic: random variable defined by\n\\[\\chi^2(obs) = \\sum^I_{i=1} \\dfrac{(obs_i - exp_i)^2}{exp_i}\\]\n\\[\\chi^2(obs) (\\text{uniform case})= \\sum^I_{i=1} \\dfrac{(obs_i - n/I)^2}{exp_i} \\] and \\[\\chi^2(obs) \\sim \\chi^2(I-1) \\] The computed statistic being compare to a chi-squared value with \\(I-1\\) degrees of freedom.\n\nExample:\nSuppose there would be 3 categories in our previous example, with the 30 non-success being subdivided into two types of fails:\n\nchisq.test(c(70,20,10))\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  c(70, 20, 10)\n#&gt; X-squared = 62, df = 2, p-value = 3.442e-14\n\nwhich assumes, each category has equal chances by default, i.e. 1/3, i.e. fit to a unifrom distribution. Hence it is obviously rejected. But we may suppose a different set of probabilities. See:\n\nchisq.test(c(70,20,10), p=c(1/3,1/3,1/3))\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  c(70, 20, 10)\n#&gt; X-squared = 62, df = 2, p-value = 3.442e-14\nchisq.test(c(70,20,10), p=c(0.65,0.25,0.10))\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  c(70, 20, 10)\n#&gt; X-squared = 1.3846, df = 2, p-value = 0.5004\n\nSince we can have different probabilities for the different counts, we are just one step away to using a chi-squared test to test the normality of a distribution. For a distribution to be normal, we know what share of the data should fall within -1 and +1 standard deviation, +2 and -2, etc.. We can thus compare counts to the expected ones along a normal distribution using the same chi-squared function. This methods however has been demonstrated to be powerful for discrete data but is not performing as well as other tests for the normality of a continuous variable after binning (to obtain counts). We will be using the Shapiro-Wilk test for testing normality rather than the chi-squared test.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Hypotheses testing - One sample tests</span>"
    ]
  },
  {
    "objectID": "064_hypotheses_two_samples.html",
    "href": "064_hypotheses_two_samples.html",
    "title": "22  Hypotheses testing - Two samples tests",
    "section": "",
    "text": "22.1 Tests about two means",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypotheses testing - Two samples tests</span>"
    ]
  },
  {
    "objectID": "064_hypotheses_two_samples.html#tests-about-two-means",
    "href": "064_hypotheses_two_samples.html#tests-about-two-means",
    "title": "22  Hypotheses testing - Two samples tests",
    "section": "",
    "text": "22.1.1 Normal distribution and known variances (Z-test)\nWe consider a random variable \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and a random variable \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) with \\(\\mu_1\\) and \\(\\mu_2\\) unknown, and \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) known. We use two samples of size \\(n_1\\): \\(x = {x_1, x_2,..., x_{n_1}}\\) and \\(n_2\\): \\(y = {y_1, y_2,..., y_{n_2}}\\) made of independent realizations of that random variables \\(X\\) and \\(Y\\). \\(n_1\\) and \\(n_2\\) can have different values.\n\nTesting: \\(H_0: \\mu_1 = \\mu_2\\) against \\(H_1: \\mu_1 \\neq \\mu_2\\).\nTest Statistic: random variable defined by \\[Z_{n_1, n_2} = \\dfrac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\sqrt{\\dfrac{\\sigma^2_1} {n_1} + \\dfrac{\\sigma^2_2} {n_2}}} \\sim N(0, 1)\\]\nThe critical values are read on a standard normal distribution table.\n\n\nExample:\n\nset.seed(123) \nsigma1 &lt;- 2 # Standard deviation for X\nsigma2 &lt;- 3 # Standard deviation for Y\nn1 &lt;- 30\nn2 &lt;- 40\n\n# Generate samples\nx &lt;- rnorm(n1, mean = 5, sd = sigma1)\ny &lt;- rnorm(n2, mean = 7, sd = sigma2)\n\n# Sample means\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\n\n# Test statistic\nz &lt;- (mean_x - mean_y) / sqrt((sigma1^2 / n1) + (sigma2^2 / n2))\n\n# P-value for two-sided test\np_value &lt;- 2 * (1 - pnorm(abs(z)))\np_value\n#&gt; [1] 1.539303e-05\n\nThe means are not the same!\n\n\n\n22.1.2 Normal distribution and unknown, yet assumed equal, variances (pooled t-test)\nWe consider a random variable \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and a random variable \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) with \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) unknown. We use two samples of size \\(n_1\\): \\(x = {x_1, x_2,..., x_{n_1}}\\) and \\(n_2\\): \\(y = {y_1, y_2,..., y_{n_2}}\\) made of independent realizations of that random variables \\(X\\) and \\(Y\\). \\(n_1\\) and \\(n_2\\) can have different values but we need \\(\\sigma^2_1 = \\sigma^2_2 = \\sigma^2\\) (to test this hypothesis, see the Fisher-Snedecor Test on two variances).\n\nTesting: \\(H_0: \\mu_1 = \\mu_2\\) against \\(H_1: \\mu_1 \\neq \\mu_2\\).\nTest Statistic: random variable defined by \\[T_{n_1 + n_2 - 2} = \\dfrac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\hat{\\sigma} \\sqrt{\\dfrac{1} {n_1} + \\dfrac{1} {n_2}}} \\sim t(n_1 + n_2 - 2)\\] with \\[\\hat{\\sigma} = \\sqrt{ \\dfrac{n_1 S^2_{n_1} + n_2 S^2_{n_2}}{n_1 + n_2 - 2} }\\]\nThe critical values are read on a t-student distribution table.\n\nSimilarly to the one sample case, we can use the built-in function t.test() now indicating the two samples:\n\nExample:\nSuppose final exam score of students who completed their regular assignments and the exam scores of those who didn’t. Note n differs for both\n\n# Exam scores for students who did their regular assignment\nassignment &lt;- c(94, 81, 88, 90, 89, 91, 78, 83, 88, 88, 91, 90)\n\n# sample mean and sd\nmean(assignment)\n#&gt; [1] 87.58333\nsd(assignment)\n#&gt; [1] 4.621262\n\n# Exam scores for students who didn't do their regular assignment\nno_assignment &lt;- c(71, 80, 84, 82, 88, 75, 86, 81, 84)\n\n# sample mean and sd\nmean(no_assignment)\n#&gt; [1] 81.22222\nsd(no_assignment)\n#&gt; [1] 5.35672\n\nIs the difference in mean siginificant?\n\nt.test(assignment,no_assignment,var.equal=TRUE)\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  assignment and no_assignment\n#&gt; t = 2.9176, df = 19, p-value = 0.008829\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;   1.797853 10.924370\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;  87.58333  81.22222\n\nYes, we reject the null hypothesis of equal mean. And we recommend you do your assignments!\nUsually the Welch test (below) is preferred because it is rare we can assume same variance from two samples. For using this pooled t-test, we need to know that most of the variance is due to measurements errors or due to the toolset that was used for building the two samples. In any case the Welch would work even if the variance is the same.\n\n\n\n22.1.3 Normal distribution and unknown, possibly different, variances (Welch test)\nWe consider a random variable \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and a random variable \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) with \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) unknown. We use two samples of size \\(n_1\\): \\(x = {x_1, x_2,..., x_{n_1}}\\) and \\(n_2\\): \\(y = {y_1, y_2,..., y_{n_2}}\\) made of independent realizations of that random variables \\(X\\) and \\(Y\\). \\(n_1\\) and \\(n_2\\) can have different values and we accept \\(\\sigma^2_1 \\ne \\sigma^2_2\\) (to test this hypothesis, see the Fisher-Snedecor Test).\n\nTesting: \\(H_0: \\mu_1 = \\mu_2\\) against \\(H_1: \\mu_1 \\neq \\mu_2\\).\nTest Statistic: random variable defined by \\[T_{v} = \\dfrac{\\hat{\\mu_1} - \\hat{\\mu_2}}{\\sqrt{\\dfrac{S^2_{n_1}} {n_1 - 1} + \\dfrac{S^2_{n_2}} {n_2 - 1}}} \\sim t(v)\\]\n\nwith \\(v\\) close to \\((\\dfrac{S^2_{n_1}} {n_1 - 1} + \\dfrac{S^2_{n_2}} {n_2 - 1})^2 / (\\dfrac{S^4_{n_1}} {(n_1 - 1)^3} + \\dfrac{S^4_{n_2}} {(n_2 - 1)^3})\\) - The critical values are read on a student distribution table.\n\nExample:\nUsing the same example and the default t.test, we perform a Welch test:\n\nt.test(assignment, no_assignment)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  assignment and no_assignment\n#&gt; t = 2.8539, df = 15.835, p-value = 0.01158\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;   1.632084 11.090139\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;  87.58333  81.22222\n\nThe null hypothesis is again rejected. Which confirms you need to do your assignments!",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypotheses testing - Two samples tests</span>"
    ]
  },
  {
    "objectID": "064_hypotheses_two_samples.html#tests-about-two-variances",
    "href": "064_hypotheses_two_samples.html#tests-about-two-variances",
    "title": "22  Hypotheses testing - Two samples tests",
    "section": "22.2 Tests about two variances",
    "text": "22.2 Tests about two variances\n\n22.2.1 Normal distribution and unknown means - F test (Fisher-Snedecor Test)\nWe consider a random variable \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and a random variable \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) with \\(\\mu_1\\) and \\(\\mu_2\\) unknown, and \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) known. We use two samples of size \\(n_1\\): \\(x = {x_1, x_2,..., x_{n_1}}\\) and \\(n_n\\): \\(y = {y_1, y_2,..., y_{n_2}}\\) made of independent realizations of that random variables \\(X\\) and \\(Y\\). \\(n_1\\) and \\(n_2\\) can have different values.\n\nTesting: \\(H_0: \\sigma^2_1 = \\sigma^2_2\\) against \\(H_1: \\sigma^2_1 \\neq  \\sigma^2_2\\).\nTest Statistic: random variable defined by \\[F = \\dfrac{S^2_{n_1,c}}{S^2_{n_2,c}} \\sim F(n_1 - 1, n_2 - 1)\\]\nThe critical values are read on a Fisher distribution table.\n\n\nExample\n\nvar.test(assignment, no_assignment)\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  assignment and no_assignment\n#&gt; F = 0.74426, num df = 11, denom df = 8, p-value = 0.6345\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  0.1753913 2.7268254\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;          0.7442577\n\nThe p-value is higher in this case than the default significance level (0.05) so you cannot reject the null hypothesis that the ratio of the two variances is 1 (i.e. equal variances). There is no difference in the variance of the exam results beween those who completed their regular assignments and those who didn’t.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypotheses testing - Two samples tests</span>"
    ]
  },
  {
    "objectID": "064_hypotheses_two_samples.html#tests-about-two-proportions",
    "href": "064_hypotheses_two_samples.html#tests-about-two-proportions",
    "title": "22  Hypotheses testing - Two samples tests",
    "section": "22.3 Tests about two proportions",
    "text": "22.3 Tests about two proportions\nSimilar to the one sample case, we can have two proportions and ask whether they equal (i.e. their ratio is one). They may have different sample size, which is a case you encounter quite often with a yes/no question asked to two imbalanced samples (two classes, smokers/non-smokers,…)\n\nprop.test(x = c(50, 60), n = c(150, 200))\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  c(50, 60) out of c(150, 200)\n#&gt; X-squared = 0.30078, df = 1, p-value = 0.5834\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.07111329  0.13777996\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2 \n#&gt; 0.3333333 0.3000000\n\nIn this case the null hypothesis is not rejected and proportions can be said to be similar.\nThe test uses the chi-squared approximation, which may not be valid for smaller samples. See the warning here:\n\nprop.test(x = c(5, 6), n = c(15, 20))\n#&gt; Warning in prop.test(x = c(5, 6), n = c(15, 20)): Chi-squared approximation may\n#&gt; be incorrect\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  c(5, 6) out of c(15, 20)\n#&gt; X-squared = 1.3349e-31, df = 1, p-value = 1\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.3118426  0.3785093\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2 \n#&gt; 0.3333333 0.3000000\n\nIn this case an exact test would be conducted. An exact fisher.test() can be performed but requires that the data is structured as a matrix: counts of “yes” and “no”, rather than the “yes” and size “n” we provided so far to the prop.test().\nWe can rewrite our data as suggested for a prop.test() as well (see help prop.test(): a two-dimensional table (or matrix) with 2 columns, giving the counts of successes and failures, respectively ). It is usually the way we have it after a table or aggregate sum anyway. It is a contingency table where one of the factor is the yes/no answer. See our chapter on cross-tabulation.\n\nD&lt;-matrix(c(5, 10, 6, 14),ncol=2)\nD\n#&gt;      [,1] [,2]\n#&gt; [1,]    5    6\n#&gt; [2,]   10   14\naddmargins(D)\n#&gt;           Sum\n#&gt;      5  6  11\n#&gt;     10 14  24\n#&gt; Sum 15 20  35\n\nThe exact test:\n\nfisher.test(D)\n#&gt; \n#&gt;  Fisher's Exact Test for Count Data\n#&gt; \n#&gt; data:  D\n#&gt; p-value = 1\n#&gt; alternative hypothesis: true odds ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  0.2139426 6.1311561\n#&gt; sample estimates:\n#&gt; odds ratio \n#&gt;   1.161511\n\nAnd rerunning prop.test() for both the small and large dataset shows that using the contingency matrix structure holds the same result.\n\nprop.test(D)\n#&gt; Warning in prop.test(D): Chi-squared approximation may be incorrect\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  D\n#&gt; X-squared = 1.361e-31, df = 1, p-value = 1\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.3542429  0.4300004\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2 \n#&gt; 0.4545455 0.4166667\nD2&lt;-matrix(c(50, 100, 60, 140),ncol=2)\nprop.test(D2)\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  D2\n#&gt; X-squared = 0.30078, df = 1, p-value = 0.5834\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.08077143  0.15652901\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2 \n#&gt; 0.4545455 0.4166667\n\nNote that applied to a 2x2 case, i.e. comparing the proportions of successes between two groups, is a particular case of more general case where we would test if there is a significant association between two categorical variables , irrespective of the number of levels, i.e. resulting from any contingency table. This more general independence test is a chi-squared test (see next chapter).\nWhich means that the following two are technically equivalent:\n\nprop.test(D2)\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  D2\n#&gt; X-squared = 0.30078, df = 1, p-value = 0.5834\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.08077143  0.15652901\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2 \n#&gt; 0.4545455 0.4166667\nchisq.test(D2)\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  D2\n#&gt; X-squared = 0.30078, df = 1, p-value = 0.5834",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Hypotheses testing - Two samples tests</span>"
    ]
  },
  {
    "objectID": "065_hypotheses_contingency.html",
    "href": "065_hypotheses_contingency.html",
    "title": "23  Hypotheses testing - contingency tables",
    "section": "",
    "text": "23.1 Chi-squared test of independence\nThe chi-squared test for independence assesses whether the distribution of sample categorical data matches the expected distribution under the assumption of independence between the two variables.\nIf the observed frequencies in a contingency table that is cross tabulating two categorical variables, significantly deviate from the expected frequencies, it suggests that there is an association between some categories (levels) of the two variables (factors).\nThe chi-squared test of independence is formalised as follows:\nGiven two random qualitative/factorial variables \\(X\\) and \\(Y\\), we aim to determine whether there is a dependent or independent relationship between them.\nIn other words we want to know if the observations are distributed within the cells of the contingency table (cross tabulation of counts) according to their respective share or if they are over- or under-represented in one or more cells.\nSuppose there are \\(I\\) categories \\(i\\) for \\(X\\) and \\(J\\) categories \\(j\\) for \\(Y\\). Let \\(exp_{i,j}\\) represent the theoretical frequencies and \\(obs_{i,j}\\) the observed ones.\nTesting: \\(H_0: obs_{i,j} = exp_{i,j}\\) \\(\\forall i,j\\)\nTest Statistic: a random variable defined by\n\\[ \\chi^2(\\text{obs}) = \\sum_{i=1}^I \\sum_{j=1}^J \\dfrac{(\\text{obs}_{i,j} - \\text{exp}_{i,j})^2}{\\text{exp}_{i,j}}\\]\n\\[\\chi^2(\\text{obs}) \\sim \\chi^2((I-1)(J-1)) \\] where \\((I-1)(J-1)\\) is the degrees of freedom.\nThe expectation in each cell is simply based on the occurrence of each categroy independently, that is \\[ \\text{exp}_{i,j} = \\frac{(\\text{obs}_{i \\cdot} \\cdot \\text{obs}_{\\cdot j})}{n} \\] where - {i } ) is the total number of observations in row \\(i\\) - ) is the total number of observations in column \\(j\\) and \\(n\\) is the total count, i.e. $n=^I* =^J* $",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypotheses testing - contingency tables</span>"
    ]
  },
  {
    "objectID": "065_hypotheses_contingency.html#chi-squared-test-of-independence",
    "href": "065_hypotheses_contingency.html#chi-squared-test-of-independence",
    "title": "23  Hypotheses testing - contingency tables",
    "section": "",
    "text": "Note: - When there are only two categories for the two variables, we have seen earlier it is similar to a test of proportions between two samples - The chi-squared test for adequation/goodness of fit we have seen earlier considers one variable only (could be with more than 2 levels) but uses the same function applied to a case where the contingency table has only one row.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypotheses testing - contingency tables</span>"
    ]
  },
  {
    "objectID": "065_hypotheses_contingency.html#example",
    "href": "065_hypotheses_contingency.html#example",
    "title": "23  Hypotheses testing - contingency tables",
    "section": "23.2 Example",
    "text": "23.2 Example\nSuppose the following matrix representing counts of individuals from 3 countries using one of 4 modes of transportation:\n\nn&lt;-1000\nset.seed(15)\np&lt;-runif(12)\nv&lt;-matrix(round(n*p/sum(p)),ncol=4)\nrownames(v)&lt;-c(\"LU\",\"BE\",\"FR\")\ncolnames(v)&lt;-c(\"foot\",\"cycle\",\"bus\",\"car\")\naddmargins(v)\n#&gt;     foot cycle bus car  Sum\n#&gt; LU    85    92 115 117  409\n#&gt; BE    27    52  36  15  130\n#&gt; FR   136   139  97  91  463\n#&gt; Sum  248   283 248 223 1002\n\nIf the matrix is turned into a table, a kind of bar plot can represent those occurrences visually\n\nplot(as.table(v))\n\n\n\n\n\n\n\n\nThe chi-square test (applicable to the matrix ot to to the table) is returned as follows:\n\nchisq.test(v)\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  v\n#&gt; X-squared = 39.658, df = 6, p-value = 5.317e-07\n\nThe null hypothesis is that the joint distribution of the cell counts is the product of the row and column marginal proportions.\nIn addition to the test itself, the function returns the sample and the expected joint distributions:\n\nG&lt;-chisq.test(v)\nG$observed\n#&gt;    foot cycle bus car\n#&gt; LU   85    92 115 117\n#&gt; BE   27    52  36  15\n#&gt; FR  136   139  97  91\nG$expected\n#&gt;         foot     cycle       bus       car\n#&gt; LU 101.22954 115.51597 101.22954  91.02495\n#&gt; BE  32.17565  36.71657  32.17565  28.93214\n#&gt; FR 114.59481 130.76747 114.59481 103.04291\n\nThe observed matrix is the one we provided and the expected matrix is simply the multiplication of the two margin proportions for each item, multiplied by n to turn proportions into occurrences:\n\nrowSums(v)/n\n#&gt;    LU    BE    FR \n#&gt; 0.409 0.130 0.463\ncolSums(v)/n\n#&gt;  foot cycle   bus   car \n#&gt; 0.248 0.283 0.248 0.223\n\nFor example the number of car drivers from France is expected to be\n\nn*(rowSums(v)/n)[3]*(colSums(v)/n)[4]\n#&gt;      FR \n#&gt; 103.249\n\nWe can thus see that a chi-square test is simply an answer to “Knowing the marginal proportions, what count can you expect for each item ?” and then “Is the total difference over all items a significant one?” In the case the observed counts per cell differ from the expected based on marginal proportions, it does mean that one or several categories of one variable is affected by the categories of the other variable, e.g. transportation mode is not independent from the country of origin.\nIn addition to the test, the observed and epected counts, the function also returns the residual. The residual is the difference between observed and expected counts in each cell, but is divided by the square root of the expected value. Since you expect that difference to be larger for a larger occurrence you actually want to get rid of that size effect with this division and better compare cells whether they have a high or low expectation.\nNote that in the case of smaller samples, expected values will get smaller as well. The chi-square() function will not be considered safe and returns a warning as soon one cell is expected to be less than 5.\nSee for example the output with a smaller sample, while keeping the same observed proportions:\n\nn2&lt;-100\nset.seed(15)\nv2&lt;-matrix(round(n2*p/sum(p)),ncol=4)\nrownames(v2)&lt;-c(\"LU\",\"BE\",\"FR\")\ncolnames(v2)&lt;-c(\"foot\",\"cycle\",\"bus\",\"car\")\naddmargins(v2)\n#&gt;     foot cycle bus car Sum\n#&gt; LU     8     9  11  12  40\n#&gt; BE     3     5   4   1  13\n#&gt; FR    14    14  10   9  47\n#&gt; Sum   25    28  25  22 100\nG2&lt;-chisq.test(v2)\n#&gt; Warning in chisq.test(v2): Chi-squared approximation may be incorrect\n\nYou see that several cells are expected to be below 5:\n\nG2$expected\n#&gt;     foot cycle   bus   car\n#&gt; LU 10.00 11.20 10.00  8.80\n#&gt; BE  3.25  3.64  3.25  2.86\n#&gt; FR 11.75 13.16 11.75 10.34\n\nAnd the conclusion drawn from the small sample is different than with the large one, since we cannot now reject the null hypothesis and must conclude that transportation is independent from the country of origin.\n\nG\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  v\n#&gt; X-squared = 39.658, df = 6, p-value = 5.317e-07\nG2\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  v2\n#&gt; X-squared = 4.9246, df = 6, p-value = 0.5535\n\nIn such circumstances where you see that the Belgian cases were too few, it may make sense to aggregate some levels. Let’s group Belgians and Luxembourgers together:\n\nvgr&lt;-v2  #copy previous table\nvgr[1,]&lt;-vgr[2,]+vgr[1,] #sum BE and LU into first row\nvgr&lt;-vgr[c(1,3),] #keep only first row (BE+LU) and FR row\nrownames(vgr)&lt;-c(\"BE+LU\",\"FR\") #adapt name\n\nWhich turns the chi squared test into a valid one this time:\n\nG3&lt;-chisq.test(vgr)\nG3\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  vgr\n#&gt; X-squared = 1.7335, df = 3, p-value = 0.6295\nG3$expected\n#&gt;        foot cycle   bus   car\n#&gt; BE+LU 13.25 14.84 13.25 11.66\n#&gt; FR    11.75 13.16 11.75 10.34\n\nStill, given the small sample, you see the null hypothesis can’t be rejected (but the chi.square test is relevant now)",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypotheses testing - contingency tables</span>"
    ]
  },
  {
    "objectID": "065_hypotheses_contingency.html#exercise",
    "href": "065_hypotheses_contingency.html#exercise",
    "title": "23  Hypotheses testing - contingency tables",
    "section": "23.3 Exercise",
    "text": "23.3 Exercise\nExplore a number of cross-tabulation tables from the Luxembourg2021 (census) data folder.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hypotheses testing - contingency tables</span>"
    ]
  },
  {
    "objectID": "066_hypotheses_normality.html",
    "href": "066_hypotheses_normality.html",
    "title": "24  Normality Test",
    "section": "",
    "text": "24.1 Shapiro-Wilk Test\nTesting: \\(H_0: X\\) is normally distributed\nThe Shapiro-Wilk test assess how well an observed variable fits a normal distribution. It is similar to testing the correlation between the ordered sample values and the expected ones from a normal distribution.\nIt computes a W statistic, which measures how well the ordered sample values match the expected values from the normal distribution. If they don’t correlate well, the W statistic is significantly lower than expected and the null hypothesis of normality is rejected.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normality Test</span>"
    ]
  },
  {
    "objectID": "066_hypotheses_normality.html#basic-example.",
    "href": "066_hypotheses_normality.html#basic-example.",
    "title": "24  Normality Test",
    "section": "24.2 Basic example.",
    "text": "24.2 Basic example.\nSuppose a data of 10 values. We are not quite sure they are normally distributed after looking at the histogram or the qq plot against a normal distribution (qqnorm())\n\ndata &lt;- c(5, 7, 8, 9, 11, 10, 7, 8, 9, 11)\nn&lt;-length(data)\n\nhist(data)\n\n\n\n\n\n\n\nqqnorm(data)\n\n\n\n\n\n\n\n\nWe perform a shapiro-wilk test as follows:\n\nshapiro.test(data)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  data\n#&gt; W = 0.95259, p-value = 0.6992\n\nshowing we can’t reject the null hypothesis. There is no significant evidence against the data being normally distributed.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normality Test</span>"
    ]
  },
  {
    "objectID": "066_hypotheses_normality.html#decomposing-shapiro-wilk-test",
    "href": "066_hypotheses_normality.html#decomposing-shapiro-wilk-test",
    "title": "24  Normality Test",
    "section": "24.3 Decomposing Shapiro-wilk test",
    "text": "24.3 Decomposing Shapiro-wilk test\nWe have seen earlier that an approach to check normality could be to use a chi-squared test after binning and checking observed counts and expected counts along a normal distribution (goodness of fit chi-squared). The approach of Shapiro-Wilk is technically closer to a correlation coefficient. See how the W statistic is obtained:\nFirst, the data is sorted and deviations to the mean computed:\n\nn &lt;- length(data)\nsorted_data &lt;- sort(data)\ndev_sort&lt;-sorted_data-mean(data)\ndev_sort\n#&gt;  [1] -3.5 -1.5 -1.5 -0.5 -0.5  0.5  0.5  1.5  2.5  2.5\n\nSecond, expected values from a standard normal distribution are computed based on qnorm() function:\n\nqnorm((1:n) / (n))\n#&gt;  [1] -1.2815516 -0.8416212 -0.5244005 -0.2533471  0.0000000  0.2533471\n#&gt;  [7]  0.5244005  0.8416212  1.2815516        Inf\n\nexpected_vals &lt;- qnorm((1:n - 0.375) / (n + 0.25))\nexpected_vals\n#&gt;  [1] -1.5466353 -1.0004905 -0.6554235 -0.3754618 -0.1225808  0.1225808\n#&gt;  [7]  0.3754618  0.6554235  1.0004905  1.5466353\n\nAs you can see the values are not exactly those from the qnorm() function to avoid an infinity and to correct for small sample size (see the addition of -0.375 and 0.25 terms)\nThose expected values are then scaled after the sum of squares (and the units become irrelevant) to make expected weights\n\nweights &lt;- expected_vals / sqrt(sum(expected_vals^2))\n\nThen those weights are multiplied by the observed deviations to the mean. This cross-product (see numerator) is where it is similar to a correlation coefficient: if both observed and expected vary together the value is high:\n\nnumerator &lt;- sum(weights * dev_sort)^2\ndenominator &lt;- sum(dev_sort^2)\nW&lt;-numerator/denominator\nW\n#&gt; [1] 0.9587322",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normality Test</span>"
    ]
  },
  {
    "objectID": "066_hypotheses_normality.html#example",
    "href": "066_hypotheses_normality.html#example",
    "title": "24  Normality Test",
    "section": "24.4 Example:",
    "text": "24.4 Example:\n\nlibrary(AER)\n#&gt; Warning: package 'AER' was built under R version 4.4.1\n#&gt; Loading required package: car\n#&gt; Loading required package: carData\n#&gt; Loading required package: lmtest\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; Loading required package: sandwich\n#&gt; Loading required package: survival\ndata(Parade2005)\nattach(Parade2005)\n\nshapiro.test(earnings)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  earnings\n#&gt; W = 0.27025, p-value &lt; 2.2e-16\nshapiro.test(log(earnings))\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  log(earnings)\n#&gt; W = 0.6562, p-value = 5.06e-16\nshapiro.test(earnings[celebrity=='no'])\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  earnings[celebrity == \"no\"]\n#&gt; W = 0.51054, p-value &lt; 2.2e-16\n\nqqnorm(scale(earnings))\n\n\n\n\n\n\n\nqqnorm(scale(earnings[earnings &lt; 100000]))",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normality Test</span>"
    ]
  },
  {
    "objectID": "067_power_contingency.html",
    "href": "067_power_contingency.html",
    "title": "25  Power analysis - contingency table",
    "section": "",
    "text": "25.1 Example\nWe have two categorical variables, A and B, with the following marginal proportions: - Variable A: 3 categories (A1, A2, A3), e.g. “rural”, “urban”, “suburban”) - Variable B: 2 categories (B1, B2), e.g. “not vegeterian”, “vegetarian”\nIn the population we know that freqeuncies are not homogeneously distributed across the categories. Suppose the following marginal proportions. They are typically known form a census of residential places and a nation-wide study of vegetarianism:\nOur goal is to find the minmum total sample size \\(N\\) needed for a link between A and B, i.e. urbanity and vegetarianism to be detected. From \\(N\\) and the proportions of the A variable (rural, urban or suburban people), we can then invite a relevant number of participants based on their residential place.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power analysis - contingency table</span>"
    ]
  },
  {
    "objectID": "067_power_contingency.html#example",
    "href": "067_power_contingency.html#example",
    "title": "25  Power analysis - contingency table",
    "section": "",
    "text": "\\(P(A1) = 0.3\\)\n\\(P(A2) = 0.4\\)\n\\(P(A3) = 0.3\\)\n\\(P(B1) = 0.8\\)\n\\(P(B2) = 0.2\\)",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power analysis - contingency table</span>"
    ]
  },
  {
    "objectID": "067_power_contingency.html#power-analysis-elements",
    "href": "067_power_contingency.html#power-analysis-elements",
    "title": "25  Power analysis - contingency table",
    "section": "25.2 Power analysis elements",
    "text": "25.2 Power analysis elements\nThe power analysis will determine the sample size required to detect an effect of a given size with a given degree of confidence. It requires you to define three parameters, one of which i salpha which we know already well:\n\nPower ((1-)): the probability that the test will correctly reject the null hypothesis when the alternative hypothesis is true (see type of errors). A power of 0.80 is commonly used, meaning there is an 80% chance of detecting a true effect.\nSignificance Level (()): the probability of rejecting the null hypothesis when it is actually true (Type I error). Again we can use the typical value of 0.05.\nEffect Size: this is a measure of the magnitude of the phenomenon we want to be able to identify. In chi-square tests, Cohen’s \\(w\\) is often used to quantify this effect size. Cohen’s \\(w\\) is easily calculated from the chi-squared statistics applied to the contingency table. It is defined as\n\n\\[w = \\sqrt{\\frac{\\chi^2}{N}}\\]\nwhere \\(\\chi^2\\) is the chi-square statistic and \\(N\\) is the total sample size.\nA standard approach is to fix Cohen’s \\(w=0.3\\), which is said to be a medium effect. We then see that if \\(w\\) is fixed we obtain \\(N\\) from the \\(\\chi^2\\):\n\\[N = \\sqrt{\\frac{\\chi^2}{w}}\\] ## Implementation\nFirst we load the power analysis package pwr and define our parameters:\n\nlibrary(pwr)\n\n# Define the parameters\npower &lt;- 0.80 # Desired power level\neffectw &lt;- 0.3 # Medium effect size Cohen w\nalpha &lt;- 0.05 # Significance level\n\ndf &lt;- 2 # Degrees of freedom for a 3x2 table, i.e (3-1)(2-1)=2\n\nSecond, we compute the power based on the chi-squared test:\n\npwr.chisq.test(w = effectw, df = df, sig.level = alpha, power = power)\n#&gt; \n#&gt;      Chi squared power calculation \n#&gt; \n#&gt;               w = 0.3\n#&gt;               N = 107.0521\n#&gt;              df = 2\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.8\n#&gt; \n#&gt; NOTE: N is the number of observations\n\nThe output indicates the total sample size required to achieve the desired power and effect size.\nThe above doesn’t make use of the observed margins proportions which we use simply now to allocate the total sample size according to the proportions for each category of variable A, that is a stratified sampling:\n\npwrresult&lt;-pwr.chisq.test(w = effectw, df = df, sig.level = alpha, power = power)\n\nceiling(pwrresult$N*c(A1=0.3,A2=0.4,A3=0.3))\n#&gt; A1 A2 A3 \n#&gt; 33 43 33",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power analysis - contingency table</span>"
    ]
  },
  {
    "objectID": "067_power_contingency.html#effect-size-and-degrees-of-freedom",
    "href": "067_power_contingency.html#effect-size-and-degrees-of-freedom",
    "title": "25  Power analysis - contingency table",
    "section": "25.3 Effect size and degrees of freedom",
    "text": "25.3 Effect size and degrees of freedom\nWith alpha being set at 0.05 and the pover level 1-beta set to 0.80, we can cross-tabulate the effect size \\(w\\) against the degrees of freedom to get an idea of the effect of being more or less demanding and the effect of aggregating/subdividing categories:\n\ndfs&lt;-1:6\nws&lt;-seq(0.2,0.5, by=0.05)\nD&lt;-data.frame(df=rep(dfs, each=length(ws)),w=rep(ws,length(dfs)))\n\nD$N&lt;-apply(D, 1, function(x){\n  pwrresult&lt;-pwr.chisq.test(w = x[2], df = x[1],\n                 sig.level = 0.05, power = 0.8)\n  ceiling(pwrresult$N)\n  })\nD\n#&gt;    df    w   N\n#&gt; 1   1 0.20 197\n#&gt; 2   1 0.25 126\n#&gt; 3   1 0.30  88\n#&gt; 4   1 0.35  65\n#&gt; 5   1 0.40  50\n#&gt; 6   1 0.45  39\n#&gt; 7   1 0.50  32\n#&gt; 8   2 0.20 241\n#&gt; 9   2 0.25 155\n#&gt; 10  2 0.30 108\n#&gt; 11  2 0.35  79\n#&gt; 12  2 0.40  61\n#&gt; 13  2 0.45  48\n#&gt; 14  2 0.50  39\n#&gt; 15  3 0.20 273\n#&gt; 16  3 0.25 175\n#&gt; 17  3 0.30 122\n#&gt; 18  3 0.35  90\n#&gt; 19  3 0.40  69\n#&gt; 20  3 0.45  54\n#&gt; 21  3 0.50  44\n#&gt; 22  4 0.20 299\n#&gt; 23  4 0.25 191\n#&gt; 24  4 0.30 133\n#&gt; 25  4 0.35  98\n#&gt; 26  4 0.40  75\n#&gt; 27  4 0.45  59\n#&gt; 28  4 0.50  48\n#&gt; 29  5 0.20 321\n#&gt; 30  5 0.25 206\n#&gt; 31  5 0.30 143\n#&gt; 32  5 0.35 105\n#&gt; 33  5 0.40  81\n#&gt; 34  5 0.45  64\n#&gt; 35  5 0.50  52\n#&gt; 36  6 0.20 341\n#&gt; 37  6 0.25 218\n#&gt; 38  6 0.30 152\n#&gt; 39  6 0.35 112\n#&gt; 40  6 0.40  86\n#&gt; 41  6 0.45  68\n#&gt; 42  6 0.50  55\n\nGraphically:\n\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(data=D)+\n  geom_line(aes(x=df,y=N, group=factor(w), col=w))+\n  theme_bw()\n\n\n\n\n\n\n\n\nOr a two entries table to find N\n\nreshape2::dcast(D, df ~ w, value.var=\"N\")\n#&gt;   df 0.2 0.25 0.3 0.35 0.4 0.45 0.5\n#&gt; 1  1 197  126  88   65  50   39  32\n#&gt; 2  2 241  155 108   79  61   48  39\n#&gt; 3  3 273  175 122   90  69   54  44\n#&gt; 4  4 299  191 133   98  75   59  48\n#&gt; 5  5 321  206 143  105  81   64  52\n#&gt; 6  6 341  218 152  112  86   68  55",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power analysis - contingency table</span>"
    ]
  },
  {
    "objectID": "071_correlation.html",
    "href": "071_correlation.html",
    "title": "26  Correlation and regression",
    "section": "",
    "text": "26.1 Double objective:\nCorrelation comes first, it is more exploratory",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "071_correlation.html#double-objective",
    "href": "071_correlation.html#double-objective",
    "title": "26  Correlation and regression",
    "section": "",
    "text": "Explaining the spatial distribution of a process using another or several other processes.\n\n\nA correlation examines the degree of association (no direction of causality)\nA regression identifies a process to be explained, represented by a dependent variable and one or several explanatory processes, the independent variables\n\n\n\nPredictingthe spatial distribution of a process \\(Y\\), given the spatial distribution of an associated process \\(X\\)\n\n\nNot necessarily looking for explanations\nBut predicting with a measured precision",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "071_correlation.html#covariation-covariance-and-correlation",
    "href": "071_correlation.html#covariation-covariance-and-correlation",
    "title": "26  Correlation and regression",
    "section": "26.2 Covariation, Covariance and Correlation",
    "text": "26.2 Covariation, Covariance and Correlation\nLet \\(X\\) and \\(Y\\) be - cardinal (ratio) numeric variables - varying across space, i.e. in every \\(i\\) of \\(n\\) places\nTo what degree do the two variables vary together ?\nTo what degree are they interdependent ?\nLet’s define:\n\nMean: \\(\\bar{X}\\), \\(\\bar{Y}\\)\nDeviation to the mean: \\(x_i = X_i - \\hat{X}\\) and \\(y_i = Y_i - \\hat{Y}\\)\nCovariation: sum over each place of the product of the deviations in \\(X\\) and \\(Y\\). \\[Covariation (X,Y) = \\sum_{i=1}^n x_i y_i\\]\n\nThe idea of a covariation is to create a value that increases when observations vary ‘together’ along the two variables. If an observation i is just above the mean in X but far above the mean in Y, the product is rather small. But if is far from the mean in both X and Y the product of both is very high. However, everything else equal, covariation increases with the number of observations\n\nCovariance: \\[Cov(X,Y) = \\dfrac {\\sum_{i=1}^n x_i y_i} {n}\\]\n\nThe covariance embeds the same idea as the covariation but resolves the issue of sample size with \\(n\\) at the denominator. Two samples of the same two variables can thus be compared even if they have different numbers of observations.\nHowever, if those two samples have different dispersion (e.g. weight measured over a different range of ages), the one with the larger dispersion will still have a larger covariance and impact our understanding of how \\(X\\) and \\(Y\\) are related.\n\nCorrelation coefficient (Pearson… no assumption on random distribution): \\[\\rho_{XY} = \\dfrac {Cov(X,Y)} {\\sigma_X \\sigma_Y}\\]\n\nwith \\(\\sigma_X\\) and \\(\\sigma_Y\\) the standard deviations.\nThe correlation resolves the issue of a different dispersion by dividing the covariance by the product of the standard deviations. Doing so, the measurement units also disappear, hence it is comparable even if we would measure one of the variable using a different (linearly related) metric. For example if we had \\(X\\) in [kg] and \\(Y\\) in [cm], the covariance would be in [kg x cm] and not directly comparable to another set where [g] and [mm] would be used. With the correlation, we no longer care since we divide by the same product of units as the denominator",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "071_correlation.html#correlation-coefficient-characteristics",
    "href": "071_correlation.html#correlation-coefficient-characteristics",
    "title": "26  Correlation and regression",
    "section": "26.3 Correlation coefficient characteristics",
    "text": "26.3 Correlation coefficient characteristics\n\nis a pure number (no units)\nis invariant to any linear transformation (including standardization)\nindicates whether there is a linear relationship between \\(X\\) and \\(Y\\).\n\\[−1≤ \\rho_{XY} ≤1\\]\n\\(\\rho\\) is close to 1 if the slope of the line that can be drawn in the plot is positive\n\\(\\rho\\) is close to -1 if the slope of the line that can be drawn in the plot is negative\n\\(\\rho\\) is close to 0 if the plot has no particular form. Note that it does not mean that the variables are independent but that the relationship is non linear !\n\n\nset.seed(123)\nx&lt;-c(1,1.5,1.5,2,2.5,3,4,4,4.5,4.5,5.5,6,6.5)\ny1&lt;-c(10,15,12,17,30,28,40,30,44,40,59,58,67)\ny2&lt;-mean(y1)+runif(length(y1),min = -30, max=30)\ny3&lt;-70-y1\npar(mfrow = c(1,3))\nplot(x,y1,ylim=c(1,70), col=\"black\", pch=16)\nplot(x,y2,ylim=c(1,70), col=\"black\", pch=16)\nplot(x,y3,ylim=c(1,70), col=\"black\", pch=16)\n\n\n\n\n\n\n\n\nOn the above figure, the correlations are respectively:\n\ncor(x,y1)\n#&gt; [1] 0.9786531\ncor(x,y2)\n#&gt; [1] 0.1299222\ncor(x,y3)\n#&gt; [1] -0.9786531\n\nSee that the order of x and y doesn’t matter and that if x (or y) is added a fixed number and multiplied by another, i.e. undergoes a linear transformation, the correlation coefficient doesn’t change\n\ncor(y1,x)\n#&gt; [1] 0.9786531\ncor(5+10*x,y1)\n#&gt; [1] 0.9786531",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "071_correlation.html#correlation-confidence",
    "href": "071_correlation.html#correlation-confidence",
    "title": "26  Correlation and regression",
    "section": "26.4 Correlation confidence",
    "text": "26.4 Correlation confidence\nIf we have access to the total population, we know \\(X\\) and \\(Y\\) for every individual. The population correlation coefficient is usually denoted \\(\\rho_{XY}\\).\nUsually, however, only a sample of the population is observed. On that sample, we can calculate \\(r_{XY}\\), which is an estimation of \\(\\rho_{XY}\\).\nFrom \\(r_{XY}\\), confidence intervals can be calculated and thus hypotheses can be tested about \\(\\rho_{XY}\\).\nTest \\(H0: \\rho_{XY} = 0\\) (alternative: \\(H1: \\rho_{XY} \\ne0\\))\nSuppose \\(X\\), \\(Y\\) follow a normal distribution (or at least one of them).\nCalculate \\(t\\) statistic: \\[t_0 = \\dfrac {|r|\\sqrt{n-2}}{\\sqrt{1-r^2}} \\sim t_{n-2}\\] If \\(t_0 \\le t_{n-2}\\)\n\\(H0\\) is rejected when \\(P(t_0 \\le t_{n-2}) = \\alpha\\).\nThe relevant t.test is included in the cor.test() function. The test is applied to our example graphs\n\ncor.test(x,y1)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  x and y1\n#&gt; t = 15.793, df = 11, p-value = 6.617e-09\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.9281459 0.9937728\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.9786531\ncor.test(x,y2)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  x and y2\n#&gt; t = 0.43459, df = 11, p-value = 0.6723\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.4535291  0.6354207\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.1299222\ncor.test(x,y3)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  x and y3\n#&gt; t = -15.793, df = 11, p-value = 6.617e-09\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.9937728 -0.9281459\n#&gt; sample estimates:\n#&gt;        cor \n#&gt; -0.9786531\n\nThe null hypothesis is well rejected in the first and third cases. It cannot be rejected in the second case, showing there is no correlation.\nNote: using ordinal data requires other measures, e.g. Spearman or Kendall. These measures can be used with continuous data as well and may help identifying data problems (strange values) or asymetric distributions.",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html",
    "href": "072_regression.html",
    "title": "27  Regression",
    "section": "",
    "text": "27.1 Example\nTo illustrate the functioning of a regression model, we use a dataset from Ferguson (n.d.) (table 1 and 2) representing average precipitation and elevation across southern Scotland (between national grid lines 600 and 601 km N).\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nOur first variable of interest is Rainfall in mm/yr and the second is Elevation in m above sea level.\nAlways start with a scatterplot when possible:\nplot(x=RainScotland$Elevation, y=RainScotland$Rainfall,\n    xlab=\"X : Elevation (m)\", ylab=\"Y : Rainfall (mm)\")\nWe can see and measure that the two variables correlate positively and significantly:\ncor(RainScotland$Elevation, RainScotland$Rainfall)\n#&gt; [1] 0.7839175\ncor.test(RainScotland$Elevation, RainScotland$Rainfall)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  RainScotland$Elevation and RainScotland$Rainfall\n#&gt; t = 5.3568, df = 18, p-value = 4.317e-05\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.5227325 0.9105639\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.7839175",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#simple-regression-principle",
    "href": "072_regression.html#simple-regression-principle",
    "title": "27  Regression",
    "section": "27.2 Simple regression – principle",
    "text": "27.2 Simple regression – principle\nWhat is the straight line that best fits this plot ?\nLet’s draw one line and define the residual, \\(e_i\\), as the vertical difference between the ‘best line’ and the point \\((X_i, Y_i)\\).\nAs you know a straight line is entirely defined by its slope (\\(b\\)) and its intercept (\\(a\\)). With \\(a=700\\) and \\(b=3\\) we have quite a good proxy:\n\nplot(x=RainScotland$Elevation, y=RainScotland$Rainfall,\n    xlab=\"X : Elevation (m)\", ylab=\"Y : Rainfall (mm)\")\nl_a=700\nl_b=3\nabline(a=l_a,b=l_b, col=\"orange\")\ntext(x=RainScotland$Elevation[18]+20,y=RainScotland$Rainfall[18]+100,labels = \"e_18\", col=\"orange\")\nsegments(x0=RainScotland$Elevation[18],y0=RainScotland$Rainfall[18],\n         x1=RainScotland$Elevation[18], y1=l_a+l_b*RainScotland$Elevation[18], col=\"darkorange\",lwd=2)\n\n\n\n\n\n\n\n\nThere could be many lines to approximate the graph and for which residuals can be computed.\nRather that testing an infinity of line equations, we choose the line that minimises the sum of the square residuals. It is the least square line. The method is named the Ordinary Least Square estimation method (OLS)\nBeware! The result would differ if differences would be measured vertically!\n\nplot(x=RainScotland$Elevation, y=RainScotland$Rainfall,\n    xlab=\"X : Elevation (m)\", ylab=\"Y : Rainfall (mm)\")\nl_a=700\nl_b=3\nabline(a=l_a,b=l_b, col=\"orange\")\ntext(x=RainScotland$Elevation[18]+20,y=RainScotland$Rainfall[18]+100,labels = \"e_18\", col=\"orange\")\nsegments(x0=RainScotland$Elevation[18],y0=RainScotland$Rainfall[18],\n         x1=(RainScotland$Rainfall[18]-l_a)/l_b, y1=RainScotland$Rainfall[18], col=\"darkorange\",lwd=2)\n\n\n\n\n\n\n\n\nAs a result, the respective role of \\(X\\) and \\(Y\\) is not the same in regression (compare to correlation)!!\nRegression is not symetrical: - \\(Y\\) is the variable to be explained, the dependent variable - \\(X\\) is the independent variable",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#simple-regression-ols-estimates",
    "href": "072_regression.html#simple-regression-ols-estimates",
    "title": "27  Regression",
    "section": "27.3 Simple regression – OLS estimates",
    "text": "27.3 Simple regression – OLS estimates\nHow do we define the OLS line? How do we find its defining parameters \\(a\\) and \\(b\\)?\nWe have the set of points \\((X_i,Y_i)\\), \\(i=1,…,n\\).\nAny linear function is of the form \\[Y = a + bX\\], where the value for \\(a\\) and \\(b\\) must be found.\nThe residual in each \\(i\\) is thus defined by \\[e_i = Y_i - a - bX_i\\]\nAnd the OLS criteria writes as\n\\[\\min_{a,b} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - a - bX_i)^2\\]\nIts solution is: \\[a = \\bar{Y}-b\\bar{X}\\] The intercept is completely determined by the estimated slope \\(b\\) and the two sample means.\n\\[b = \\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\dfrac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2}\\] The denominator of \\(b\\) should ring a bell… it is again the sum of the product of the deviations to the mean in \\(X\\) and \\(Y\\), and is thus increasing when the two changes go along in the same direction. Conversely to the correlation coefficient, however, it is made relative to squared changes in \\(X\\) not to a product of standard deviations. It has therefore units and is expressed in units of \\(Y\\) per units of \\(X\\) (i.e in our case \\(b\\) mm of precipitation for each m of elevation)\nThe regression line (i.e. estimated \\(Y\\)) is denoted by: \\[\\hat{Y_i} = a + b X_i\\] and thus the residual is \\[e_i = Y_i - \\hat{Y_i}\\]\nIn R, we use the lm()function to obtain \\(a\\) and \\(b\\)\n\nlm(Rainfall~Elevation,data=RainScotland)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation, data = RainScotland)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Elevation  \n#&gt;     895.322        2.377\n\nWe can store this into an object for later and to retrieve the parameters of the line:\n\nOLS1&lt;-lm(Rainfall~Elevation,data=RainScotland)\nOLS1$coefficients\n#&gt; (Intercept)   Elevation \n#&gt;  895.322341    2.377353\n\nWhere we see the best fit using the OLS estimator leads to the following equation: \\[\\hat{Y}= \\text{Rainfall}  (mm) = 895 (mm) + 2.377 \\text{ Elevation} (m)\\]\nInterpretation:\n\nEvery increase of 1m in elevation leads to an additional 2.377 mm of water\nAt sea level, i.e. elevation 0, rainfall is expected to be 895 mm\nIf we observe an elevation of 350 m, we can expect a rainfall of \\(895 + 2.377 * 350= 1727 mm\\)\n\nWe can represent that equation on the graph by using directly its paramaters:\n\nplot(x=RainScotland$Elevation, y=RainScotland$Rainfall,\n    xlab=\"X : Elevation (m)\", ylab=\"Y : Rainfall (mm)\")\nOLS_a=OLS1$coefficients[1]\nOLS_b=OLS1$coefficients[2]\nabline(a=OLS_a,b=OLS_b, col=\"blue\")\ntext(x=350,y=1800,\"OLS\", col=\"blue\")",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#quality-of-the-fitprediction",
    "href": "072_regression.html#quality-of-the-fitprediction",
    "title": "27  Regression",
    "section": "27.4 Quality of the fit/prediction?",
    "text": "27.4 Quality of the fit/prediction?\n\nUse the following property: the mean prediction (estimation) equals the observed mean: \\(\\bar{Y_i} = \\dfrac{1}{n} \\sum_{i=1}^n \\hat{Y_i}\\)\nDefine the deviations of the estimated values to the mean: \\(\\hat{y_i} = \\hat{Y_i} - \\bar{Y}\\)\nThen the quality of the fit is\n\n\\[R^2 = \\dfrac{\\sum_{i=1}^n \\hat{y_i^2}}{\\sum_{i=1}^n y_i^2} = \\dfrac{\\text{Sum of squared differences to the mean that are explained}}{\\text{Sum of squared differences to the mean to be explained}} = (r_{XY})^2\\] with \\[0 \\le R^2 \\le 1\\]\nThis R^2 is displayed in the summary of the OLS model object, together with the estimated parameters and a series of test.\n\nsummary(OLS1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -486.08 -175.04   91.28  130.15  402.42 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 895.3223   149.7609   5.978 1.18e-05 ***\n#&gt; Elevation     2.3774     0.4438   5.357 4.32e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 242.8 on 18 degrees of freedom\n#&gt; Multiple R-squared:  0.6145, Adjusted R-squared:  0.5931 \n#&gt; F-statistic:  28.7 on 1 and 18 DF,  p-value: 4.317e-05\n\nThe R^2 can be retreived individually from the summary itself. It is indeed the square of the correlation coefficient:\n\nsummary(OLS1)$r.squared\n#&gt; [1] 0.6145266\ncor(RainScotland$Elevation, RainScotland$Rainfall)^2\n#&gt; [1] 0.6145266",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#testing",
    "href": "072_regression.html#testing",
    "title": "27  Regression",
    "section": "27.5 Testing:",
    "text": "27.5 Testing:\nIn order to know how significant are the estimated parameters and the explained variance, we need a hypothesis related to how good the line estimated with our sample points is to represnet the true relationship between rainfall and elevation.\nThis means formulating a statistical model and testing an hypothesis.\nThe statistical model assumes that the \\(X\\) values are given, i.e. observed with no further conditions on the distribution, and indicate how for a given value of \\(X\\), we can obtain a \\(Y\\) value.\n\n27.5.1 True (population) line and its estimation\nAssume a « true » regression equation \\[Y = \\alpha + \\beta X\\]\n\n#&gt; Error in knitr::include_graphics(c(\"img/reg1.png\", \"img/reg2.png\")): Cannot find the file(s): \"img/reg1.png\"; \"img/reg2.png\"\n\nTake any \\(X_0\\), which thus determines \\(Y'_0\\) from the true equation i.e. \\(Y'_0 = \\alpha + \\beta X_0\\). Then, add a value \\(\\epsilon_0\\) generated from a normal distribution \\(N(0,\\sigma^2)\\) to \\(Y’_0\\) in order to obtain \\(Y_0\\).\nRepeat for all individuals that make the population. The « true » regression line \\(Y = \\alpha + \\beta X\\) is unknown but can be estimated with OLS from those points. We call the error term \\(\\epsilon\\) for each point the difference between this line and the « true » line. Conversely, to residuals \\(e\\), error terms \\(\\epsilon\\) cannot be observed. Yet we have assumed \\(\\epsilon\\) come out of a normal distribution around the true line and so we can formulate expectations as to the distribution of residuals. And \\(a\\) et \\(b\\) of the OLS line are considered as good estimators of the parameters \\(\\alpha\\) and \\(\\beta\\) of the « true » regression line provided the distribution of residuals are normal.\n\n\n27.5.2 Hypotheses:\nThe null hypothesis: \\(H0: Y = \\alpha + \\epsilon\\) i.e. assumes that \\(X\\) is not useful and provides no information to find the corresponding value of \\(Y\\).\nThe alternative hypothesis is\n\\(H1: Y = \\alpha + \\beta X + \\epsilon\\) i.e. conversely, it says that \\(X\\) is useful to find the right level of \\(Y\\).\n\n#&gt; Error in knitr::include_graphics(\"img/reg8.png\"): Cannot find the file(s): \"img/reg8.png\"\n\nTests’ statistics:\n\nA Student t test for each \\(X\\) and (intercept)\nand a Fisher for the variance of the whole regression\n\n\nsummary(OLS1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -486.08 -175.04   91.28  130.15  402.42 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 895.3223   149.7609   5.978 1.18e-05 ***\n#&gt; Elevation     2.3774     0.4438   5.357 4.32e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 242.8 on 18 degrees of freedom\n#&gt; Multiple R-squared:  0.6145, Adjusted R-squared:  0.5931 \n#&gt; F-statistic:  28.7 on 1 and 18 DF,  p-value: 4.317e-05\n\nIn our example, we see the t.tests for both our intercept and the slope for Elevation are clearly significant. They are not zeros, we can interpret them. And so is the F-test for the total R squared.",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#residuals-and-fitted-values",
    "href": "072_regression.html#residuals-and-fitted-values",
    "title": "27  Regression",
    "section": "27.6 Residuals and fitted values:",
    "text": "27.6 Residuals and fitted values:\nFinally, the regression object stores - the estimated (fitted) values at each of the individual observation - and the individual residuals, which are terribly useful for mapping and checking that there is no other geographical (or not) structure into play.\nWe can easily append both fitted values and residuals to the original dataset:\n\nRainScotland_withmodels&lt;-RainScotland\nRainScotland_withmodels$fitOLS1&lt;-OLS1$fitted.values\nRainScotland_withmodels$residOLS1&lt;-OLS1$residuals\n\n\nplot(RainScotland_withmodels$Elevation,RainScotland_withmodels$Rainfall)\nabline(a=OLS_a,b=OLS_b, col=\"blue\")\npoints(RainScotland_withmodels$Elevation,RainScotland_withmodels$fitOLS1, col=\"blue\")\nsegments(x0=RainScotland_withmodels$Elevation,y0=RainScotland_withmodels$Rainfall, x=RainScotland_withmodels$Elevation,y=RainScotland_withmodels$fitOLS1, col=\"lightblue\")\n\n\n\n\n\n\n\n\nOne can also use the predict() function to estimate values at any points, even those that were not in the sample. For example we earlier asked what Rainfall is expected at 350m. If we have a data.frame with \\(X\\) values within a column of the same name (Elevation) we can apply the model. Suppose we want the answer for 350 and other multiples:\n\nNewData&lt;-data.frame(Elevation=c(350,700,1050,3500))\npredict(OLS1,NewData )\n#&gt;        1        2        3        4 \n#&gt; 1727.396 2559.470 3391.543 9216.059",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "072_regression.html#sum-of-residuals-and-the-rmse",
    "href": "072_regression.html#sum-of-residuals-and-the-rmse",
    "title": "27  Regression",
    "section": "27.7 Sum of residuals and the RMSE",
    "text": "27.7 Sum of residuals and the RMSE\nThe sum of the squared residuals (residualSS, which was minimized) can be obtained from the sum of the residuals. It is also named the deviance of the model.\n\nsum(resid(OLS1)^2)\n#&gt; [1] 1061062\ndeviance(OLS1)\n#&gt; [1] 1061062\n\nresidualSS is also by definition the totalSS minus the explainedSS while the \\(R^2\\) is the explainedSS/totalSS\n\ntotalSS&lt;- sum((RainScotland_withmodels$Rainfall - mean(RainScotland_withmodels$Rainfall))^2)\n\nexplainedSS&lt;-sum((RainScotland_withmodels$fitOLS1 - mean(RainScotland_withmodels$Rainfall))^2)\n\nresidualSS&lt;-totalSS-explainedSS\nresidualSS\n#&gt; [1] 1061062\nR2&lt;-explainedSS/totalSS\nR2\n#&gt; [1] 0.6145266\n\nIn some fields, another metric, the Root Mean Square Error (RMSE), is preferred over, or added to the R2 to express the performance of a regression. The RMSE is expressed in the same units as \\(Y\\) and tells us how distant on average are the predictions from the observations.\nAs its name indicates, the RMSE is the square root of the Mean Square Error, which is the deviance (i.e. residualSS) divided by the number of observations:\n\nMSE&lt;-deviance(OLS1)/nrow(RainScotland)\nMSE\n#&gt; [1] 53053.09\nRMSE&lt;-sqrt(MSE)\nRMSE\n#&gt; [1] 230.3326\n\nSo we can say that on average this model of using elevation to find rainfall levels leads to an error of around 230 mm of water per observation.\n\n\n\n\nFerguson, Rob. n.d. Linear Regression in Geography. Vol. 15. CATMOG (Concepts And Techniques in MOdern Geography). https://github.com/qmrg/CATMOG/blob/Main/15-linear-regression-in-geography.pdf.",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "073_CIregression.html",
    "href": "073_CIregression.html",
    "title": "28  Confidence interval of a regression",
    "section": "",
    "text": "28.1 Load Data and Packages\nWe use the Scotland rain data and load the ggplot package plus an additional related ggplot package (ggpmisc) to ease some formatting\nRainScotland &lt;- read.csv(\"data/Ferguson/RainScotland.csv\")\nlibrary(ggplot2)\nlibrary(ggpmisc)\n#&gt; Loading required package: ggpp\n#&gt; Registered S3 methods overwritten by 'ggpp':\n#&gt;   method                  from   \n#&gt;   heightDetails.titleGrob ggplot2\n#&gt;   widthDetails.titleGrob  ggplot2\n#&gt; \n#&gt; Attaching package: 'ggpp'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     annotate",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Confidence interval of a regression</span>"
    ]
  },
  {
    "objectID": "073_CIregression.html#scatter-plot",
    "href": "073_CIregression.html#scatter-plot",
    "title": "28  Confidence interval of a regression",
    "section": "28.2 Scatter plot",
    "text": "28.2 Scatter plot\nWe plot the data and add the OLS line, its equation, vertical residual lines and the confidence interval (pink area).\nWe emphasize two points (the 7th and 18th) for which we later calculate the confidence interval. We see 18 is located outside of the confidence interval and 7 is located right on the estimated line, but is also one of the largest points in terms of elevation.\n\np&lt;-ggplot(RainScotland, aes(x = Elevation, y = Rainfall))  +\n  stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")),formula = y ~ x, parse = TRUE) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\",fill=\"lightpink\", alpha = 0.3)+\n  geom_segment(aes(xend = Elevation, yend = predict(lm(Rainfall ~ Elevation, data = RainScotland))), linetype = \"dashed\", color = \"gray\", linewidth  = 0.5) +\n  labs(title = \"Rainfall as a function of Elevation\", x = \"Elevation (m)\", y = \"Rainfall (mm/yr)\")+\n  geom_point(data=RainScotland[18,], size=5, color=\"red\")+\n  geom_point(data=RainScotland[7,], size=5, color=\"lightblue\")+\n  geom_point()+\n  theme_minimal()\np\n#&gt; Warning: The dot-dot notation (`..eq.label..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(eq.label)` instead.\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Confidence interval of a regression</span>"
    ]
  },
  {
    "objectID": "073_CIregression.html#confidence-interval-calculation",
    "href": "073_CIregression.html#confidence-interval-calculation",
    "title": "28  Confidence interval of a regression",
    "section": "28.3 Confidence Interval calculation",
    "text": "28.3 Confidence Interval calculation\nWe now show how the pink area representing the 95% confidence interval is calculated.\nThe lower and upper bound for each predicted value can be obtained using the predict function. We can ask prediction and confidence bounds for any point (using newdata with the same variables as the original variables), but here we do it for all the observed ones and show the values for point 7 and 18.\n\nmodel &lt;- lm(Rainfall ~ Elevation, data = RainScotland)\n\npred &lt;- predict(model, newdata = data.frame(Elevation = RainScotland$Elevation), interval = \"confidence\")\n\npred[c(7,18),]\n#&gt;         fit      lwr      upr\n#&gt; 7  2131.546 1908.562 2354.530\n#&gt; 18 1537.208 1415.837 1658.579\n\npredicted_value18 &lt;- pred[18,\"fit\"]\npredicted_value7 &lt;- pred[7,\"fit\"]\n\nWe now compute the total residual standard error (RSE) of the model (also sometimes called the Root Mean Square Error though they are slightly different):\n\\[RSE=\\sqrt{\\frac{\\sum (\\text{residuals(model)})^2}{\\text{model degrees of freedom}}}\\] which measures the typical distance at which the observed values fall from the regression line. The residuals are the differences between the observed and predicted values (our vertical dashed lines on the figure).\nThe sum of squared residuals is divided by the degrees of freedom. In this case the degrees of freedom is 18 because we have n=20 individuals and p=2 estimated parameters: the slope (2.38mm/yr) and the intercept (elevation 895m).\n(NB: I understand the denominator \\(n-p\\) is where RSE differs from the Root Mean Square Error RMSE, which is often used to describe a fit, which is the mean, i.e. divided by \\(n\\))\nWe take the square root to get the RSE back to rainfall units, in this case it is equal to 243mm (compare to the RMSE being 230 mm).\n\nRSE&lt;-sqrt(sum(residuals(model)^2) / model$df.residual)\nRSE\n#&gt; [1] 242.7918\n\nNote that the RSE often denoted by sigma is also found in the summary of the regression model:\n\nsummary(model)$sigma\n#&gt; [1] 242.7918\n\nFor a specific observation, say the 18th or the 7th observation, we adjust the RSE for the known characteristics of the observation, i.e. its value along elevation, the predictor variable. More specifically if the observed elevation is very much away from the mean, it is going to have a large deviation to the mean, hence a large share of the total deviations (always squared to avoid mixing pluses and minuses) to the mean elevation in the sample and thus potentially more impact on the regression line.\nThese relative deviations are\n\nelevation_relative_dev18&lt;-(RainScotland$Elevation[18] - mean(RainScotland$Elevation))^2 / sum((RainScotland$Elevation - mean(RainScotland$Elevation))^2)\n\nelevation_relative_dev7&lt;-(RainScotland$Elevation[7] - mean(RainScotland$Elevation))^2 / sum((RainScotland$Elevation - mean(RainScotland$Elevation))^2)\n\nelevation_relative_dev18\n#&gt; [1] 0.006616382\nelevation_relative_dev7\n#&gt; [1] 0.1410991\n\nMathematically, the standard error of the fit at a given point ( ) is given by: \\[\n   \\text{SE}(\\hat{y}) = RSE\\sqrt{\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\n   \\]\nwhere we see it increases with the global variance (variance of residuals) of the model, i.e. RSE, decreases with the sample size n, and increases with the distance to the mean along the x variable, which is the relative deviation quantity we just computed (compare 7 and 18).\nLeaving aside the RSE, which we already calculated, we can combine the last two terms present under the square root and define the leverage of an observation (and simplify the standard error writing)\n\\[L=\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\\\\\n\\text{SE}(\\hat{y}) = RSE\\sqrt{L}\\]\nLeverage values range between \\(\\frac{1}{n}\\) and 1, with higher values indicating greater influence on the regression model. 1 is for an observation that would stand extremely far from the mean and taking most of the variation in x.\nPoints farther from the mean of x have higher leverage, have a greater influence on the regression line and thus can increase the uncertainty of the predicted line. In Greene (p.99) ’s terms:\n\n“the farther the forecasted point is from the center of our experience, the greater is the degree of uncertainty”\n\nThis is why the confidence intervals (pink ribbon) usually get wider away from the mean of x. This process is also reinforced by the density of data points, which is typically higher around the mean of x. The higher density providing more information and reducing uncertainty near the mean.\nThe equation above also shows that the leverage will decrease for every point as soon as n increases. When n increases the uncertainty decreases and the ribbon is narrower.\nTo compute the leverage for 7 and 18 we thus simply add \\(1/n\\) to our relative deviations:\n\nlever7&lt;-1/nrow(RainScotland)+elevation_relative_dev7\nlever18&lt;-1/ nrow(RainScotland)+elevation_relative_dev18\nlever7\n#&gt; [1] 0.1910991\nlever18\n#&gt; [1] 0.05661638\n\nWith p predictors (here p=1) and n observations (here n=20), a rule of thumb is to consider a leverage is too high when \\(\\frac{p+1}{n}\\) is significantly different from the average leverage. In our case, we have \\(\\frac{p+1}{n}=0.1\\) and observation 7 may be considered having too much influence. In practice however, the significance of a leverage is examined after removing high leverage points and evaluating the effect of this removal on the regression coefficients and the overall model fit.\nPursuing, we compute the standard errors of the fit at the level of our observations 7 and 18:\n\nse7&lt;-RSE*(lever7)^(1/2)\nse18&lt;-RSE*(lever18)^(1/2)\nse7\n#&gt; [1] 106.1362\nse18\n#&gt; [1] 57.77037\n\nIn order to draw the pink ribbon, we need to additionally set a level of confidence to our estimate. By default most researchers use a 95% confidence interval. We then multiply our point standard error by the corresponding t-statistics, and add/remove it from the prediction to obtain the upper and lower bounds of the confidence interval area:\nWe first get the critical value from the t-distribution, using half of the 5% (thus 0.975) on both side:\n\nt_value &lt;- qt(0.975, df = model$df.residual)\n\nthen multiply by the standard error and add/remove it to/from prediction:\n\nlower18&lt;-predicted_value18-t_value*se18\nlower7&lt;-predicted_value7-t_value*se7\nupper18&lt;-predicted_value18+t_value*se18\nupper7&lt;-predicted_value7+t_value*se7\n\nmanualCI&lt;-data.frame(\n  Elevation=RainScotland[c(7,18),\"Elevation\"],\n  UpperCI=c(upper7,upper18),\n  LowerCI=c(lower7,lower18)\n  )\nmanualCI\n#&gt;   Elevation  UpperCI  LowerCI\n#&gt; 1       520 2354.530 1908.562\n#&gt; 2       270 1658.579 1415.837\n\nWe can verify that these values are the same as the interval made up by the predict function with the confidence option (see above)\n\npred[c(7,18),]\n#&gt;         fit      lwr      upr\n#&gt; 7  2131.546 1908.562 2354.530\n#&gt; 18 1537.208 1415.837 1658.579\n\nas well as the one computed by ggplot with se=TRUE option in geom_smooth\n\np+geom_segment(data=manualCI, aes(x=Elevation,xend=Elevation,y=LowerCI, yend =UpperCI), color = \"blue\", size = 1)\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Confidence interval of a regression</span>"
    ]
  },
  {
    "objectID": "073_CIregression.html#prediction-interval-versus-confidence-interval",
    "href": "073_CIregression.html#prediction-interval-versus-confidence-interval",
    "title": "28  Confidence interval of a regression",
    "section": "28.4 Prediction interval versus confidence interval",
    "text": "28.4 Prediction interval versus confidence interval\nFinally, note that there is another interval to be used for prediction when we get new data, i.e. the prediction interval. You can get this interval from the predict function with the prediction option instead of confidence.\nThe standard error one uses here has the same ingredients but a slightly different definition:\n\\[\\text{SE}(\\hat{y}) = RSE\\sqrt{1+ \\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\\\\\n   \\text{SE}(\\hat{y}) = RSE\\sqrt{1+L}\\]\nWhile the confidence interval we have computed previously tells you how confident you can be that the fitted model represents well the data that was used for estimating the model, the prediction interval is for use when you want to infer a value for a new data based on the fitted model.\nSuppose you need to infer the rainfall for a 400m elevation. You can input the elevation (and all the other predictor variables in case of a multiple regression) in the predict function to find the range within which you can be 95% confident that the new observation is included, and thus for which the model is a reasonable tool to predict the rainfall.\n\nnewdata = data.frame(Elevation = 400)\npred_p &lt;- predict(model, newdata, interval = \"prediction\")\npred_p\n#&gt;        fit      lwr      upr\n#&gt; 1 1846.264 1317.536 2374.991\n\nIn this case you can then say that there is 95% chances that the precipitation will be between 1318 and 2375 mm.\nYou can be tempted to have a more precise prediction, but you then loose in terms of certainty.\n\npred_p90 &lt;- predict(model, newdata = data.frame(Elevation = 400), interval = \"prediction\", level=0.90)\npred_p90\n#&gt;        fit      lwr      upr\n#&gt; 1 1846.264 1409.861 2282.666\n\nFrom 1410 to 2283 is indeed a narrower guess but it is also less certain.\nWe can display the prediction area on the previous plot after taking the values from the predict function (not directly from geom_smooth which logically is about the data used for estimating the curve).\n\nnewobs&lt;-data.frame(Elevation = seq(100,600, by=50))\npredictions&lt;-predict(model, newdata = newobs , interval = \"prediction\")\np+geom_ribbon(data=cbind(newobs,predictions),aes(x=Elevation, y=NULL, ymin = lwr, ymax = upr), fill = \"lightblue\", alpha = 0.2)\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nBy construction, confidence intervals are narrower than prediction intervals. Prediction intervals have an extra RSE in the calculation. The intuition is that we don’t know the mean nor the variance of the population from which new observations are taken and the only thing we can do is to estimate the mean from the mean of our estimates, which itself will vary based on the variance considered. We can only use the RSE again to estimate this variance for the mean and the variance itself. Hence the RSE enters twice the equation.\nAn important implication of the presence of that extra \\(1\\) added to \\(L\\) under the square root is that while \\(L\\) decreases when sample size increases, one RSE remains and the decrease in uncertainty for the prediction interval is limited. In other words (again Greene, p99):\n\n“No matter how much data we have, we can never predict perfectly”",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Confidence interval of a regression</span>"
    ]
  },
  {
    "objectID": "074_multi_regression.html",
    "href": "074_multi_regression.html",
    "title": "29  Multiple Regression",
    "section": "",
    "text": "29.1 Example with two continuous regressors\nLet’s remind our first model with a single independent variable:\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nOLS1&lt;-lm(Rainfall~Elevation,data=RainScotland)\nsummary(OLS1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -486.08 -175.04   91.28  130.15  402.42 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 895.3223   149.7609   5.978 1.18e-05 ***\n#&gt; Elevation     2.3774     0.4438   5.357 4.32e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 242.8 on 18 degrees of freedom\n#&gt; Multiple R-squared:  0.6145, Adjusted R-squared:  0.5931 \n#&gt; F-statistic:  28.7 on 1 and 18 DF,  p-value: 4.317e-05\nWe think that we can improve the model if we include the distance (eastward) from the Western coastline and augment our model:\nOLS2&lt;-lm(Rainfall~Elevation+DistanceE,data=RainScotland)\nsummary(OLS2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation + DistanceE, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -268.04 -129.63   21.96  120.61  223.58 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 1536.6951   157.6228   9.749 2.24e-08 ***\n#&gt; Elevation      1.8238     0.3053   5.975 1.51e-05 ***\n#&gt; DistanceE     -5.2209     1.0152  -5.143 8.14e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 156.3 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.8492, Adjusted R-squared:  0.8314 \n#&gt; F-statistic: 47.86 on 2 and 17 DF,  p-value: 1.04e-07\nCompare the two models. What do you see?\nFirst, the model is of higher quality and predictions will be better: - More of the total variance is explained: the R-squared increased from 0.61 to 0.83 (adjusted). - the residual standard error drops from 243 to 156 mm\nSecond, the distance to the coast is significant and negative. It is expressed in km, hence we interpret it as: “when you get farther from the Western coast, every km inland will reduce rainfall by 5.22 mm”.\nThird, we could expect an interaction between elevation and distance from the sea. We observe that introducing the distance indeed changes our estimate of the elevation effect: from 2.38 mm of water for every m higher, we now estimate the effect of elevation to be a 1.82 mm per m. The effect remain well significant although it is reduced. We usually say that this new estimate is “cleared from the effect of distance” or “this is the effect of elevation after controlling for the effect of distance from the sea”.\nDANGER ZONE - DON’T COMPARE COEFFICIENTS DIRECTLY. You cannot say that the effect of distance is 4 times larger than the effect of elevation! - BEAR THE UNITS IN MIND! \\(\\beta\\) coefficients have units. For example, the coefficient associated to distance is mm/km in this case. If you had measured distance in m, the coefficient would be -0.00522, which looks lower than the effect of elevation and much closer to zero, but it is not!",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "074_multi_regression.html#example-with-one-continuous-and-one-categorical-regressor",
    "href": "074_multi_regression.html#example-with-one-continuous-and-one-categorical-regressor",
    "title": "29  Multiple Regression",
    "section": "29.2 Example with one continuous and one categorical regressor",
    "text": "29.2 Example with one continuous and one categorical regressor\nSuppose you were not able to measure the distance between the meteorological station and the sea, but you had a variable indicating the station is in the Western part or the Eastern part of the country\nSay the West variable is 1 when the collection site is on the Western side of Scotland and 0 on the Eastern side. It is important to represent those categorical variables as factors so their coding and effect are not seen as a continuous one. You can also use your prefereed reference category (here East then)\n\nRainScotland$West&lt;-as.factor(as.numeric(RainScotland$DistanceE&lt;90))\n\nAnd we estimate a new model:\n\nOLS3&lt;-lm(Rainfall~Elevation+West,data=RainScotland)\nsummary(OLS3)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation + West, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -314.920 -135.718   -1.407  103.452  309.258 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 849.1774   120.4115   7.052 1.94e-06 ***\n#&gt; Elevation     1.9867     0.3732   5.324 5.60e-05 ***\n#&gt; West1       307.2854    91.7632   3.349  0.00381 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 193.9 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.7677, Adjusted R-squared:  0.7404 \n#&gt; F-statistic:  28.1 on 2 and 17 DF,  p-value: 4.082e-06\n\nThis model 3 is less effective than model 2 in general (R-squared is lower) but better than model 1. Also with this “specification”, the elevation effect is now around 2mm.\nHow do you interpret the West variable coefficient? It is not continuous so one can’t phrase it as “for every increase in…”, rather we will say that\n“Being in the Western part of Scotland, one can expect to add 307mm of water relative to the level of rain in the Eastern part”\nNote that now the intercept (now 849mm) refers to the level in the Eastern part. If there were several categorical variables, it would relate to the combined references across all variables.\nGraphically you also see that introducing a categorical variable this way leads to a fixed amount of water (307mm) to be added everywhere to the relationship between elevation and rainfall (In some fields this is actually called a “fixed effect” model):\nSee in red the Western point and the corresponding fit, parallel to the corresponding fit for the Eastern stations in black.\nThe dashed blue line is the estimation from our first model (OLS1) for comparison.\n\nplot(x=RainScotland$Elevation, y=RainScotland$Rainfall,\n     col=RainScotland$West,\n    xlab=\"X : Elevation (m)\", ylab=\"Y : Rainfall (mm)\")\nabline(a=OLS3$coefficients[1],b=OLS3$coefficients[2], col=\"black\")\nabline(a=OLS3$coefficients[1]+OLS3$coefficients[3],b=OLS3$coefficients[2], col=\"red\")\nabline(a=OLS1$coefficients[1],b=OLS1$coefficients[2], col=\"blue\", lty=2)",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "074_multi_regression.html#exercise",
    "href": "074_multi_regression.html#exercise",
    "title": "29  Multiple Regression",
    "section": "29.3 Exercise",
    "text": "29.3 Exercise\nUsing the dataset Lux116.csv within the data folder. - Test the hypothesis that employment per municipality is related to ‘urbanness’ (using density), distance the main jobs (using road distance) and percentage of foreigners. - Test whether there could be a specific effect (downward or upward) of any the 3 districts?",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "075_regression_diagnostics.html",
    "href": "075_regression_diagnostics.html",
    "title": "30  Regression diagnostics",
    "section": "",
    "text": "30.1 Stressing the visual: Anscombe\nConsider the following 4 datasets (constructed in 1973 by the statistician Francis Anscombe).\nanscombe\n#&gt;    x1 x2 x3 x4    y1   y2    y3    y4\n#&gt; 1  10 10 10  8  8.04 9.14  7.46  6.58\n#&gt; 2   8  8  8  8  6.95 8.14  6.77  5.76\n#&gt; 3  13 13 13  8  7.58 8.74 12.74  7.71\n#&gt; 4   9  9  9  8  8.81 8.77  7.11  8.84\n#&gt; 5  11 11 11  8  8.33 9.26  7.81  8.47\n#&gt; 6  14 14 14  8  9.96 8.10  8.84  7.04\n#&gt; 7   6  6  6  8  7.24 6.13  6.08  5.25\n#&gt; 8   4  4  4 19  4.26 3.10  5.39 12.50\n#&gt; 9  12 12 12  8 10.84 9.13  8.15  5.56\n#&gt; 10  7  7  7  8  4.82 7.26  6.42  7.91\n#&gt; 11  5  5  5  8  5.68 4.74  5.73  6.89\nIn each case the mean and variance of \\(Xs\\) an \\(Ys\\) are almost exactly the same:\nsapply(anscombe, mean)\n#&gt;       x1       x2       x3       x4       y1       y2       y3       y4 \n#&gt; 9.000000 9.000000 9.000000 9.000000 7.500909 7.500909 7.500000 7.500909\nsapply(anscombe, var)\n#&gt;        x1        x2        x3        x4        y1        y2        y3        y4 \n#&gt; 11.000000 11.000000 11.000000 11.000000  4.127269  4.127629  4.122620  4.123249\nWe estimate 4 OLS models:\n# code from R help\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\nfor (i in 1:4) {\n    ff[2:3] &lt;- lapply(paste0(c(\"y\", \"x\"), i), as.name)\n    mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n    print(summary(lmi))\n}\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ff, data = anscombe)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)   3.0001     1.1247   2.667  0.02573 * \n#&gt; x1            0.5001     0.1179   4.241  0.00217 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.237 on 9 degrees of freedom\n#&gt; Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n#&gt; F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ff, data = anscombe)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.9009 -0.7609  0.1291  0.9491  1.2691 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)    3.001      1.125   2.667  0.02576 * \n#&gt; x2             0.500      0.118   4.239  0.00218 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.237 on 9 degrees of freedom\n#&gt; Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n#&gt; F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ff, data = anscombe)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)   3.0025     1.1245   2.670  0.02562 * \n#&gt; x3            0.4997     0.1179   4.239  0.00218 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.236 on 9 degrees of freedom\n#&gt; Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n#&gt; F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ff, data = anscombe)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -1.751 -0.831  0.000  0.809  1.839 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)   3.0017     1.1239   2.671  0.02559 * \n#&gt; x4            0.4999     0.1178   4.243  0.00216 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.236 on 9 degrees of freedom\n#&gt; Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n#&gt; F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\nThe quality of the fit and the regression coefficients are very similar across the 4 cases. But looking at it more closely, you see why a visual inspection is always needed:\n# code from R help\nop &lt;- par(mfrow = c(2, 2), mar = 0.1 + c(4, 4, 1, 1), oma = c(0, 0, 2, 0))\nfor (i in 1:4) {\n    ff[2:3] &lt;- lapply(paste0(c(\"y\", \"x\"), i), as.name)\n    plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2, \n        xlim = c(3, 19), ylim = c(3, 13))\n    abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression diagnostics</span>"
    ]
  },
  {
    "objectID": "075_regression_diagnostics.html#visual-diagnostics",
    "href": "075_regression_diagnostics.html#visual-diagnostics",
    "title": "30  Regression diagnostics",
    "section": "30.2 Visual diagnostics:",
    "text": "30.2 Visual diagnostics:\n\n30.2.1 Example\n\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nOLS2&lt;-lm(Rainfall~Elevation+DistanceE,data=RainScotland)\nsummary(OLS2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Rainfall ~ Elevation + DistanceE, data = RainScotland)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -268.04 -129.63   21.96  120.61  223.58 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 1536.6951   157.6228   9.749 2.24e-08 ***\n#&gt; Elevation      1.8238     0.3053   5.975 1.51e-05 ***\n#&gt; DistanceE     -5.2209     1.0152  -5.143 8.14e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 156.3 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.8492, Adjusted R-squared:  0.8314 \n#&gt; F-statistic: 47.86 on 2 and 17 DF,  p-value: 1.04e-07\n\n\n\n30.2.2 Plots\nPlot() applied to a lm object results in a sequence of 4 plots that are most useful for a visual inspection of the relationship you uncovered, as well as assessing whether the regression is valid and if any data behave as an outlier.\n\npar(mfrow=c(2,2))\nplot(OLS2)\n\n\n\n\n\n\n\n\n\nPlot 1 shows the distribution of residuals against the predicted \\(Y\\) values. For a linear relationship, one expects that the residuals are equally distributed along the fitted values, i.e. spread similarly around the dashed horizontal line. Typically if the trend red curve would be clearly upward or downward sloping, or show (inverted-) U or V -shaped, that would mean the linearity assumption is not good. Some transformation or additional explanatory variables might be needed.\nPlot 2 is a qqplot against the theoretical normal distribution (qqnorm). While we have seen that variations are more likely at the extremes, one expect the general shape of the curve to follow 1 to 1 straight line.\nPlot 3 is often similar to plot 1, but more specifically aimed at finding if residuals are homoscedastic, i.e. have equal variance. The residuals are standardized (studentized) and the smoothed red line indicates changes in the variance. Again we expect it to be horizontal and the points to be equally distributed within a horizontal rectangle buffer.\nPlot 4 shows standardized residuals (same as Plot 2) now plotted against leverage, indicating whether some points may influence the fit more than others. Points spotted far on the right side of the plot may deserve some attention, as their absence may significantly affect the coefficients of the model. In addition, isolines (0.5, 1, …) of Cook’s distance statistic is provided. Again values beyond those lines must be investigated as potentially too influential.\n\n\n\n30.2.3 Plots for Anscombe data",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression diagnostics</span>"
    ]
  },
  {
    "objectID": "075_regression_diagnostics.html#normality-and-homoscedasticity-tests",
    "href": "075_regression_diagnostics.html#normality-and-homoscedasticity-tests",
    "title": "30  Regression diagnostics",
    "section": "30.3 Normality and homoscedasticity tests",
    "text": "30.3 Normality and homoscedasticity tests",
    "crumbs": [
      "Part VII - Bivariate and regression analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression diagnostics</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html",
    "href": "901_copy_paste.html",
    "title": "31  Copy-pasting",
    "section": "",
    "text": "31.1 clipr way\nA simple way to access to input some small data using the clipboard and that should work across all platforms is to use the read_clip() function from the clipr package.\nSuppose you have a series of numbers copied from a series like this one: 11 12 13 14 15 16 or this one: 11, 12, 13, 14, 15, 16\nget to the console and type clipr::read_clip()\nIn this case you will notice the whole set is a single character string entry, which then necessitates a split. See\na&lt;-clipr::read_clip()\na\n#&gt; [1] \"11, 12, 13, 14, 15, 16\"\nstrsplit(a,\", \")\n#&gt;[[1]]\n#&gt;[1] \"11\" \"12\" \"13\" \"14\" \"15\" \"16\"\nHowever, when it comes from a spreadsheet (e.g. open office)\nyou’ll get separate character entries directly:\nclipr::read_clip()\n#&gt; [1] \"0.7226332924\" \"0.5949296139\" \"0.0513909524\"\n#&gt; [4] \"0.2215940265\" \"0.8725634748\" \"0.0032392712\"\n#&gt; [7] \"0.774327883\"  \"0.1773198219\" \"0.1791877889\"\n#&gt; [10] \"0.004243708\"",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html#read.table-way",
    "href": "901_copy_paste.html#read.table-way",
    "title": "31  Copy-pasting",
    "section": "31.2 read.table way",
    "text": "31.2 read.table way\nYou can also use the read/write table approach after saying the clipboard in the source or output. The inconvenience is that the MacOSX and MS Window approach have a slightly different code:\nCopy from spreadsheet, then paste in R using\n\nb &lt;- read.table(pipe(\"pbpaste\"),header = TRUE) #on macOSX \nb &lt;- read.table(\"clipboard\",header = TRUE) #on MS Windows\n\nand similarly to write to a spreadsheet:\n\nb3&lt;-b^3\nwrite.table(b3, pipe(\"pbcopy\"),row.names = FALSE,sep = \"\\t\") #MACOSX\nwrite.table(A3, \"clipboard\",row.names = FALSE,sep = \"\\t\") #MS Windows\n\nThen paste in spreadsheet.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "902_making_data.html",
    "href": "902_making_data.html",
    "title": "32  Making data",
    "section": "",
    "text": "32.1 Scraping a wikipedia table\nExample of the Tour de France winners table from wikipedia, used in the Vectors chapter of the course: Section 6.3.\n#R script to scrape the tour de France winners table from wikipedia page:\n#https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\n#Geoffrey Caruso  Sept 18th 2024\n#\n# Script is adapted from\n# https://help.displayr.com/hc/en-us/articles/360003582875-How-to-Import-a-Wikipedia-Table-using-R\n# \n# Since there are different tables in Tour de France page and the one of interest is a sortable one,\n# I have adapted following suggestions in\n# \n# https://stackoverflow.com/questions/72380279/how-to-scrape-with-table-class-name-with-r\n# \n# I still had to look into the source of the table to find out what table class this is\n# In this case it was a \"wikitable plainrowheaders sortable'\n\nLeTour_url&lt;-\"https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\"\nLeTour_Page&lt;-rvest::read_html(LeTour_url)\nLeTour_Table&lt;-rvest::html_node(LeTour_Page, xpath=\"//table[@class='wikitable plainrowheaders sortable']\")\nLeTour_Tibble = rvest::html_table(LeTour_Table, fill = TRUE)\nLeTour_df&lt;-as.data.frame(LeTour_Tibble)\n\nsaveRDS(LeTour_df,\"data/TourDeFrance/LeTour_df.rds\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "href": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "title": "32  Making data",
    "section": "32.2 Scotland rain and elevation (by Ferguson)",
    "text": "32.2 Scotland rain and elevation (by Ferguson)\n\n#Ferguson Rob,\n#linear regression in geography, CATMOG 15. https://github.com/qmrg/CATMOG/blob/Main/15-linear-regression-in-geography.pdf\n\n#Data from Table 1 and 2, p8:\n#Average precipitation and elevation across southern Scotland\n#\n#Source caption: British Rainfall (HMSO),\n#selected raingauges between national grid lines 600 and 601 km N.\n#Sites are in West-East order\n#Elevation in m above OD\n#Rainfall in mm/yr\n#DistanceE in km from W coast\n#\nRainScotland&lt;-data.frame(\n  SiteNo=1:20,\n  Elevation=c(240,430,420,470,300,150,520,460,300,410,\n              140,540,280,240,200,210,160,270,320,230),\n  Rainfall=c(1720,2320,2050,1870,1690,1250,2130,2090, 1730,2040,\n             1460,1860,1670,1580,1490,1420,900,1250,1170,1170),\n  DistanceE=c(37,43,48,49,52,59,73,75,76,77,\n              86,97,100,103,104,114,138,152,153,154)\n    )\nwrite.csv(RainScotland, \"data/Ferguson/RainScotland.csv\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "903_summarizing.html",
    "href": "903_summarizing.html",
    "title": "33  Summary tables",
    "section": "",
    "text": "33.1 Descriptive statistics with summarytools\nTake the RainScotland dataset as a first example and use the dfSummary() function that provides a rather comprehensive set of statistics for entire dataframes:\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nsummarytools::dfSummary(RainScotland)\n#&gt; Data Frame Summary  \n#&gt; RainScotland  \n#&gt; Dimensions: 20 x 5  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------\n#&gt; No   Variable    Stats / Values              Freqs (% of Valid)   Graph               Valid      Missing  \n#&gt; ---- ----------- --------------------------- -------------------- ------------------- ---------- ---------\n#&gt; 1    X           Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 2    SiteNo      Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 3    Elevation   Mean (sd) : 314.5 (125.5)   18 distinct values       : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                     : :     :       (100.0%)   (0.0%)   \n#&gt;                  140 &lt; 290 &lt; 540                                  . . : :     : . .                       \n#&gt;                  IQR (CV) : 197.5 (0.4)                           : : : :     : : :                       \n#&gt;                                                                   : : : : :   : : :                       \n#&gt; \n#&gt; 4    Rainfall    Mean (sd) : 1643 (380.6)    18 distinct values         : :   :       20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                       : :   :       (100.0%)   (0.0%)   \n#&gt;                  900 &lt; 1680 &lt; 2320                                  . . : : . :                           \n#&gt;                  IQR (CV) : 535 (0.2)                               : : : : : :                           \n#&gt;                                                                   : : : : : : : :                         \n#&gt; \n#&gt; 5    DistanceE   Mean (sd) : 89.5 (37.7)     20 distinct values     :                 20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                   : :               (100.0%)   (0.0%)   \n#&gt;                  37 &lt; 81.5 &lt; 154                                    : : : :   :                           \n#&gt;                  IQR (CV) : 49.2 (0.4)                              : : : :   :                           \n#&gt;                                                                   : : : : : : :                           \n#&gt; ----------------------------------------------------------------------------------------------------------\nAn nice feature is that it can generate its own view (beware: view not View(), as an html file for display in any browser:\nsummarytools::view(summarytools::dfSummary(RainScotland), file = \"output/RainScotland.html\")\nIf we would a more complex dataset (including factors) such as the wikipedia table of the Tour de France winners, we would have:\nTDF&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nsummarytools::dfSummary(TDF)\n#&gt; Data Frame Summary  \n#&gt; TDF  \n#&gt; Dimensions: 122 x 8  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------------\n#&gt; No   Variable       Stats / Values                 Freqs (% of Valid)    Graph              Valid      Missing  \n#&gt; ---- -------------- ------------------------------ --------------------- ------------------ ---------- ---------\n#&gt; 1    Year           Mean (sd) : 1963.5 (35.4)      122 distinct values   . : : : : :        122        0        \n#&gt;      [integer]      min &lt; med &lt; max:               (Integer sequence)    : : : : : :        (100.0%)   (0.0%)   \n#&gt;                     1903 &lt; 1963.5 &lt; 2024                                 : : : : : :                            \n#&gt;                     IQR (CV) : 60.5 (0)                                  : : : : : :                            \n#&gt;                                                                          : : : : : : :                          \n#&gt; \n#&gt; 2    Country        1. France                      36 (29.5%)            IIIII              122        0        \n#&gt;      [character]    2. —                           18 (14.8%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Belgium                     18 (14.8%)            II                                     \n#&gt;                     4. Spain                       12 ( 9.8%)            I                                      \n#&gt;                     5. Italy                       10 ( 8.2%)            I                                      \n#&gt;                     6. Great Britain                6 ( 4.9%)                                                   \n#&gt;                     7. Luxembourg                   5 ( 4.1%)                                                   \n#&gt;                     8. Denmark                      3 ( 2.5%)                                                   \n#&gt;                     9. Slovenia                     3 ( 2.5%)                                                   \n#&gt;                     10. United States               3 ( 2.5%)                                                   \n#&gt;                     [ 6 others ]                    8 ( 6.6%)            I                                      \n#&gt; \n#&gt; 3    Cyclist        1. ~Not contested due to Wor    7 ( 5.7%)            I                  122        0        \n#&gt;      [character]    2. No winner[c]                 7 ( 5.7%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. Jacques Anquetil             5 ( 4.1%)                                                   \n#&gt;                     4. Miguel Indurain              5 ( 4.1%)                                                   \n#&gt;                     5. ~Not contested due to Wor    4 ( 3.3%)                                                   \n#&gt;                     6. Bernard Hinault              4 ( 3.3%)                                                   \n#&gt;                     7. Chris Froome                 3 ( 2.5%)                                                   \n#&gt;                     8. Greg LeMond                  3 ( 2.5%)                                                   \n#&gt;                     9. Louison Bobet                3 ( 2.5%)                                                   \n#&gt;                     10. Philippe Thys               3 ( 2.5%)                                                   \n#&gt;                     [ 67 others ]                  78 (63.9%)            IIIIIIIIIIII                           \n#&gt; \n#&gt; 4    Sponsor/Team   1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. France                      13 (10.7%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Alcyon–Dunlop                7 ( 5.7%)            I                                      \n#&gt;                     4. Peugeot–Wolber               7 ( 5.7%)            I                                      \n#&gt;                     5. Team Sky                     6 ( 4.9%)                                                   \n#&gt;                     6. Banesto                      5 ( 4.1%)                                                   \n#&gt;                     7. Italy                        5 ( 4.1%)                                                   \n#&gt;                     8. Automoto–Hutchinson          3 ( 2.5%)                                                   \n#&gt;                     9. Belgium                      3 ( 2.5%)                                                   \n#&gt;                     10. La Sportive                 3 ( 2.5%)                                                   \n#&gt;                     [ 40 others ]                  52 (42.6%)            IIIIIIII                               \n#&gt; \n#&gt; 5    Distance       1. —                           11 ( 9.0%)            I                  122        0        \n#&gt;      [character]    2. 2,428 km (1,509 mi)          2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 3,765 km (2,339 mi)          2 ( 1.6%)                                                   \n#&gt;                     4. 4,498 km (2,795 mi)          2 ( 1.6%)                                                   \n#&gt;                     5. 2,994 km (1,860 mi)          1 ( 0.8%)                                                   \n#&gt;                     6. 3,278 km (2,037 mi)          1 ( 0.8%)                                                   \n#&gt;                     7. 3,285 km (2,041 mi)          1 ( 0.8%)                                                   \n#&gt;                     8. 3,286 km (2,042 mi)          1 ( 0.8%)                                                   \n#&gt;                     9. 3,328 km (2,068 mi)          1 ( 0.8%)                                                   \n#&gt;                     10. 3,349 km (2,081 mi)         1 ( 0.8%)                                                   \n#&gt;                     [ 99 others ]                  99 (81.1%)            IIIIIIIIIIIIIIII                       \n#&gt; \n#&gt; 6    Time/Points    1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 100h 30′ 35″                 1 ( 0.8%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 100h 49′ 30″                 1 ( 0.8%)                                                   \n#&gt;                     4. 101h 01′ 20″                 1 ( 0.8%)                                                   \n#&gt;                     5. 103h 06′ 50″                 1 ( 0.8%)                                                   \n#&gt;                     6. 103h 38′ 38″                 1 ( 0.8%)                                                   \n#&gt;                     7. 105h 07′ 52″                 1 ( 0.8%)                                                   \n#&gt;                     8. 108h 17′ 18″                 1 ( 0.8%)                                                   \n#&gt;                     9. 108h 18′ 00″                 1 ( 0.8%)                                                   \n#&gt;                     10. 109h 19′ 14″                1 ( 0.8%)                                                   \n#&gt;                     [ 95 others ]                  95 (77.9%)            IIIIIIIIIIIIIII                        \n#&gt; \n#&gt; 7    Margin         1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. + 3′ 10″                     2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. + 3′ 21″                     2 ( 1.6%)                                                   \n#&gt;                     4. + 4′ 01″                     2 ( 1.6%)                                                   \n#&gt;                     5. + 4′ 35″                     2 ( 1.6%)                                                   \n#&gt;                     6. + 4′ 59″                     2 ( 1.6%)                                                   \n#&gt;                     7. + 1′ 07″                     1 ( 0.8%)                                                   \n#&gt;                     8. + 1′ 11″                     1 ( 0.8%)                                                   \n#&gt;                     9. + 1′ 12″                     1 ( 0.8%)                                                   \n#&gt;                     10. + 1′ 22″                    1 ( 0.8%)                                                   \n#&gt;                     [ 90 others ]                  90 (73.8%)            IIIIIIIIIIIIII                         \n#&gt; \n#&gt; 8    Stage wins     1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 0                            8 ( 6.6%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. 1                           20 (16.4%)            III                                    \n#&gt;                     4. 2                           27 (22.1%)            IIII                                   \n#&gt;                     5. 3                           19 (15.6%)            III                                    \n#&gt;                     6. 4                           12 ( 9.8%)            I                                      \n#&gt;                     7. 5                            8 ( 6.6%)            I                                      \n#&gt;                     8. 6                            6 ( 4.9%)                                                   \n#&gt;                     9. 7                            2 ( 1.6%)                                                   \n#&gt;                     10. 8                           2 ( 1.6%)                                                   \n#&gt; ----------------------------------------------------------------------------------------------------------------\nsummarytools::view(summarytools::dfSummary(TDF), file = \"output/LeTour.html\")\nA less visual but even more complete set (including range and indicators of the shape of the distributions) can be obtained with the descr()function.\nsummarytools::descr(RainScotland)\n#&gt; Descriptive Statistics  \n#&gt; RainScotland  \n#&gt; N: 20  \n#&gt; \n#&gt;                     DistanceE   Elevation   Rainfall   SiteNo        X\n#&gt; ----------------- ----------- ----------- ---------- -------- --------\n#&gt;              Mean       89.50      314.50    1643.00    10.50    10.50\n#&gt;           Std.Dev       37.74      125.51     380.62     5.92     5.92\n#&gt;               Min       37.00      140.00     900.00     1.00     1.00\n#&gt;                Q1       55.50      220.00    1335.00     5.50     5.50\n#&gt;            Median       81.50      290.00    1680.00    10.50    10.50\n#&gt;                Q3      109.00      425.00    1955.00    15.50    15.50\n#&gt;               Max      154.00      540.00    2320.00    20.00    20.00\n#&gt;               MAD       38.55      155.67     459.61     7.41     7.41\n#&gt;               IQR       49.25      197.50     535.00     9.50     9.50\n#&gt;                CV        0.42        0.40       0.23     0.56     0.56\n#&gt;          Skewness        0.40        0.33      -0.09     0.00     0.00\n#&gt;       SE.Skewness        0.51        0.51       0.51     0.51     0.51\n#&gt;          Kurtosis       -1.13       -1.30      -1.05    -1.38    -1.38\n#&gt;           N.Valid       20.00       20.00      20.00    20.00    20.00\n#&gt;         Pct.Valid      100.00      100.00     100.00   100.00   100.00\nIn the case we apply it to the Tour de France, only the year is available as a numeric, which is not quite interesting because there is only one Tour per year, thus providing basically no useful information.\nsummarytools::descr(TDF)\n#&gt; Non-numerical variable(s) ignored: Country, Cyclist, Sponsor/Team, Distance, Time/Points, Margin, Stage wins\n#&gt; Descriptive Statistics  \n#&gt; TDF$Year  \n#&gt; N: 122  \n#&gt; \n#&gt;                        Year\n#&gt; ----------------- ---------\n#&gt;              Mean   1963.50\n#&gt;           Std.Dev     35.36\n#&gt;               Min   1903.00\n#&gt;                Q1   1933.00\n#&gt;            Median   1963.50\n#&gt;                Q3   1994.00\n#&gt;               Max   2024.00\n#&gt;               MAD     45.22\n#&gt;               IQR     60.50\n#&gt;                CV      0.02\n#&gt;          Skewness      0.00\n#&gt;       SE.Skewness      0.22\n#&gt;          Kurtosis     -1.23\n#&gt;           N.Valid    122.00\n#&gt;         Pct.Valid    100.00\nThe specific function freq() is rather to be used here. Given most variables are categorical, we are interested in counts and percentages:\nsummarytools::freq(TDF)\n#&gt; Variable(s) ignored: Year\n#&gt; Frequencies  \n#&gt; TDF$Country  \n#&gt; Type: Character  \n#&gt; \n#&gt;                       Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------- ------ --------- -------------- --------- --------------\n#&gt;                   —     18     14.75          14.75     14.75          14.75\n#&gt;           Australia      1      0.82          15.57      0.82          15.57\n#&gt;             Belgium     18     14.75          30.33     14.75          30.33\n#&gt;            Colombia      1      0.82          31.15      0.82          31.15\n#&gt;             Denmark      3      2.46          33.61      2.46          33.61\n#&gt;              France     36     29.51          63.11     29.51          63.11\n#&gt;             Germany      1      0.82          63.93      0.82          63.93\n#&gt;       Great Britain      6      4.92          68.85      4.92          68.85\n#&gt;             Ireland      1      0.82          69.67      0.82          69.67\n#&gt;               Italy     10      8.20          77.87      8.20          77.87\n#&gt;          Luxembourg      5      4.10          81.97      4.10          81.97\n#&gt;         Netherlands      2      1.64          83.61      1.64          83.61\n#&gt;            Slovenia      3      2.46          86.07      2.46          86.07\n#&gt;               Spain     12      9.84          95.90      9.84          95.90\n#&gt;         Switzerland      2      1.64          97.54      1.64          97.54\n#&gt;       United States      3      2.46         100.00      2.46         100.00\n#&gt;                &lt;NA&gt;      0                               0.00         100.00\n#&gt;               Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Cyclist  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                            Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ---------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;        ~Not contested due to World War I      4      3.28           3.28      3.28           3.28\n#&gt;       ~Not contested due to World War II      7      5.74           9.02      5.74           9.02\n#&gt;                         Alberto Contador      1      0.82           9.84      0.82           9.84\n#&gt;                        Alberto Contador#      1      0.82          10.66      0.82          10.66\n#&gt;                             André Leducq      2      1.64          12.30      1.64          12.30\n#&gt;                         Andy Schleck#[e]      1      0.82          13.11      0.82          13.11\n#&gt;                            Antonin Magne      2      1.64          14.75      1.64          14.75\n#&gt;                          Bernard Hinault      4      3.28          18.03      3.28          18.03\n#&gt;                         Bernard Hinault†      1      0.82          18.85      0.82          18.85\n#&gt;                         Bernard Thévenet      2      1.64          20.49      1.64          20.49\n#&gt;                           Bjarne Riis[b]      1      0.82          21.31      0.82          21.31\n#&gt;                          Bradley Wiggins      1      0.82          22.13      0.82          22.13\n#&gt;                              Cadel Evans      1      0.82          22.95      0.82          22.95\n#&gt;                            Carlos Sastre      1      0.82          23.77      0.82          23.77\n#&gt;                              Charly Gaul      1      0.82          24.59      0.82          24.59\n#&gt;                             Chris Froome      3      2.46          27.05      2.46          27.05\n#&gt;                             Chris Froome      1      0.82          27.87      0.82          27.87\n#&gt;                              Eddy Merckx      1      0.82          28.69      0.82          28.69\n#&gt;                              Eddy Merckx      1      0.82          29.51      0.82          29.51\n#&gt;                             Eddy Merckx†      2      1.64          31.15      1.64          31.15\n#&gt;                             Eddy Merckx‡      1      0.82          31.97      0.82          31.97\n#&gt;                             Egan Bernal#      1      0.82          32.79      0.82          32.79\n#&gt;                             Fausto Coppi      2      1.64          34.43      1.64          34.43\n#&gt;                      Federico Bahamontes      1      0.82          35.25      0.82          35.25\n#&gt;                           Felice Gimondi      1      0.82          36.07      0.82          36.07\n#&gt;                         Ferdinand Kübler      1      0.82          36.89      0.82          36.89\n#&gt;                            Firmin Lambot      2      1.64          38.52      1.64          38.52\n#&gt;                           François Faber      1      0.82          39.34      0.82          39.34\n#&gt;                          Gastone Nencini      1      0.82          40.16      0.82          40.16\n#&gt;                         Georges Speicher      1      0.82          40.98      0.82          40.98\n#&gt;                           Geraint Thomas      1      0.82          41.80      0.82          41.80\n#&gt;                             Gino Bartali      2      1.64          43.44      1.64          43.44\n#&gt;                              Greg LeMond      3      2.46          45.90      2.46          45.90\n#&gt;                         Gustave Garrigou      1      0.82          46.72      0.82          46.72\n#&gt;                          Henri Cornet[a]      1      0.82          47.54      0.82          47.54\n#&gt;                          Henri Pélissier      1      0.82          48.36      0.82          48.36\n#&gt;                              Hugo Koblet      1      0.82          49.18      0.82          49.18\n#&gt;                         Jacques Anquetil      5      4.10          53.28      4.10          53.28\n#&gt;                              Jan Janssen      1      0.82          54.10      0.82          54.10\n#&gt;                             Jan Ullrich#      1      0.82          54.92      0.82          54.92\n#&gt;                               Jean Robic      1      0.82          55.74      0.82          55.74\n#&gt;                         Jonas Vingegaard      1      0.82          56.56      0.82          56.56\n#&gt;                         Jonas Vingegaard      1      0.82          57.38      0.82          57.38\n#&gt;                           Joop Zoetemelk      1      0.82          58.20      0.82          58.20\n#&gt;                           Laurent Fignon      1      0.82          59.02      0.82          59.02\n#&gt;                          Laurent Fignon#      1      0.82          59.84      0.82          59.84\n#&gt;                              Léon Scieur      1      0.82          60.66      0.82          60.66\n#&gt;                        Louis Trousselier      1      0.82          61.48      0.82          61.48\n#&gt;                            Louison Bobet      3      2.46          63.93      2.46          63.93\n#&gt;                             Lucien Aimar      1      0.82          64.75      0.82          64.75\n#&gt;                            Lucien Buysse      1      0.82          65.57      0.82          65.57\n#&gt;                      Lucien Petit-Breton      2      1.64          67.21      1.64          67.21\n#&gt;                          Lucien Van Impe      1      0.82          68.03      0.82          68.03\n#&gt;                               Luis Ocaña      1      0.82          68.85      0.82          68.85\n#&gt;                            Marco Pantani      1      0.82          69.67      0.82          69.67\n#&gt;                         Maurice De Waele      1      0.82          70.49      0.82          70.49\n#&gt;                            Maurice Garin      1      0.82          71.31      0.82          71.31\n#&gt;                          Miguel Indurain      5      4.10          75.41      4.10          75.41\n#&gt;                           Nicolas Frantz      2      1.64          77.05      1.64          77.05\n#&gt;                             No winner[c]      7      5.74          82.79      5.74          82.79\n#&gt;                            Octave Lapize      1      0.82          83.61      0.82          83.61\n#&gt;                            Odile Defraye      1      0.82          84.43      0.82          84.43\n#&gt;                         Óscar Pereiro[d]      1      0.82          85.25      0.82          85.25\n#&gt;                       Ottavio Bottecchia      2      1.64          86.89      1.64          86.89\n#&gt;                            Pedro Delgado      1      0.82          87.70      0.82          87.70\n#&gt;                            Philippe Thys      3      2.46          90.16      2.46          90.16\n#&gt;                             René Pottier      1      0.82          90.98      0.82          90.98\n#&gt;                            Roger Lapébie      1      0.82          91.80      0.82          91.80\n#&gt;                            Roger Pingeon      1      0.82          92.62      0.82          92.62\n#&gt;                          Roger Walkowiak      1      0.82          93.44      0.82          93.44\n#&gt;                              Romain Maes      1      0.82          94.26      0.82          94.26\n#&gt;                            Stephen Roche      1      0.82          95.08      0.82          95.08\n#&gt;                             Sylvère Maes      1      0.82          95.90      0.82          95.90\n#&gt;                             Sylvère Maes      1      0.82          96.72      0.82          96.72\n#&gt;                            Tadej Pogačar      1      0.82          97.54      0.82          97.54\n#&gt;                           Tadej Pogačar§      2      1.64          99.18      1.64          99.18\n#&gt;                          Vincenzo Nibali      1      0.82         100.00      0.82         100.00\n#&gt;                                     &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                    Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Sponsor/Team  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                           Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; --------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;                                       —     18     14.75          14.75     14.75          14.75\n#&gt;             AD Renting–W-Cup–Bottecchia      1      0.82          15.57      0.82          15.57\n#&gt;                           Alcyon–Dunlop      7      5.74          21.31      5.74          21.31\n#&gt;                                  Astana      2      1.64          22.95      1.64          22.95\n#&gt;                                Automoto      1      0.82          23.77      0.82          23.77\n#&gt;                     Automoto–Hutchinson      3      2.46          26.23      2.46          26.23\n#&gt;                                 Banesto      5      4.10          30.33      4.10          30.33\n#&gt;                                 Belgium      3      2.46          32.79      2.46          32.79\n#&gt;                                     Bic      1      0.82          33.61      0.82          33.61\n#&gt;                         BMC Racing Team      1      0.82          34.43      0.82          34.43\n#&gt;          Caisse d'Epargne–Illes Balears      1      0.82          35.25      0.82          35.25\n#&gt;                  Carrera Jeans–Vagabond      1      0.82          36.07      0.82          36.07\n#&gt;                                   Conte      1      0.82          36.89      0.82          36.89\n#&gt;                       Discovery Channel      1      0.82          37.70      0.82          37.70\n#&gt;                                   Faema      1      0.82          38.52      0.82          38.52\n#&gt;                           Faemino–Faema      1      0.82          39.34      0.82          39.34\n#&gt;                  Ford France–Hutchinson      1      0.82          40.16      0.82          40.16\n#&gt;                                  France     13     10.66          50.82     10.66          50.82\n#&gt;                       Gitane–Campagnolo      1      0.82          51.64      0.82          51.64\n#&gt;                                   Italy      5      4.10          55.74      4.10          55.74\n#&gt;                            La Française      1      0.82          56.56      0.82          56.56\n#&gt;                             La Sportive      3      2.46          59.02      2.46          59.02\n#&gt;                           La Vie Claire      2      1.64          60.66      1.64          60.66\n#&gt;                              Luxembourg      1      0.82          61.48      0.82          61.48\n#&gt;                   Mercatone Uno–Bianchi      1      0.82          62.30      0.82          62.30\n#&gt;                                 Molteni      3      2.46          64.75      2.46          64.75\n#&gt;                Pelforth–Sauvage–Lejeune      1      0.82          65.57      0.82          65.57\n#&gt;                     Peugeot–BP–Michelin      2      1.64          67.21      1.64          67.21\n#&gt;                   Peugeot–Esso–Michelin      1      0.82          68.03      0.82          68.03\n#&gt;                          Peugeot–Wolber      7      5.74          73.77      5.74          73.77\n#&gt;                             Renault–Elf      2      1.64          75.41      1.64          75.41\n#&gt;                      Renault–Elf–Gitane      2      1.64          77.05      1.64          77.05\n#&gt;                          Renault–Gitane      1      0.82          77.87      0.82          77.87\n#&gt;               Renault–Gitane–Campagnolo      1      0.82          78.69      0.82          78.69\n#&gt;                                Reynolds      1      0.82          79.51      0.82          79.51\n#&gt;             Saint-Raphaël–Gitane–Dunlop      1      0.82          80.33      0.82          80.33\n#&gt;       Saint-Raphaël–Gitane–R. Geminiani      1      0.82          81.15      0.82          81.15\n#&gt;        Saint-Raphaël–Helyett–Hutchinson      1      0.82          81.97      0.82          81.97\n#&gt;                               Salvarani      1      0.82          82.79      0.82          82.79\n#&gt;                                   Spain      1      0.82          83.61      0.82          83.61\n#&gt;                             Switzerland      2      1.64          85.25      1.64          85.25\n#&gt;                                Team CSC      1      0.82          86.07      0.82          86.07\n#&gt;                              Team Ineos      1      0.82          86.89      0.82          86.89\n#&gt;                        Team Jumbo–Visma      2      1.64          88.52      1.64          88.52\n#&gt;                          Team Saxo Bank      1      0.82          89.34      0.82          89.34\n#&gt;                                Team Sky      6      4.92          94.26      4.92          94.26\n#&gt;                            Team Telekom      2      1.64          95.90      1.64          95.90\n#&gt;                        TI–Raleigh–Creda      1      0.82          96.72      0.82          96.72\n#&gt;                       UAE Team Emirates      3      2.46          99.18      2.46          99.18\n#&gt;                               Z–Tomasso      1      0.82         100.00      0.82         100.00\n#&gt;                                    &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                   Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Distance  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                 Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------------------------- ------ --------- -------------- --------- --------------\n#&gt;                             —     11      9.02           9.02      9.02           9.02\n#&gt;           2,428 km (1,509 mi)      2      1.64          10.66      1.64          10.66\n#&gt;           2,994 km (1,860 mi)      1      0.82          11.48      0.82          11.48\n#&gt;           3,278 km (2,037 mi)      1      0.82          12.30      0.82          12.30\n#&gt;           3,285 km (2,041 mi)      1      0.82          13.11      0.82          13.11\n#&gt;           3,286 km (2,042 mi)      1      0.82          13.93      0.82          13.93\n#&gt;           3,328 km (2,068 mi)      1      0.82          14.75      0.82          14.75\n#&gt;           3,349 km (2,081 mi)      1      0.82          15.57      0.82          15.57\n#&gt;           3,359 km (2,087 mi)      1      0.82          16.39      0.82          16.39\n#&gt;       3,360.3 km (2,088.0 mi)      1      0.82          17.21      0.82          17.21\n#&gt;           3,366 km (2,092 mi)      1      0.82          18.03      0.82          18.03\n#&gt;           3,391 km (2,107 mi)      1      0.82          18.85      0.82          18.85\n#&gt;           3,404 km (2,115 mi)      1      0.82          19.67      0.82          19.67\n#&gt;           3,406 km (2,116 mi)      1      0.82          20.49      0.82          20.49\n#&gt;       3,414.4 km (2,121.6 mi)      1      0.82          21.31      0.82          21.31\n#&gt;           3,427 km (2,129 mi)      1      0.82          22.13      0.82          22.13\n#&gt;           3,430 km (2,130 mi)      1      0.82          22.95      0.82          22.95\n#&gt;           3,458 km (2,149 mi)      1      0.82          23.77      0.82          23.77\n#&gt;           3,459 km (2,149 mi)      1      0.82          24.59      0.82          24.59\n#&gt;           3,484 km (2,165 mi)      1      0.82          25.41      0.82          25.41\n#&gt;           3,496 km (2,172 mi)      1      0.82          26.23      0.82          26.23\n#&gt;           3,498 km (2,174 mi)      1      0.82          27.05      0.82          27.05\n#&gt;           3,504 km (2,177 mi)      1      0.82          27.87      0.82          27.87\n#&gt;           3,507 km (2,179 mi)      1      0.82          28.69      0.82          28.69\n#&gt;           3,529 km (2,193 mi)      1      0.82          29.51      0.82          29.51\n#&gt;           3,540 km (2,200 mi)      1      0.82          30.33      0.82          30.33\n#&gt;           3,559 km (2,211 mi)      1      0.82          31.15      0.82          31.15\n#&gt;           3,570 km (2,220 mi)      1      0.82          31.97      0.82          31.97\n#&gt;           3,608 km (2,242 mi)      1      0.82          32.79      0.82          32.79\n#&gt;           3,635 km (2,259 mi)      1      0.82          33.61      0.82          33.61\n#&gt;           3,642 km (2,263 mi)      1      0.82          34.43      0.82          34.43\n#&gt;           3,657 km (2,272 mi)      1      0.82          35.25      0.82          35.25\n#&gt;       3,660.5 km (2,274.5 mi)      1      0.82          36.07      0.82          36.07\n#&gt;           3,662 km (2,275 mi)      1      0.82          36.89      0.82          36.89\n#&gt;           3,687 km (2,291 mi)      1      0.82          37.70      0.82          37.70\n#&gt;           3,714 km (2,308 mi)      1      0.82          38.52      0.82          38.52\n#&gt;           3,753 km (2,332 mi)      1      0.82          39.34      0.82          39.34\n#&gt;           3,765 km (2,339 mi)      2      1.64          40.98      1.64          40.98\n#&gt;           3,809 km (2,367 mi)      1      0.82          41.80      0.82          41.80\n#&gt;           3,842 km (2,387 mi)      1      0.82          42.62      0.82          42.62\n#&gt;           3,846 km (2,390 mi)      1      0.82          43.44      0.82          43.44\n#&gt;           3,875 km (2,408 mi)      1      0.82          44.26      0.82          44.26\n#&gt;           3,908 km (2,428 mi)      1      0.82          45.08      0.82          45.08\n#&gt;           3,914 km (2,432 mi)      1      0.82          45.90      0.82          45.90\n#&gt;           3,950 km (2,450 mi)      1      0.82          46.72      0.82          46.72\n#&gt;           3,978 km (2,472 mi)      1      0.82          47.54      0.82          47.54\n#&gt;           3,983 km (2,475 mi)      1      0.82          48.36      0.82          48.36\n#&gt;           4,000 km (2,500 mi)      1      0.82          49.18      0.82          49.18\n#&gt;           4,017 km (2,496 mi)      1      0.82          50.00      0.82          50.00\n#&gt;           4,021 km (2,499 mi)      1      0.82          50.82      0.82          50.82\n#&gt;           4,090 km (2,540 mi)      1      0.82          51.64      0.82          51.64\n#&gt;           4,094 km (2,544 mi)      1      0.82          52.46      0.82          52.46\n#&gt;           4,096 km (2,545 mi)      1      0.82          53.28      0.82          53.28\n#&gt;           4,098 km (2,546 mi)      1      0.82          54.10      0.82          54.10\n#&gt;           4,109 km (2,553 mi)      1      0.82          54.92      0.82          54.92\n#&gt;           4,117 km (2,558 mi)      1      0.82          55.74      0.82          55.74\n#&gt;           4,138 km (2,571 mi)      1      0.82          56.56      0.82          56.56\n#&gt;           4,173 km (2,593 mi)      1      0.82          57.38      0.82          57.38\n#&gt;           4,188 km (2,602 mi)      1      0.82          58.20      0.82          58.20\n#&gt;           4,224 km (2,625 mi)      1      0.82          59.02      0.82          59.02\n#&gt;           4,231 km (2,629 mi)      1      0.82          59.84      0.82          59.84\n#&gt;           4,254 km (2,643 mi)      1      0.82          60.66      0.82          60.66\n#&gt;           4,274 km (2,656 mi)      1      0.82          61.48      0.82          61.48\n#&gt;           4,319 km (2,684 mi)      1      0.82          62.30      0.82          62.30\n#&gt;           4,329 km (2,690 mi)      1      0.82          63.11      0.82          63.11\n#&gt;           4,338 km (2,696 mi)      1      0.82          63.93      0.82          63.93\n#&gt;           4,358 km (2,708 mi)      1      0.82          64.75      0.82          64.75\n#&gt;           4,395 km (2,731 mi)      1      0.82          65.57      0.82          65.57\n#&gt;           4,397 km (2,732 mi)      1      0.82          66.39      0.82          66.39\n#&gt;           4,415 km (2,743 mi)      1      0.82          67.21      0.82          67.21\n#&gt;           4,442 km (2,760 mi)      1      0.82          68.03      0.82          68.03\n#&gt;           4,470 km (2,780 mi)      1      0.82          68.85      0.82          68.85\n#&gt;           4,476 km (2,781 mi)      1      0.82          69.67      0.82          69.67\n#&gt;           4,479 km (2,783 mi)      1      0.82          70.49      0.82          70.49\n#&gt;           4,488 km (2,789 mi)      1      0.82          71.31      0.82          71.31\n#&gt;           4,492 km (2,791 mi)      1      0.82          72.13      0.82          72.13\n#&gt;           4,495 km (2,793 mi)      1      0.82          72.95      0.82          72.95\n#&gt;           4,497 km (2,794 mi)      1      0.82          73.77      0.82          73.77\n#&gt;           4,498 km (2,795 mi)      2      1.64          75.41      1.64          75.41\n#&gt;           4,504 km (2,799 mi)      1      0.82          76.23      0.82          76.23\n#&gt;           4,637 km (2,881 mi)      1      0.82          77.05      0.82          77.05\n#&gt;           4,642 km (2,884 mi)      1      0.82          77.87      0.82          77.87\n#&gt;           4,656 km (2,893 mi)      1      0.82          78.69      0.82          78.69\n#&gt;           4,669 km (2,901 mi)      1      0.82          79.51      0.82          79.51\n#&gt;           4,690 km (2,910 mi)      1      0.82          80.33      0.82          80.33\n#&gt;           4,694 km (2,917 mi)      1      0.82          81.15      0.82          81.15\n#&gt;           4,734 km (2,942 mi)      1      0.82          81.97      0.82          81.97\n#&gt;           4,773 km (2,966 mi)      1      0.82          82.79      0.82          82.79\n#&gt;           4,779 km (2,970 mi)      1      0.82          83.61      0.82          83.61\n#&gt;           4,808 km (2,988 mi)      1      0.82          84.43      0.82          84.43\n#&gt;           4,822 km (2,996 mi)      1      0.82          85.25      0.82          85.25\n#&gt;           4,898 km (3,043 mi)      1      0.82          86.07      0.82          86.07\n#&gt;           4,922 km (3,058 mi)      1      0.82          86.89      0.82          86.89\n#&gt;           5,091 km (3,163 mi)      1      0.82          87.70      0.82          87.70\n#&gt;           5,286 km (3,285 mi)      1      0.82          88.52      0.82          88.52\n#&gt;           5,287 km (3,285 mi)      1      0.82          89.34      0.82          89.34\n#&gt;           5,289 km (3,286 mi)      1      0.82          90.16      0.82          90.16\n#&gt;           5,343 km (3,320 mi)      1      0.82          90.98      0.82          90.98\n#&gt;           5,375 km (3,340 mi)      1      0.82          91.80      0.82          91.80\n#&gt;           5,380 km (3,340 mi)      1      0.82          92.62      0.82          92.62\n#&gt;           5,386 km (3,347 mi)      1      0.82          93.44      0.82          93.44\n#&gt;           5,398 km (3,354 mi)      1      0.82          94.26      0.82          94.26\n#&gt;           5,425 km (3,371 mi)      1      0.82          95.08      0.82          95.08\n#&gt;           5,440 km (3,380 mi)      1      0.82          95.90      0.82          95.90\n#&gt;           5,476 km (3,403 mi)      1      0.82          96.72      0.82          96.72\n#&gt;           5,485 km (3,408 mi)      1      0.82          97.54      0.82          97.54\n#&gt;           5,503 km (3,419 mi)      1      0.82          98.36      0.82          98.36\n#&gt;           5,560 km (3,450 mi)      1      0.82          99.18      0.82          99.18\n#&gt;           5,745 km (3,570 mi)      1      0.82         100.00      0.82         100.00\n#&gt;                          &lt;NA&gt;      0                               0.00         100.00\n#&gt;                         Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Time/Points  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;       100h 30′ 35″      1      0.82          15.57      0.82          15.57\n#&gt;       100h 49′ 30″      1      0.82          16.39      0.82          16.39\n#&gt;       101h 01′ 20″      1      0.82          17.21      0.82          17.21\n#&gt;       103h 06′ 50″      1      0.82          18.03      0.82          18.03\n#&gt;       103h 38′ 38″      1      0.82          18.85      0.82          18.85\n#&gt;       105h 07′ 52″      1      0.82          19.67      0.82          19.67\n#&gt;       108h 17′ 18″      1      0.82          20.49      0.82          20.49\n#&gt;       108h 18′ 00″      1      0.82          21.31      0.82          21.31\n#&gt;       109h 19′ 14″      1      0.82          22.13      0.82          22.13\n#&gt;       110h 35′ 19″      1      0.82          22.95      0.82          22.95\n#&gt;       112h 03′ 40″      1      0.82          23.77      0.82          23.77\n#&gt;       112h 08′ 42″      1      0.82          24.59      0.82          24.59\n#&gt;       113h 24′ 23″      1      0.82          25.41      0.82          25.41\n#&gt;       113h 30′ 05″      1      0.82          26.23      0.82          26.23\n#&gt;       114h 31′ 54″      1      0.82          27.05      0.82          27.05\n#&gt;       114h 35′ 31″      1      0.82          27.87      0.82          27.87\n#&gt;       115h 27′ 42″      1      0.82          28.69      0.82          28.69\n#&gt;       115h 38′ 30″      1      0.82          29.51      0.82          29.51\n#&gt;       116h 16′ 02″      1      0.82          30.33      0.82          30.33\n#&gt;       116h 16′ 58″      1      0.82          31.15      0.82          31.15\n#&gt;       116h 22′ 23″      1      0.82          31.97      0.82          31.97\n#&gt;       116h 42′ 06″      1      0.82          32.79      0.82          32.79\n#&gt;       116h 59′ 05″      1      0.82          33.61      0.82          33.61\n#&gt;       117h 34′ 21″      1      0.82          34.43      0.82          34.43\n#&gt;       119h 31′ 49″      1      0.82          35.25      0.82          35.25\n#&gt;       122h 01′ 33″      1      0.82          36.07      0.82          36.07\n#&gt;       122h 25′ 34″      1      0.82          36.89      0.82          36.89\n#&gt;       123h 46′ 45″      1      0.82          37.70      0.82          37.70\n#&gt;       124h 01′ 16″      1      0.82          38.52      0.82          38.52\n#&gt;       127h 09′ 44″      1      0.82          39.34      0.82          39.34\n#&gt;       129h 23′ 25″      1      0.82          40.16      0.82          40.16\n#&gt;       130h 29′ 26″      1      0.82          40.98      0.82          40.98\n#&gt;       132h 03′ 17″      1      0.82          41.80      0.82          41.80\n#&gt;       133h 49′ 42″      1      0.82          42.62      0.82          42.62\n#&gt;       135h 44′ 42″      1      0.82          43.44      0.82          43.44\n#&gt;       136h 53′ 50″      1      0.82          44.26      0.82          44.26\n#&gt;       138h 58′ 31″      1      0.82          45.08      0.82          45.08\n#&gt;       140h 06′ 05″      1      0.82          45.90      0.82          45.90\n#&gt;       141h 23′ 00″      1      0.82          46.72      0.82          46.72\n#&gt;       142h 20′ 14″      1      0.82          47.54      0.82          47.54\n#&gt;       142h 47′ 32″      1      0.82          48.36      0.82          48.36\n#&gt;       145h 36′ 56″      1      0.82          49.18      0.82          49.18\n#&gt;       147h 10′ 36″      1      0.82          50.00      0.82          50.00\n#&gt;       147h 13′ 58″      1      0.82          50.82      0.82          50.82\n#&gt;       147h 51′ 37″      1      0.82          51.64      0.82          51.64\n#&gt;       148h 11′ 25″      1      0.82          52.46      0.82          52.46\n#&gt;       148h 29′ 12″      1      0.82          53.28      0.82          53.28\n#&gt;       149h 40′ 49″      1      0.82          54.10      0.82          54.10\n#&gt;       151h 57′ 20″      1      0.82          54.92      0.82          54.92\n#&gt;       154h 11′ 49″      1      0.82          55.74      0.82          55.74\n#&gt;       172h 12′ 16″      1      0.82          56.56      0.82          56.56\n#&gt;       177h 10′ 03″      1      0.82          57.38      0.82          57.38\n#&gt;       186h 39′ 15″      1      0.82          58.20      0.82          58.20\n#&gt;       192h 48′ 58″      1      0.82          59.02      0.82          59.02\n#&gt;       197h 54′ 00″      1      0.82          59.84      0.82          59.84\n#&gt;       198h 16′ 42″      1      0.82          60.66      0.82          60.66\n#&gt;       200h 28′ 48″      1      0.82          61.48      0.82          61.48\n#&gt;       219h 10′ 18″      1      0.82          62.30      0.82          62.30\n#&gt;       221h 50′ 26″      1      0.82          63.11      0.82          63.11\n#&gt;       222h 08′ 06″      1      0.82          63.93      0.82          63.93\n#&gt;       222h 15′ 30″      1      0.82          64.75      0.82          64.75\n#&gt;       226h 18′ 21″      1      0.82          65.57      0.82          65.57\n#&gt;       228h 36′ 13″      1      0.82          66.39      0.82          66.39\n#&gt;       231h 07′ 15″      1      0.82          67.21      0.82          67.21\n#&gt;       238h 44′ 25″      1      0.82          68.03      0.82          68.03\n#&gt;                 31      1      0.82          68.85      0.82          68.85\n#&gt;                 35      1      0.82          69.67      0.82          69.67\n#&gt;                 36      1      0.82          70.49      0.82          70.49\n#&gt;                 37      1      0.82          71.31      0.82          71.31\n#&gt;                 43      1      0.82          72.13      0.82          72.13\n#&gt;                 47      1      0.82          72.95      0.82          72.95\n#&gt;                 49      1      0.82          73.77      0.82          73.77\n#&gt;                 63      1      0.82          74.59      0.82          74.59\n#&gt;        79h 32′ 29″      1      0.82          75.41      0.82          75.41\n#&gt;        82h 05′ 42″      1      0.82          76.23      0.82          76.23\n#&gt;        82h 56′ 36″      1      0.82          77.05      0.82          77.05\n#&gt;        82h 57′ 00″      1      0.82          77.87      0.82          77.87\n#&gt;        83h 17′ 13″      1      0.82          78.69      0.82          78.69\n#&gt;        83h 38′ 56″      1      0.82          79.51      0.82          79.51\n#&gt;        83h 56′ 20″      1      0.82          80.33      0.82          80.33\n#&gt;        84h 27′ 53″      1      0.82          81.15      0.82          81.15\n#&gt;        84h 46′ 14″      1      0.82          81.97      0.82          81.97\n#&gt;        85h 48′ 35″      1      0.82          82.79      0.82          82.79\n#&gt;        86h 12′ 22″      1      0.82          83.61      0.82          83.61\n#&gt;        86h 20′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;        87h 20′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;        87h 34′ 47″      1      0.82          86.07      0.82          86.07\n#&gt;        87h 38′ 35″      1      0.82          86.89      0.82          86.89\n#&gt;        87h 52′ 52″      1      0.82          87.70      0.82          87.70\n#&gt;        89h 04′ 48″      1      0.82          88.52      0.82          88.52\n#&gt;        89h 40′ 27″      1      0.82          89.34      0.82          89.34\n#&gt;        89h 59′ 06″      1      0.82          90.16      0.82          90.16\n#&gt;        90h 43′ 20″      1      0.82          90.98      0.82          90.98\n#&gt;        91h 00′ 26″      1      0.82          91.80      0.82          91.80\n#&gt;        91h 59′ 27″      1      0.82          92.62      0.82          92.62\n#&gt;        92h 08′ 46″      1      0.82          93.44      0.82          93.44\n#&gt;        92h 44′ 59″      1      0.82          94.26      0.82          94.26\n#&gt;        92h 49′ 46″      1      0.82          95.08      0.82          95.08\n#&gt;        94h 33′ 14″      1      0.82          95.90      0.82          95.90\n#&gt;        95h 57′ 09″      1      0.82          96.72      0.82          96.72\n#&gt;        95h 57′ 16″      1      0.82          97.54      0.82          97.54\n#&gt;        96h 05′ 55″      1      0.82          98.36      0.82          98.36\n#&gt;        96h 19′ 38″      1      0.82          99.18      0.82          99.18\n#&gt;        96h 45′ 14″      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Margin  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;           + 1′ 07″      1      0.82          15.57      0.82          15.57\n#&gt;           + 1′ 11″      1      0.82          16.39      0.82          16.39\n#&gt;           + 1′ 12″      1      0.82          17.21      0.82          17.21\n#&gt;           + 1′ 22″      1      0.82          18.03      0.82          18.03\n#&gt;           + 1′ 25″      1      0.82          18.85      0.82          18.85\n#&gt;           + 1′ 34″      1      0.82          19.67      0.82          19.67\n#&gt;           + 1′ 41″      1      0.82          20.49      0.82          20.49\n#&gt;           + 1′ 42″      1      0.82          21.31      0.82          21.31\n#&gt;           + 1′ 50″      1      0.82          22.13      0.82          22.13\n#&gt;           + 1′ 51″      1      0.82          22.95      0.82          22.95\n#&gt;          + 10′ 32″      1      0.82          23.77      0.82          23.77\n#&gt;          + 10′ 41″      1      0.82          24.59      0.82          24.59\n#&gt;          + 10′ 55″      1      0.82          25.41      0.82          25.41\n#&gt;          + 12′ 14″      1      0.82          26.23      0.82          26.23\n#&gt;          + 12′ 41″      1      0.82          27.05      0.82          27.05\n#&gt;          + 12′ 56″      1      0.82          27.87      0.82          27.87\n#&gt;          + 13′ 07″      1      0.82          28.69      0.82          28.69\n#&gt;          + 14′ 13″      1      0.82          29.51      0.82          29.51\n#&gt;          + 14′ 18″      1      0.82          30.33      0.82          30.33\n#&gt;          + 14′ 34″      1      0.82          31.15      0.82          31.15\n#&gt;          + 14′ 56″      1      0.82          31.97      0.82          31.97\n#&gt;          + 15′ 49″      1      0.82          32.79      0.82          32.79\n#&gt;          + 15′ 51″      1      0.82          33.61      0.82          33.61\n#&gt;          + 17′ 52″      1      0.82          34.43      0.82          34.43\n#&gt;          + 17′ 54″      1      0.82          35.25      0.82          35.25\n#&gt;          + 18′ 27″      1      0.82          36.07      0.82          36.07\n#&gt;          + 18′ 36″      1      0.82          36.89      0.82          36.89\n#&gt;       + 1h 22′ 25″      1      0.82          37.70      0.82          37.70\n#&gt;       + 1h 42′ 54″      1      0.82          38.52      0.82          38.52\n#&gt;       + 1h 48′ 41″      1      0.82          39.34      0.82          39.34\n#&gt;           + 2′ 16″      1      0.82          40.16      0.82          40.16\n#&gt;           + 2′ 40″      1      0.82          40.98      0.82          40.98\n#&gt;           + 2′ 43″      1      0.82          41.80      0.82          41.80\n#&gt;           + 2′ 47″      1      0.82          42.62      0.82          42.62\n#&gt;          + 22′ 00″      1      0.82          43.44      0.82          43.44\n#&gt;              + 23″      1      0.82          44.26      0.82          44.26\n#&gt;          + 24′ 03″      1      0.82          45.08      0.82          45.08\n#&gt;          + 26′ 16″      1      0.82          45.90      0.82          45.90\n#&gt;          + 26′ 55″      1      0.82          46.72      0.82          46.72\n#&gt;          + 27′ 31″      1      0.82          47.54      0.82          47.54\n#&gt;          + 28′ 17″      1      0.82          48.36      0.82          48.36\n#&gt;       + 2h 16′ 14″      1      0.82          49.18      0.82          49.18\n#&gt;       + 2h 59′ 21″      1      0.82          50.00      0.82          50.00\n#&gt;           + 3′ 10″      2      1.64          51.64      1.64          51.64\n#&gt;           + 3′ 21″      2      1.64          53.28      1.64          53.28\n#&gt;           + 3′ 35″      1      0.82          54.10      0.82          54.10\n#&gt;           + 3′ 36″      1      0.82          54.92      0.82          54.92\n#&gt;           + 3′ 40″      1      0.82          55.74      0.82          55.74\n#&gt;           + 3′ 56″      1      0.82          56.56      0.82          56.56\n#&gt;           + 3′ 58″      1      0.82          57.38      0.82          57.38\n#&gt;          + 30 '41″      1      0.82          58.20      0.82          58.20\n#&gt;          + 30′ 38″      1      0.82          59.02      0.82          59.02\n#&gt;              + 32″      1      0.82          59.84      0.82          59.84\n#&gt;          + 35′ 36″      1      0.82          60.66      0.82          60.66\n#&gt;              + 38″      1      0.82          61.48      0.82          61.48\n#&gt;           + 4′ 01″      2      1.64          63.11      1.64          63.11\n#&gt;           + 4′ 04″      1      0.82          63.93      0.82          63.93\n#&gt;           + 4′ 05″      1      0.82          64.75      0.82          64.75\n#&gt;           + 4′ 11″      1      0.82          65.57      0.82          65.57\n#&gt;           + 4′ 14″      1      0.82          66.39      0.82          66.39\n#&gt;           + 4′ 20″      1      0.82          67.21      0.82          67.21\n#&gt;           + 4′ 35″      2      1.64          68.85      1.64          68.85\n#&gt;           + 4′ 53″      1      0.82          69.67      0.82          69.67\n#&gt;           + 4′ 59″      2      1.64          71.31      1.64          71.31\n#&gt;              + 40″      1      0.82          72.13      0.82          72.13\n#&gt;          + 41′ 15″      1      0.82          72.95      0.82          72.95\n#&gt;              + 48″      1      0.82          73.77      0.82          73.77\n#&gt;           + 5′ 02″      1      0.82          74.59      0.82          74.59\n#&gt;           + 5′ 20″      1      0.82          75.41      0.82          75.41\n#&gt;           + 5′ 39″      1      0.82          76.23      0.82          76.23\n#&gt;          + 50′ 07″      1      0.82          77.05      0.82          77.05\n#&gt;          + 54′ 20″      1      0.82          77.87      0.82          77.87\n#&gt;              + 54″      1      0.82          78.69      0.82          78.69\n#&gt;              + 55″      1      0.82          79.51      0.82          79.51\n#&gt;          + 57′ 21″      1      0.82          80.33      0.82          80.33\n#&gt;              + 58″      1      0.82          81.15      0.82          81.15\n#&gt;              + 59″      1      0.82          81.97      0.82          81.97\n#&gt;           + 6′ 17″      1      0.82          82.79      0.82          82.79\n#&gt;           + 6′ 21″      1      0.82          83.61      0.82          83.61\n#&gt;           + 6′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;           + 7′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;           + 7′ 17″      1      0.82          86.07      0.82          86.07\n#&gt;           + 7′ 29″      1      0.82          86.89      0.82          86.89\n#&gt;           + 7′ 37″      1      0.82          87.70      0.82          87.70\n#&gt;           + 8′ 04″      1      0.82          88.52      0.82          88.52\n#&gt;           + 8′ 37″      1      0.82          89.34      0.82          89.34\n#&gt;               + 8″      1      0.82          90.16      0.82          90.16\n#&gt;           + 9′ 09″      1      0.82          90.98      0.82          90.98\n#&gt;           + 9′ 30″      1      0.82          91.80      0.82          91.80\n#&gt;           + 9′ 51″      1      0.82          92.62      0.82          92.62\n#&gt;           +44′ 23″      1      0.82          93.44      0.82          93.44\n#&gt;                 18      1      0.82          94.26      0.82          94.26\n#&gt;                 19      1      0.82          95.08      0.82          95.08\n#&gt;                 20      1      0.82          95.90      0.82          95.90\n#&gt;                 26      1      0.82          96.72      0.82          96.72\n#&gt;                 32      1      0.82          97.54      0.82          97.54\n#&gt;                  4      1      0.82          98.36      0.82          98.36\n#&gt;                 59      1      0.82          99.18      0.82          99.18\n#&gt;                  8      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Stage wins  \n#&gt; Type: Character  \n#&gt; \n#&gt;               Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------- ------ --------- -------------- --------- --------------\n#&gt;           —     18     14.75          14.75     14.75          14.75\n#&gt;           0      8      6.56          21.31      6.56          21.31\n#&gt;           1     20     16.39          37.70     16.39          37.70\n#&gt;           2     27     22.13          59.84     22.13          59.84\n#&gt;           3     19     15.57          75.41     15.57          75.41\n#&gt;           4     12      9.84          85.25      9.84          85.25\n#&gt;           5      8      6.56          91.80      6.56          91.80\n#&gt;           6      6      4.92          96.72      4.92          96.72\n#&gt;           7      2      1.64          98.36      1.64          98.36\n#&gt;           8      2      1.64         100.00      1.64         100.00\n#&gt;        &lt;NA&gt;      0                               0.00         100.00\n#&gt;       Total    122    100.00         100.00    100.00         100.00\nBut here again we need to be careful. For example the distance variable is coded as a character, and its reporting with frequencies is thus quite stupid.\nThere is nothing automatic, you still need to think about, transform or subset your data.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Summary tables</span>"
    ]
  },
  {
    "objectID": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "href": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "title": "33  Summary tables",
    "section": "33.2 Descriptive statistics with modelsummary",
    "text": "33.2 Descriptive statistics with modelsummary\nto be done",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Summary tables</span>"
    ]
  },
  {
    "objectID": "904_curve.html",
    "href": "904_curve.html",
    "title": "34  Drawing curves in R",
    "section": "",
    "text": "curve() takes an expression, i.e. a quote or a function call, and evaluates it when drawing for a series of points between the from and to limits (default 0 to 1)\n\ncurve((x))\n\n\n\n\n\n\n\ncurve((x),from=-2, to=2)\n\n\n\n\n\n\n\n\n\npar(pty=\"s\") #square graphic\ncurve((x),-10,10, axes = FALSE, xlim=c(-10,10),ylim=c(-10,10))\naxis(1, pos=0, col=\"grey\")\naxis(2, pos=0, col=\"grey\")\ncurve(x^2, add=TRUE, col=\"blue\")\ncurve(x^3, add=TRUE, col=\"orange\")\ncurve(log, add=TRUE, col=\"red\")\n#&gt; Warning in log(x): NaNs produced\ncurve(exp, add=TRUE, col=\"purple\")\n\n\n\n\n\n\n\n\n\npar(pty=\"s\") #square graphic\ncurve(tan,-5,5, col=\"gold\",\n      axes = FALSE, xlim=c(-5,5),ylim=c(-2,2))\naxis(1, pos=0, col=\"grey\")\naxis(2, pos=0, col=\"grey\")\ncurve(cos, add=TRUE, col=\"red\")\ncurve(sin, add=TRUE, col=\"blue\")\n\n\n\n\n\n\n\n\n\ncurve((0.3*x^2-10*x+20), from=-10, to=30)",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Drawing curves in R</span>"
    ]
  },
  {
    "objectID": "999_references.html",
    "href": "999_references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear\nRegression Models with an Introduction to Spatial Econometrics:\nRegression Models with an Anselin Bera i. INTRODUCTION.” In. CRC\nPress.\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An\nAxiomatic Approach to\nGeographical Space.” Geographical\nAnalysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.\n\n\nFerguson, Rob. n.d. Linear Regression in Geography. Vol. 15.\nCATMOG (Concepts And Techniques in MOdern Geography). https://github.com/qmrg/CATMOG/blob/Main/15-linear-regression-in-geography.pdf.\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied\nRegression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced\nData.” In, edited by Manfred M. Fischer and Arthur Getis,\n197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd\nEdition. https://rc2e.com/.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem.\nConcepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth\nin the Detroit Region in Economic Geography 46: 2,\n234240.” Clark University, Worcester, MA.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to\nr.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d.\n“R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Bibliography",
      "References"
    ]
  },
  {
    "objectID": "010_intro.html",
    "href": "010_intro.html",
    "title": "Part I - Introduction",
    "section": "",
    "text": "1  Statistical data analysis for geographers\n2  Spatial data analysis: a definition\n3  Geographical space\n4  Spatial data issues",
    "crumbs": [
      "Part I - Introduction"
    ]
  },
  {
    "objectID": "020_getting_started.html",
    "href": "020_getting_started.html",
    "title": "Part II - Getting Started with R",
    "section": "",
    "text": "5  Getting started with RStudio\n6  Vectors\n7  Data frames and lists\n8  Working with data frames and functions\n9  Reading and writing data to and from R\nFor better introductory material, we recommend\n\nVenables, Bill and Smith, David, M (n.d.)\nLong and Teetor (n.d.)\nWickham, Çetinkaya-Rundel, and Grolemund (n.d.) (including RStudio introduction) (Note it uses a tidyverse approach while we rather stick to base R where possible, except for graphics (ggplot))\n\n\n\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd Edition. https://rc2e.com/.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to r.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d. “R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part II - Getting Started with R"
    ]
  }
]