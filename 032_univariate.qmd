# Univariate statistics

**Univariate** analysis seeks to summarize a single variable or characteristic, such as the distribution of wages in a city or the number of commuters in each regions of a country.

We usually describe a variable using a general, average value, which is think is representative of the sample or of the population - we call it a **measure of center** - to which we add a notion of how spread the values are around that general/average/central value, we call a measure of **dispersion**. For both, there different possibilities, depending on the distribution of values and on the type of data.

## Measures of Center

### Mean

The arithmetic mean of a vector *x* having *n* observations: *x = (x~1~, x~2~,... , x~i~, ..., x~n~)* is given by the following formulae for the empirical and theoretical means:

$$\bar{x} = \dfrac{1}{n} \sum_{i=1}^n x_i$$ $$\mu_X = E(X) = \dfrac{1}{N} \sum_{i=1}^N X_i$$

with $n$ the size of the sample and $N$ the size of the population.

Let's take a variable with the ages of some individuals:

```{r}
Age <- c (25, 27, 28, 23, 52, 27, 27, 26, 25, 30)
```

The empirical mean of this variable is given by:

$(1 / 10) * (25 + 27 + 28 + 23 + 52 + 27 + 27 + 26 + 25 + 30) = 290 / 10 = 29$

You can compute the mean of a vector as follows in R:

```{r}
sum(Age) / length(Age)
mean(Age)
```

But in some cases, the mean is not a good indicator of the central value of a distribution. For the above variable, we can see that one individual is 52 years old. **The mean is sensitive to extreme values** or outliers.

### Median

The median corresponds to the value such that 50% of the individuals have a smaller or equal value and 50% of the individuals have a larger or equal value. It is also called **the 50th percentile**. We consider the variable *Age* and a second variable *Age2* where the value 52 has been removed. They have respectively 10 and 9 elements.

```{r}
Age2 <- c (25, 27, 28, 23, 27, 27, 26, 25, 30)
```

First, **sort** the values of the vector considered from the smallest to the largest. For Age: ${23, 25, 25, 26, 27, 27, 27, 28, 30, 52}$

and for Age2:

${23, 25, 25, 26, 27, 27, 27, 28, 30}$

We know already how to do this in R:

```{r}
sort(Age)
sort(Age2)
```

Second,

-   For an **odd** set of numbers (Age2), find the number in the middle of the vector, this is the empirical median. The number to take is also given by: $(n + 1) / 2 = 10 / 2$ , i.e. the 5th value, 27

```{r}
sort(Age2)[5]
```

-   For an **even** set of numbers (Age), find the two numbers in the middle and compute their average value, this is the empirical median, i.e. $(27 + 27) / 2 = 27$

```{r}
sort(Age)[c(5,6)]
sum(sort(Age)[c(5,6)])/2
```

We can see that the median is much *less sensitive to extreme values*. In both cases, the median is 27. Using R built-in functions, the median is :

```{r}
median(Age)
median(Age2)
```

### Mode

The mode (not to be mistaken with R mode for vectors) corresponds to the **value(s) which appears the most often**. A vector can have 0, 1 or many modes. For our variable *Age*, the mode is 27 which appears 3 times.

```{r}
table(Age)
sort(table(Age), decreasing = TRUE)
```

The mode is an immediate output in R. But we can write what we have just done and extract the first value after sorting, i.e.

```{r}
sort(table(Age),decreasing = TRUE)[1]
```

The output however is the frequency (3), not the number we look for (27). So we should get the name and turn it into a numeric for obtaining the mode as a numeric:

```{r}
as.numeric(names(sort(table(Age),decreasing = TRUE)[1]))
```

While this requires sorting (which can be long), an alternative would be to use the `which.max()` function, but still requires to get the value from name as a numeric:

```{r}
which.max(table(Age)) #returning the position of the max, i.e. 4th position here
table(Age)[which.max(table(Age))] #then using that position into the original vector
as.numeric(names(table(Age)[which.max(table(Age))]))
```

We can also look at the mode for qualitative / categorical variables. If we take the example of the variable *score*, the mode is the value "C".

```{r}
score <- as.factor ( c ("C","C","A","B","A","C","B","B","A","C"))
table(score)
names(which.max(table(score)))
```

Note that, that the numeric case being a little complicated, we can see from the help that `table()` is based on `tabulate()` and find the following which is more direct for numeric examples:

```{r}
which.max(tabulate(Age))
```

We can assemble both factor and numeric cases in a single function, for example:

```{r}
mymode<-function(x){
  ifelse(is.factor(x)==TRUE,
         names(which.max(table(x))),
         which.max(tabulate(x)))
}
mymode(Age)
mymode(score)
```

## Measures of Dispersion

### Range

Range is the simplest measure of the spread of a distribution and corresponds to the **difference between the maximum and the minimum values**: $$Max(x) - Min(x)$$

For the variable *Age* the minimum being 23 and the maximum 52, the range is: $52 - 23 = 29$.

In R the function range returns the two extrema, not the difference, see

```{r}
max(Age) - min(Age)
range(Age)
```

### Quantiles.

Extending the concept of a median, quantiles (percentiles, deciles, quartiles,...) divide the distribution into equal slices. The *i*-th percentile corresponds to the value at which *i%* of the distribution is below that value. The median is when $i=50%$, i.e. the ditribution is split into 2 half parts so that the probability of drawing a number below the median is `50%`.

Percentiles divide the distribution into 100 slices (probability = ${0.01, 0.02, 0.03, ..., 1}$) ; deciles into 10 (probability = ${0.1, 0.2, 0.3, ..., 1}$) ; quartiles into 4 (probability = ${0.25, 0.5, 0.75, 1}$). You obtain all of these using the same function `quantile()` and the corresponding probability of picking up a number below:

```{r}
# quartiles are the default
quantile(Age)
#deciles
quantile(Age, probs = seq(0, 1, 0.1))

#Suppose a larger set of 100000 values "normally" distributed around the mean 0
set.seed(233)
x<-rnorm(n = 100000, mean=0, sd=1)
#the 1st , 5th, 95th and 99th % and some others
quantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99))
```

```{r}
hist(x, breaks = 100)
abline(v=quantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99)), col="blue")
```

### Inter-quartile range (IQR)

It is simply the difference between the 3rd and the 1st quartiles: $$IQR = Q3(x) - Q1(x)$$.

Again for the variable *Age*, it equals: $27.75 - 25.25 = 2.5$

```{r}
quantile(Age, probs = 0.75) - quantile(Age, probs = 0.25)

#or
IQR(Age)
```

### Variance and Standard Deviation

Although quantiles and plots are very much in use to describe the spread of a distribution, statistical analysis relies most heavily on the notion of variance.

First, think about the simplest way you can measure how a given observation is far from, (i.e. spread out of) a general expected value. A pretty effective way is to measure the difference between that observation and the mean of observations.

The Deviation to the mean for an individual i is $$v_i=x_i-\bar{x}$$

It is then very tempting to say that the general dispersion of a variable is simply the sum of all those values. In order for the number not to grow with the number of observations, we then compute an average deviation by dividing by $n$

$$\Sigma_i(x_i-\bar{x})/n$$

But is this a good idea?

Take the Age example:

```{r}
v<- Age-mean(Age) #set of deviations to the mean
sum(v)/length(v)
```

It seems there is no "spreading" ? In fact, all the negative deviations compensate (here exactly) the positive deviations.

We can rather remove the signs and use the absolute value of each deviation, sum them up and divide by $n$.

This is called MAD, the **Mean Absolute Deviation** and is quite easy to interpret indeed.

```{r}
abs_v<- abs(Age-mean(Age)) #set of deviations to the mean
mean(abs_v)
```

Yet, one could argue that large deviations to the mean are more important than the smaller ones to describe the pattern of deviations, especially since in a normal population there are more values closer to the mean than farther.

Rather than using absolute deviations, (most of) statisticians have therefore opted for squaring the deviations, which is still symmetrical and has the same characteristic of turning every negative value into a positive one.

We therefore usually consider the sum of squared deviations to the mean:

$$\Sigma_i^n(x_i-\bar{x})^2$$ which, we then divide by the number of observations to avoid the value to grow with the number of observations, thus allowing comparisons. It is then called the **variance**. More precisely, if we use a sample, we still need to use one of our observation in order to estimate the mean, hence we are left with $n-1$ degrees of freedom.

The empirical variance is then given by:

$$var_x = s^2_x = \dfrac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$$

and the theoretical variance:

$$var_X = \sigma^2_X = \dfrac{1}{N} \sum_{i=1}^N (X_i-E(X))^2$$ $$= E(X-E(X))^2 = E(X-\mu)^2 $$

where $E(X)$ is the expected mean of the population (or "Esperance").

The variance is thus a single number that gives insight on how the variable is spread around the mean value. A small value (close to 0) indicates a small variability: values are not very different from the mean value. A high value indicated a strong variability.

The variance cannot be negative.

In R, we use the `var()` functio, which we here first reconstruct:

```{r}
(Age-mean(Age))^2 #squared deviations to the mean
sum((Age-mean(Age))^2) #sum of squared deviations to the mean
sum((Age-mean(Age))^2)/(length(Age)-1) #...divided by n-1

var(Age)
var(Age2)
```

We see that the default in R for `var()` is to divide by $n-1$, i.e. the sample variance.

Finally, we like the "dispersion" to be expressed in the same units as the original variable, i.e. years in this case. It is already the case for the Mean Absolute Deviation. We need to take the square root of the variance to obtain a "standard deviation":

The standard deviations corresponding to the sample and population variance are then given by:

$$s_x = \sqrt{\dfrac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2}$$

$$\sigma_X = \sqrt{\dfrac{1}{N} \sum_{i=1}^N (X_i-E(X))^2}$$

And can be computed using:

```{r}
sqrt(var(Age))
sqrt(var(Age2))
#or simply
sd(Age) #again remember it is the sample sd
sd(Age2)
```

## Visualize a distribution

Visualizing a distribution with graphics is important for both supporting the analysis and dissemination.

We have seen some graphics already above and we are going to produce improved graphics with ggplot later on. Without spending much time on design, the purpose here is show how graphics accompany the univariate statistics we introduced.

### Boxplots

A boxplot visually provides a number of information about the distribution of a variable

-   the median value (thick black line),
-   the inter-quartile range (IQR) (the black box),
-   the minimum and maximum values or 1.5 times the IQR (horizontal lines),
-   the outlier(s) (dots out of the whiskers).

Values are considered outliers when they fall outside the whiskers, that is outside a distance of 1.5 times the IQR. In absence of such outliers the horizontal lines show the extrema (min and max).

We have added the mean as a red point to clarify here the difference between the mean and median.

Examine the difference again between Age and Age2

```{r}
par (mfrow = c(1, 2)) # to display multiple plots at once
boxplot(Age, ylab = "Age", main = "Boxplot of Age")
points(mean(Age), col = 2, pch = 18)

boxplot(Age2, ylab = "Age2", main = "Boxplot of Age2")
points(mean(Age2), col = 2, pch = 18)
```

### Stem and leaf

Probably less in use nowadayd for visual purpose and reporting, a stem and leaf graph is a very effective way to look into the distribution of a variable while you are exploring, analyzing your data in the console.

It is not actually a plot but a presentation of the values into a "stem", which is made of the values that are present across all cases and then a "leaf" where the remaining parts of each numbers is shown and concatenated, thus showing a kind of frequency together with the values:

Examine the case for Age and Age2:

```{r}
stem(Age)
stem(Age2)
```

### Histogram

Histograms are probably the first go to graphic in order to visualize a distribution

Let's reuse the x normal variable we created earlier and plot both its boxplot and the histogram

```{r}
set.seed(233)
x<-rnorm(n = 100000, mean=0, sd=1)

summary(x)
boxplot(x, main = "Boxplot of a random variable following N(0,1) with n = 100,000")
hist(x)
```

A histogram is more detailed than a boxplot because it shows every data but does not provide a central or dispersion measure. Key to using a histogram is to play with the number of bars, otherwise some information, gaps, or multimodalities may be not be seen. You adapt the number of bars using the option "breaks"

```{r}
par(mfrow=c(1,2))
hist(x, breaks=5)
hist(x, breaks=100)
```

For a categorical variable (factor), the function `plot()` gives the counts of each category (level). We have worked an example using Le Tour de France data earlier in the course. It is similar to a visualisation of the `table()` output and is equivalent to the function `hist()` for quantitative variables.

```{r}
table(score)
plot(score, main = "Scores", ylab = "Frequency")
```

For a numeric variable, a call to plot, shows values along the vertical axis and the index of the rows along the horizontal axis, which is rarely a useful information.

```{r}
plot(x)
```

## The shape of distributions: Skewness and Kurtosis

### First and second moments:

We have seen earlier that the very first way to characterise a distribution is to use its mean and that the second way is to use the variance (or the square root of the variance, i.e. the standard deviation).

The mean and the variance are also named, respectively, the first and second moment of a distribution

Indeed, for discrete data, the **mean** (or expectation) is calculated as:

$$\mu = E[X] = \sum_{i} x_i p(x_i)$$ where $x_i$ are the values and $p(x_i)$ are their probabilities.

Note 1: for continuous variables, we should in fact use an integral rather that the sum symbol. Note 2, the "zeroth" moment of the distribution is in fact the sum of probabilities (x_i), i.e. the total mass (the concept is borrowed from physics), i.e. 1.

The **variance** is the second order moment, measuring the spread of the distribution around the mean. We have seen it is defined in difference to the mean with a square exponent:

$$\sigma^2 = E[(X - \mu)^2] = \sum_{i} (x_i - \mu)^2 p(x_i)$$ We can actually go on with higher exponents and use higher level moments to describe the shape of a distribution.

### Third moment: Skewness

The skewness is the third order moment of a distribution. The **skewness measures the asymmetry of the distribution around the mean**. It is is calculated as:

$$\gamma_1 = E\left[\left(\frac{X - \mu}{\sigma}\right)^3\right] = \sum_{i} \left(\frac{x_i - \mu}{\sigma}\right)^3 p(x_i)$$

In addition to the cubic exponent applied to the deviation to the mean, you notice we also divide by the standard deviation $\sigma$ to make the measure dimensionless, i.e. comparable across cases with different units. The skewness is not influenced by the scale of the data.

In practice, the skewness is computed as follows:

$$\dfrac {\sum_{i=1}^{n} (x_i - \bar{x})^3} {n s^3}$$

```{r}
set.seed(101)
x<-rnorm(1000, mean=50,sd=5)
mean(x)
median(x)
e1071::skewness(x)
e1071::skewness((x-100)/1000) #with this you see it is not influenced by any rescaling
sum((x-mean(x))^3)/(sd(x)^3*length(x)) #manual computation
```

When the skewness is $>0$, the tail of the distribution is heavier on the right side. This means there are more extreme values on the higher end. The mean is greater than the median.

When the skewness is $<0$, the tail of the distribution is heavier on the left side. This means there are more extreme values on the lower end. The mean is lower than the median.

A value around zero indicates a symmetric distribution.

```{r}
set.seed(101)
right_skewed <- x + rexp(1000, rate = 0.1) #we add an exp to the previous x for generating a right skewed distribution
left_skewed <- rnorm(1000, mean = 50, sd = 5) - rexp(100, rate = 0.1) #left skewed

hist(x, main = "Symmetric", col = "blue", breaks = 20)
hist(right_skewed, main = "Right-skewed",col = "green", breaks = 20)
hist(left_skewed, main = "Left-skewed", col = "red", breaks = 20)

e1071::skewness(x)
e1071::skewness(right_skewed)
e1071::skewness(left_skewed)

```

### Fourth moment: Kurtosis

The Kurtosis is the fourth order moment of a distribution. The **Kurtosis measures the peakness of the distribution** and is calculated as:

$$\gamma_2 = E\left[\left(\frac{X - \mu}{\sigma}\right)^4\right] - 3 = \sum_{i} \left(\frac{x_i - \mu}{\sigma}\right)^4 p(x_i) - 3$$

You notice the exponent to the level 4 applied to the deviations to the mean and that similarly to the skewness, we also divide by the standard deviation $\sigma$ to make the measure dimensionless, i.e. comparable across cases with different units. The Kurtosis is not influenced by the scale of the data.

In practice in R, the Kurtosis is similar to the skewness described above:

$$\dfrac {\sum_{i=1}^{n} (x_i - \bar{x})^4} {n s^4}$$ However it is adjusted with some correction for small sample sizes and for comparison to a normal distribution for which the result would be 3. See the help for how it is computed by default

```{r}
set.seed(101)
x<-rnorm(1000, mean=50,sd=5)
e1071::kurtosis(x)
e1071::kurtosis((x-100)/1000) #with this you see it is not dependent on scale 
```

Using this implementation, a Kurtosis close to 0 then indicates a distribution similar in shape to a normal (bell-shaped) distribution. A **positive kurtosis indicates a more peaked** distribution, also named **leptokurtic**.

A **negative kurtosis** indicates a **less peaked** shape, named **platykurtic**.

as an example, we can add some extreme values to the normal values we created earlier in order to higher peak look or trim the extremes of a normal distribution to have a flatter one:

```{r}
set.seed(1010)
x<-rnorm(1000, mean=50,sd=5)
x_peaker<-c(x, rnorm(300, mean = 50, sd = 20))
x_flatter<-x[abs(x) < 60]

hist(x, main = "Normal", col = "blue", breaks = 20)
hist(x_peaker, main = "Higher peak",col = "green", breaks = 20)
hist(x_flatter, main = "Flatter", col = "red", breaks = 20)

e1071::kurtosis(x)
e1071::kurtosis(x_peaker)
e1071::kurtosis(x_flatter)
```
