[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and R for geographers",
    "section": "",
    "text": "Preface\nThis is the syllabus for the course Introduction to Data Analysis for Geographers with R (MAGEO0641) at the Department of Geography and Spatial Planning at the University of Luxembourg. It has been produced as a Quarto book by Geoffrey Caruso and Léandre Fabri with the aim of aggregating and homogenizing different material accumulated over the past few years.\nAs of September 2024, the assemblage is still a work in progress. We kindly ask you to refer to your notes during class to prepare for the examination and assignments.\nThe general structure and base material shown here have been organized by Geoffrey Caruso, who originally inherited teaching material from Dominique Peeters. We are grateful to the previous teaching assistants, Mirjam Schindler and Marlène Boura, as well as to David Dabin and Jonathan Jones, for their contributions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001_timetable.html",
    "href": "001_timetable.html",
    "title": "Timetable",
    "section": "",
    "text": "The course is made of 13 sessions. Each session comprises 5 (teaching) units of 45 minutes.\nWe have attempted to balance theoretical explanations and R practice in each session.\nThe 2024-2025 schedule is set as follows. However, please refer to your student guichet and the Moodle platform for potential changes during the semester.\n\nScheduled: 16 Sep 2024 at 14:00 to 18:00, CEST\nScheduled: 23 Sep 2024 at 14:00 to 18:00, CEST\nNo session on 30 Sep 2024\nScheduled: 7 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 14 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 21 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 28 Oct 2024 at 14:00 to 18:00, CET\nScheduled: 4 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 11 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 18 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 25 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 2 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 9 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 16 Dec 2024 at 14:00 to 18:00, CET",
    "crumbs": [
      "Timetable"
    ]
  },
  {
    "objectID": "002_learning.html",
    "href": "002_learning.html",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Objectives:",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#objectives",
    "href": "002_learning.html#objectives",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Overview of key concepts and formalisation in data analysis in geographical contexts\nUnderstanding and describing the statistical and spatial distribution of data with univariate statistical analysis\nUnderstanding how a geographical pattern relate or can be understood from others with bivariate and regression analysis\nRaising awareness as to the characteristics and difficulties of statistical and econometric (regression) analysis with geographical data.\nMastering essential R software skills for tabular data management, uni and bi-variate analysis and regression, and producing graphics",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#expected-outcomes",
    "href": "002_learning.html#expected-outcomes",
    "title": "Learning outcomes and evaluation",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nOn completion, each student should be able to\n\nDescribe the key concepts in spatial statistical analysis and the specificities of geographical space and spatial data\nDemonstrate a good command of R to handle statistical datasets and perform univariate, bivariate and multiple regressions analyses with good diagnostics\nExplain and use common univariate and bivariate statistics\nExplain and use exploratory methods\nExplain and apply standard regression methods and diagnostics, and discuss limits and problems when applied to geographical data\nExplain the principles and methods used to identify local effects and spatial autocorrelation\nRead and discuss detailed results of an empirical research article that deal with data analysis including a multivariate regression in a spatial or non-spatial context\nExplain and use mixed methods (Q-Methodology: hybrid approach between quantitative and qualitative methods)",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#evaluation",
    "href": "002_learning.html#evaluation",
    "title": "Learning outcomes and evaluation",
    "section": "Evaluation",
    "text": "Evaluation\n\nIndividual\n20% Continuous assessment: small tests in class and weekly exercises:\n40% R exam (3h) in GIS room, perform R analysis, answer a questionnaire and provide script\n40% Oral exam (30min): presenting a paper and answering questions about its details and the course",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "011_scope.html",
    "href": "011_scope.html",
    "title": "1  Statistical data analysis for geographers",
    "section": "",
    "text": "1.1 Scope\nThis course is an introduction to standard statistical techniques that geographers often encounter. Being at the crossroads of different fields, it is necessary for geographers to have a basic set of tools with which to interact with other experts and modelers who, although they may use different wording and have their own technical habits, use a common set of concepts and tools to analyze data and test their hypotheses.\nIn their own work and in their interactions with others, it is also important for geographers to keep in mind that the data they use are quite specific because they are about located objects or subjects, about places and their interactions. Most of the time, the data they use is georeferenced in some way (accurately or not). This inherently geographic aspect brings with it a number of challenges. In this course, we will highlight these challenges when performing standard data analysis. However, we won’t solve any of these geographic problems, and we won’t even explicitly use geographic features, i.e., no mapping, no use of georeferencing as such. Our goal is to equip students with standard statistical methods also used in related fields, with some critical thinking about their geographical nature or underlying spatial processes. In a nutshell, this is a journey from elementary statistics to spatial autocorrelation with a standard regression detour.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "href": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "title": "1  Statistical data analysis for geographers",
    "section": "1.2 Transparent geographical analysis empowered with R",
    "text": "1.2 Transparent geographical analysis empowered with R\nThe course is a blend of theory and practice, which we believe enhances intuition and understanding. Direct practice also helps, especially for human geographers with little training in quantitative methods, to demystify statistical concepts and provide confidence after repeated applications and interpretations.\nSoftware for statistical analysis has evolved rapidly and R is prominent in many disciplines. It is open and free. It is simply fantastic for spatial data analysis and may well be the only tool geographers really need in their data undertaking, even replacing GIS (Geographic Information Systems) software. You just need to get started with R.\n\n\n\n\n\nMost of our students have had some sort of theoretical statistics course so far in their studies, and have probably seen most of the content. Sometimes our students have had some practice with SPSS (or similar) software, but most have not used any real statistical software at all, and have a spreadsheet (e.g., Excel) as their only reference for data management, analysis, and graphing.\nWe chose R for its openness, leadership, large community, and later for its spatial features, but also because it forces students to be transparent and think about every step they take. Data analysis and visualization can be misleading and dangerous, it is necessary to think and facilitate replication for oneself and for others. R, and more generally the use of scripts and command lines, is absolutely necessary to bring robustness and trust.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html",
    "href": "012_spatial_analysis.html",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "2.1 Interdisciplinarity and perspective\nData analysis and statistics are used in many scientific fields. When the focus is on geographic objects, subjects, their patterns or relationships, we like to talk about spatial analysis. However, because geographic data is relevant to many fields and statistical methods are widely used, spatial analysis can be defined and approached differently.\nWithin geography, we understand the term spatial analysis to refer to all quantitative approaches, as opposed to qualitative approaches (although these can also be spatial and analytical). Within the quantitative part of the discipline, however, spatial analysis would mostly refer to applied statistical approaches, in contrast to GIS modeling, geosimulation, transportation modeling, or mathematical models. In this sense, this course is a spatial analysis course.\nHowever, for those researchers involved in spatial analysis close to regional science and economic geography (and perhaps close to landscape ecology and GIScience), the terms spatial data analysis or spatial statistics would more strictly refer to the explicit use of geographic information in the modeling process, not just the consideration of geographic elements. See for example the handbook of Fischer and Getis for discussion and examples.\nSimilarly, Goodchild and Longley (https://www.geos.ed.ac.uk/~gisteac/gis_book_abridged/files/ch40.pdf) suggest more broadly that spatial analysis could simply be a set of methods useful when the data are spatial (i.e. referenced in 2D frame). This definition however as they suggest would be too broad, if it does not address the question of whether the 2D frame actually matters. Rather spatial analysis is",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "href": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "the subset of analytic techniques whose results will change if the frame changes, or if objects are repositioned within it.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#links-with-theory",
    "href": "012_spatial_analysis.html#links-with-theory",
    "title": "2  Spatial data analysis: a definition",
    "section": "2.2 Links with theory",
    "text": "2.2 Links with theory\nScience progresses with tools and techniques but also by testing hypotheses and updating models and theory. How spatial analysis is linked to theory also depends on fields or sub-fields.\nFrom a quantitative geography viewpoint (adapted from Denise Pumain https://hypergeo.eu/theories-of-spatial-analysis/?lang=en), spatial analysis focuses on uncovering spatial structures and organizations. These structures can often be generalized into models, such as center-periphery relationships, gravity models, and urban hierarchies and networks.\nThe ultimate goal of spatial analysis is then to understand the processes that lead to the formation of these spatial structures.\nFrom a spatial economic or regional science viewpoint (as understood by a European quantitative geographer) spatial analysis consists of a set of techniques designed to:\n\nDescribe the location of activities and how they change over time\nEstimate reduced form models\n\nUnlike structural form models, which are direct representations or formulations of theoretical concepts, reduced form models are designed to better align with and fit the data.\nThere is probably no such a reduced or structural form model in quantitative geography, but in both case anyway, spatial analysis ultimately aims at testing and updating theories.\nWe very much agree with this perspective here, leading to giving more importance to the falsification of ideas and the interpretation of estimated coefficients than to prediction using as many data as possible.\nIf a variable is used it is because we have some idea of its importance and influence on others or its relevance, not just to obtain a fit. Hence we won’t use automatic models constructions (no stepwise regression for example) and leave out all the methods (neural networks, random forests, etc.) from which coefficients (if any) are difficult to interpret, even if these methods can be considered to belong to spatial analysis and use geographic data. This course is not about data mining or data crunching. We use a statistical lens to examine variations across space and how spatial relationships influence socio-economic patterns and behaviors or environmental effects.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html",
    "href": "013_space_axiom.html",
    "title": "3  Geographical space",
    "section": "",
    "text": "3.1 Absolute or pre-geographical space:",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "href": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "title": "3  Geographical space",
    "section": "",
    "text": "A set of places or locations \\(S\\)\nIdentified by their coordinates \\(x,y\\)\nSeparated by a distance \\(d(L)\\)\nDistance being measured along a given metric \\(L\\)",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#geographical-space",
    "href": "013_space_axiom.html#geographical-space",
    "title": "3  Geographical space",
    "section": "3.2 Geographical space:",
    "text": "3.2 Geographical space:\nS can be endowed with various attributes to form a geographical space:\n\nThe surface attribute \\(m\\), measured along a given metric related to coordinates\nVarious attributes \\(Z\\)\nDensity measures, i.e. any \\(Z/m\\)\n\n\n\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An Axiomatic Approach to Geographical Space.” Geographical Analysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html",
    "href": "014_space_issues.html",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "4.1 Equivalence and Independence\n(Based on discussions in Jayet, p. 2-13)\nStatistical analysis is based on two key principles, or invariants:\nHowever, both of these principles are challenged when applied in a spatial context. Spatial data often exhibit dependencies due to geographic proximity, which violates the assumption of independence. Similarly, the notion of statistical equivalence becomes problematic as spatial heterogeneity introduces variability across observations in different locations and because the spatial definition of objects may vary and their sampling irregular.\nThese challenges highlight the need for specialized approaches in spatial analysis, where standard statistical methods must be adapted to account for the structure and dependencies present in the spatial data.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence-and-independence",
    "href": "014_space_issues.html#equivalence-and-independence",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "All observations must be statistically equivalent: This means that no individual observation should be systematically different from others in the sample set. Each data point must have the same probability distribution, ensuring uniformity and comparability.\nAll observations must be independent from each other: In any statistical model, the assumption is that the occurrence of one observation does not influence or depend on another. Independence ensures the integrity of statistical results.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence",
    "href": "014_space_issues.html#equivalence",
    "title": "4  Spatial data issues",
    "section": "4.2 Equivalence",
    "text": "4.2 Equivalence\n\n4.2.1 Irregularity of observations and the nature of data\nWhile there are time cycles, making repetitive data logging along the time dimension doable, there is no such think as a spatial cycle for geographical data recording.\nMost spatial data has a irregular covering in space, which already challenges the equivalence of observations\n\n(source to be added, apologies if you are the author, I am happy to adapt)\n\nObservations (countries) are of different size, i.e. the surface attribute \\(m\\) matters here.\nSuppose \\(Z_{pop}\\) is the country population. One can expect \\(Z_{pop}\\) to relate to \\(m\\) if processes are homogeneous across space. However a \\(Z/m\\) density variables would still show these objects are very different.\nYet, other \\(Z_i\\) variables could still be compared using that \\(Z_{pop}\\) attributes. For example the active population of the place (country) as a percentage of its total population, or using other transformations (linear or not).\nNote that variations in volume/mass/size such as \\(Z_{pop}\\) are very common, with very few objects having a very large size compared to most others. Such a size effect\n\nimpacts on the total and central (mean, median) value of variables\nthe distribution of values when made in different observations’ regions\ntypically leads to outliers problems or heteroskedasticity (non constant variance)\n\nHowever, there are raster maps and some information is “regular”, such a precipitation, or can be “regularized”, such as population grids.\n\n(https://human-settlement.emergency.copernicus.eu/)\nThe discretization of geographic space should however be internally homogeneous, meaning the attributes within each grid cell (or other nwe objects) supposed to apply to every part of that cell.\nA difference is often made between continuous field data and discrete space objects.\nSee below the tabulation of these against the types of measurements by Haining (2010)\n\n\n\n4.2.2 Modifiable Areal unit Problem - MAUP\nOpenshaw (1984)\n\nAreal units = spatial objects such as zones or places or towns or regions\n\n\nGeography has consistently and dismally failed to tackle its entitation problems, and in that more than anything else lies the root of so many of its problems (Chapman 77)\n\n\nInsufficient thought is given to precisely what it is that is being studied. […] Little concern has been expressed about the nature and definition of the spatial objects under study\n\n\nFor many purposes the zones in a zoning system constitute the objects, or geographical individuals, that are the basic units for the observation and measurement of spatial phenomena.\n\n\nWith areal data, the spatial objects only exist after data collected for one set of entities (e.g. people) are subjected to an arbitrary aggregation (see also regularity discussion above) to produce a set of spatial units.\n\n\nHowever, there are no rules for areal aggregation, no standards, and no international conventions to guide the spatial aggregation process.\n\n\nThe areal units (zonal objects) used in many geographical studies are arbitrary, modifiable. Census areas have rarely an intrinsic geographical meaning\n\n\nA unmanageable combinatorial problem: There are approximately 10^12 different aggregations of 1,000 objects into 20 groups. If the aggregation process is constrained so that the groups consist of internally contiguous objects (i.e. all the objects assigned to the same group are geographical neighbours) then this huge number is reduced, but only by a few orders of magnitude.\n\nStan Openshaw distingues 2 interrelated issues, within the MAUP:\n\nThe scale problem: the variation in results that can often be obtained when data for one set of areal units are progressively aggregated into fewer and larger units for analysis.\nThe aggregation problem: the problem of alternative combinations of areal units at equal or similar scales. Any variation in results due to the use of alternative units of analysis when the number of units is held constant\n\n\nExample of effects on correlation coefficients:\n\nCorrelation between percentage vote for Republican candidates in the congressional election of 1968 and the percentage of the population over 60 years\nCorrelation at the 99 county level is 0.34\nAfter aggregation into six zones: 0.26 for the 6 congressional districts and 0.86 for a simple typology of Iowa into 6 rural-urban types (Openshaw and Taylor 77)\nCompare mean and dispersion of correlation coefficient after random zoning (using contiguity) and random sampling (grouping)\n\n\n\nNo systematic scale effect on correlation mean\nConsiderable variability about the mean values but reduces with increasing numbers of units\nThe standard deviations of the zoning distributions are considerably smaller than the corresponding sampling distributions but exhibit a greater degree of bias &gt; spatial autocorrelation effect",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#spatial-in-dependence",
    "href": "014_space_issues.html#spatial-in-dependence",
    "title": "4  Spatial data issues",
    "section": "4.3 Spatial (in-)dependence",
    "text": "4.3 Spatial (in-)dependence\n\n4.3.1 Interactions between observations\n\nNot only the dimensions and structures of observations is of importance but also their relative position in space\nThe distance (between objects), \\(d(L)\\), is at the very heart of geographical analysis AND the source of statistical difficulties\nThe level of interactions increases with proximity (distance functions or contiguities) (see gravity-based theories)\n\n\n\n4.3.2 Tobler’s first law of geography\nTobler (1970)\n\n\n\n4.3.3 Spatial autocorrelation\nAnselin, Luc and Bera,I (1998)\n\n\n\n\n\n\nAnselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear Regression Models with an Introduction to Spatial Econometrics: Regression Models with an Anselin Bera i. INTRODUCTION.” In. CRC Press.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced Data.” In, edited by Manfred M. Fischer and Arthur Getis, 197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem. Concepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth in the Detroit Region in Economic Geography 46: 2, 234240.” Clark University, Worcester, MA.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html",
    "href": "021_Rstudio.html",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "5.1 R, RStudio and its interface\nIn class demonstration of how to",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#r-rstudio-and-its-interface",
    "href": "021_Rstudio.html#r-rstudio-and-its-interface",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "Get R and RStudio installed\nNavigating the R studio interface\nScripting area, console, files,…\nUsing colors and TOC outline in RStudio\nAuto-completion using tabs\nNavigating history with the up/down arrows\nUnderstanding help (necessary arguments and default options)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#projects-and-workflows",
    "href": "021_Rstudio.html#projects-and-workflows",
    "title": "5  Getting started with RStudio",
    "section": "5.2 Projects and workflows",
    "text": "5.2 Projects and workflows\nLet’s compute a value in the console and store it to an object first\n\n#This is a comment\n1+2 #This is also a comment\n#&gt; [1] 3\na&lt;-3+4\na\n#&gt; [1] 7\n\nSee that we now have an object in the environment!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#objects",
    "href": "021_Rstudio.html#objects",
    "title": "5  Getting started with RStudio",
    "section": "5.3 Objects",
    "text": "5.3 Objects\nAn object is not defined ex-ante and is automatically overwritten\n\nX&lt;-3 #This X will soon be replaced\nX&lt;-1:10 \nY&lt;-X^2\n\nR is case sensitive. The following returns an Error\n\nx # x is lower case and does not exist\n\nIMPORTANT:\n\nAlways worry about Errors. They stop your process.\nAlways read and try to understand Warnings. They do not stop your process but usually indicate the result may not be as expected!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#working-directory",
    "href": "021_Rstudio.html#working-directory",
    "title": "5  Getting started with RStudio",
    "section": "5.4 Working directory!!!",
    "text": "5.4 Working directory!!!\nSuppose we want to produce a text from the above and save it to a file:\n\na_sentence&lt;-paste(\"I have computed a sum, which equals\", a)\na_sentence #Let's see this in the console\n#&gt; [1] \"I have computed a sum, which equals 7\"\ncat(a_sentence,file=\"brol/a_sentence.txt\")\n\nWhere is the file? the directory? Have you been able to run this?\nAlways indicate where you work!\nThe classical way is\n\ngetwd() #get (default) working directory\nsetwd(\"/Users/geoffrey.caruso/Dropbox/GitHub/MAGEO0641/brol\") #set working directory to YOUR OWN NETWORK SPACE HERE!\n\nThere is a more practical way in RStudio: Make an .Rproj from/to a directory\nYou can create a directory in your finder/file explorer, or create a directory or subdirectory from within the R console. This will be very useful at a more advanced level when you create many outputs and directory names result from some data processing. Think of processing something across many countries/cities.\n\ndir.create(\"brol\")\ndir.create(\"Today\")\n\n#and for removing a directory\nunlink(\"Today\", recursive = TRUE) #see help: If recursive = FALSE directories are not deleted, not even empty ones.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "href": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "title": "5  Getting started with RStudio",
    "section": "5.5 Commented R scripts vs markdown documents",
    "text": "5.5 Commented R scripts vs markdown documents\nThe above is a commented script, using #, which you can save as an .R file and re-run later.\nThe problem with this approach is that you won’t see results of the codes until you run it. So you can’t really comment your output (although many, and I, would still do it) thus mixing explanation of what is done and interpretation.\nA more advanced approach is to make a document where you integrate text, code chunks and results of the code.The text can thus document what is going to be done and the results, while the code chunks can thus document both the code itself and its result as it is processed by the Console.\nLet’s have a look at the structure of this syllabus, written using Quarto markdown.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "href": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "title": "5  Getting started with RStudio",
    "section": "5.6 Good practice with files handling and objects naming",
    "text": "5.6 Good practice with files handling and objects naming\nRproj in an excellent way to keep things at the same place.\nWhen using an Rproj, use relative path only, i.e. from the root folder of the project, not from your machine. This is the way you can easily transfer your project to friends. Only external data (e.g. from the web) should be referenced in full.\nIt is also good practice to have a specific “data” folder or subfolders for all data so you clearly differentiate your outputs with the inputs. Similarly a R folder with your scripts when you have many.\nAlthough there is no naming convention agreed by everyone, it is important to apply a consistent style for yourself and colleagues. Also you would avoid spaces and rarer characters, especially in a multilingual environment.\nA folder of file named “source data” or “data_für_rémi” are not great ideas. (This is also true for variable names in data frame, see later)\nIn general, I personally like files to be all lowercase with underscores and using action verbs to explain what is done in the R file, such as\n\nestimate_model.R\nget_statec_data.R\n\nFiles numbering can be of good help for heavier projects where there is a logical sequence (time)\n\n01_estimate_model.R\n02_get_statec_data.R\n\nFor variable names and objects I tend to use the “UpperCamelCase” form especially for vectors\n\nLuxCities\nGrowthRates\n\nand tend to add an “df” or “_lst” to disambiguate where needs be between some classes\n\nLuxCities_df\nLuxCities_lst\n\nI also like to use lowercase single letters for input parameters, such as a pvalue or number of neighbours, e.g.\n\np&lt;-0.05\nk&lt;-2\n\nor Greec symbols written in full, especially when there is a theoretical link\n\nbeta &lt;-model$coefficient[1]\nrho &lt;- 0.5\n\nFor functions (see later) I also like to use action verbs and include dots, such as:\n\nplot.bmi()\nextract.boundary()\n\nIn all case, be concise but specific and consistent within a project or even across.\n\nNeighboursCompute_Europe.Paris_project3 would be very long and inconsistent\ndf_new2bis not to use both as an object or a file\n\n…but to to be honest I have quite a number of tests.r and plot.test2.jpg files peppered in my machine.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "022_vectors.html",
    "href": "022_vectors.html",
    "title": "6  Vectors",
    "section": "",
    "text": "6.1 Introducing vectors\nVectors are the basic units of information in R, and many functions apply to vectors. If you are coming from the “spreadsheet” world (MS Excel or Open Office), where the basic unit is a cell, the change in perspective is quite important: while a cell has a single value (information), a vector contains multiple values.\nVectors, being a combination of values, are created by the combine (or concatenate) function c() or obtained from external sources.\nThere are two kinds of vectors:\nThis chapter is about atomic vectors, we will introduce lists later.\nAs a first example, see below a character (atomic) vector and a numeric (atomic) vector:\nCountries&lt;-c(\"Romania\",\"Russia\",\"Morocco\",\"Iran\",\"France\")\nmode(Countries)\n#&gt; [1] \"character\"\nAges&lt;-c(20,25,22,22,49)\nmode(Ages)\n#&gt; [1] \"numeric\"\nNote that in case you would have a long list to input manually, the scan() function is a little more interactive. Try! (I also have heard there are ways to copy-paste from external sources…(if you fancy less transparent clicking approaches, be curious and find out, for example here: Chapter 14 ;-), most of the times anyway we rather read values from readable text files).",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#introducing-vectors",
    "href": "022_vectors.html#introducing-vectors",
    "title": "6  Vectors",
    "section": "",
    "text": "Atomic vectors (which we tend to refer to simply as vectors), which are homogeneous in the sense that they contain only one “type” of data, such as characters or numbers.\nLists, which can have heterogeneous contents.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#attributes-of-vectors",
    "href": "022_vectors.html#attributes-of-vectors",
    "title": "6  Vectors",
    "section": "6.2 Attributes of vectors",
    "text": "6.2 Attributes of vectors\nA vector is always characterized by its type and length.\n\n6.2.1 Types\nA vector is a combination of values from the same type (or mode). So if you combine data from different types they will be coerced to the less demanding one, i.e. “character” in the following cases:\n\nA&lt;-c(1,\"two\",3)\nA\n#&gt; [1] \"1\"   \"two\" \"3\"\nmode(A)\n#&gt; [1] \"character\"\n\nc(Countries, Ages)\n#&gt;  [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\"  \"20\"      \"25\"     \n#&gt;  [8] \"22\"      \"22\"      \"49\"\nmode(c(Countries, Ages))\n#&gt; [1] \"character\"\n\nYou can also see from these examples that mode()` gets a vector’s “type”. R vectors have one and only one “mode”, either “numeric”, “character” or “logical” (plus “raw” and “complex”, which we don’t consider here).\nYou can also use typeof() in case you are interested to know how the data is actually encrypted, which essentially differentiates the “numeric” mode into “integer” and “double”. R vectors have one and only one “typof”, either “integer”, “double”, “character” or “logical”\nCoercion to the same type applies to the “typeof” as you can see below\n\nBNum&lt;-c(2,10,99)\ntypeof(BNum)\n#&gt; [1] \"double\"\nmode(BNum)\n#&gt; [1] \"numeric\"\nBInt&lt;-c(2L,10L,99L)\ntypeof(BInt)\n#&gt; [1] \"integer\"\nmode(BInt)\n#&gt; [1] \"numeric\"\nB&lt;-c(BNum,BInt)\ntypeof(B)\n#&gt; [1] \"double\"\nmode(B)\n#&gt; [1] \"numeric\"\n\nIn practice, you will rarely use mode() or typeof(), which distinguish well between vectors, but not between most other objects. Instead, you will use the class() function in order to know what kind of data you have and what you can do with it. For most atomic vectors, class() is basically typeof(). The difference is that “class” is not a mutually exclusive property. A vector, or any object, can belong to several classes and thus be used with different functions\n\nclass(c(20L,50L,70L))\n#&gt; [1] \"integer\"\nclass(c(20,50,70))\n#&gt; [1] \"numeric\"\nclass(Countries)\n#&gt; [1] \"character\"\nclass(Ages)\n#&gt; [1] \"numeric\"\nclass(c(Countries,Ages)) #coercion\n#&gt; [1] \"character\"\n\nSometimes, depending on some calculations, you may need to transform the type of data, especially from numeric to character and vice versa. Coercion may apply automatically but not always, so you will need to explicitly transform the data type using “as.a type” or”as.a mode”: as.numeric(), as.character(), which you will use quite often, or as.integer(),as.double(), or as.logical().\n\nA&lt;-c(1,2,3)\nB&lt;-c(1L,2L,3L)\nC&lt;-c(\"1\",\"2\",\"3\")\nA+B\n#&gt; [1] 2 4 6\nA+C\n#&gt; Error in A + C: non-numeric argument to binary operator\nA+as.numeric(C)\n#&gt; [1] 2 4 6\nas.character(A)\n#&gt; [1] \"1\" \"2\" \"3\"\n\n\n\n6.2.2 Length\nWhile a numeric vector of length one is mathematically a scalar, the way you input a scalar in R is as a vector of length 1. A vector can also be empty, i.e. of length 0, in which case its type is unknown, unless you apply one of the above transformations.\n\nX&lt;-3 #equivalent to X&lt;-c(3)\nclass(X)\n#&gt; [1] \"numeric\"\n\nY&lt;-c()\nclass(Y)\n#&gt; [1] \"NULL\"\n\nY&lt;-as.numeric(c())\nclass(Y)\n#&gt; [1] \"numeric\"\n\nY&lt;-as.integer(c())\nclass(Y)\n#&gt; [1] \"integer\"\n\nYou get the length of a vector using the function length(), which you will also use quite a lot.`\n\nlength(Countries)\n#&gt; [1] 5\nlength(X)\n#&gt; [1] 1\nlength(Y)\n#&gt; [1] 0\n\nNote that if you manually change the length of an existing vector, this will trim the vector end or extend the vector with empty values.\n\nZ&lt;-c(2, 4, 6, 7 , 10)\nlength(Z)&lt;-3\nZ\n#&gt; [1] 2 4 6\nlength(Z)&lt;-12\nZ\n#&gt;  [1]  2  4  6 NA NA NA NA NA NA NA NA NA\n\nBesides their class() and length() attributes, vectors (and other objects) can be endowed with a number of other attributes, which you will obtain from attributes(). None of the vectors we used as example so far has additional attributes, but you can define any attribute yourself. See below how we add a “source” attribute to the vector of “Countries” we created earlier and assign a “character string” to it to describe the source. Retrieving a specific attribute is done with the attr(,\"Whatever attribute\") function.\n\nattributes(Countries)\n#&gt; NULL\nattr(Countries, \"source\")&lt;-\"MAGEO students input\"\nattributes(Countries)\n#&gt; $source\n#&gt; [1] \"MAGEO students input\"",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sec-factors",
    "href": "022_vectors.html#sec-factors",
    "title": "6  Vectors",
    "section": "6.3 Factors",
    "text": "6.3 Factors\nRemember from the introduction, that we (in geography and elsewhere) usually consider discrete and continuous data, each type being split into 2 measurement levels:\n\nDiscrete: nominal (blue, green) or ordinal (high, low)\nContinuous: interval (10°C, 20°C) or ratio (10 apples, 20 apples) (difference being that 20 apples is twice as much as much as 10 apples, but 20°C is not twice as hot as 10°C because zero is not the absolute zero, hence a ratio in this case makes little sense, only differences are sensical).\n\nThis vocabulary is not used directly in R:\n\nContinuous (ratio/interval) data is coded as numeric or integer\nDiscrete (categorical) data (nominal/ordinal) is preferably coded as a factor.\n\nNominal data can still be in the “character” type, but the idea of a “factor” is that there is only a limited set of characters’ strings that you will find in a vector (e.g. set of countries, set of land uses) and you are able (willing) to enumerate them. For ordinal data, the order is important, hence it gets closer to an integer set than to a character (e.g. education levels: primary, secondary tertiary education), yet you can sum integer data but should not sum ordinal data.\n\nFactors are designed to properly solve the use of discrete/categorical data. It is based on the integer type (in the sense of typeof) on top of which a “level” attribute is added, and potentially whether it is ordered or not. They are fabricated with the as.factor() or factor() functions.\n\nCountries\n#&gt; [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\" \n#&gt; attr(,\"source\")\n#&gt; [1] \"MAGEO students input\"\ntypeof(Countries)\n#&gt; [1] \"character\"\n\nCountries_f&lt;-as.factor(Countries)\nCountries_f\n#&gt; [1] Romania Russia  Morocco Iran    France \n#&gt; Levels: France Iran Morocco Romania Russia\ntypeof(Countries_f)\n#&gt; [1] \"integer\"\nclass(Countries_f)\n#&gt; [1] \"factor\"\n\nWhile the categories are displayed with the factor, the levels() function returns those categories as a character vector.\n\nlevels(Countries_f)\n#&gt; [1] \"France\"  \"Iran\"    \"Morocco\" \"Romania\" \"Russia\"\n\nBy default the levels in a factor uses the alphabetical order. In many cases however, you will want to at least define the first one, in order to use it as a reference (typically in regression analysis with a categorical explanatory variable) or choose you own order for plotting or other purposes.\n\n6.3.1 Re-defining the reference level\nSuppose you want to use “Morocco” as the first, reference level instead of “France”.\nThe relevel() function re-orders the levels so that the one indicated as “ref” is used first and the others are moved down the series.\n\nCountries_f2&lt;-relevel(Countries_f, ref=\"Morocco\")\n\nYou could also set the order directly at the time of creating the factor using the “level” argument, using the factor() function, not as.factor(). We used as.factor() before because it is generally quicker, and there is not always a need to adapt the order of levels.\n\nCountries_f3&lt;-factor(Countries, level=c(\"Morocco\",\"France\",\"Romania\", \"Iran\",\"Belgium\"))\n\nBe careful, however, because if you don’t use the complete list of possibilities, the values that are not specified in the levels vector, will simply be ignored and turned into NA’s. In the above example we forgot “Russia” and therefore have now a NA within our vector. Conversely, we have been able to indicate a “Belgium” level, although it was not present. There is no automatic correspondence between the levels of a factor and its set of values. If you want a match, you can drop unused levels using droplevels() but for those characters (e.g. Russia) that were not taken at the moment of creating the factor, it is too late, they remain a NA.\n\nCountries_f4&lt;-droplevels(Countries_f3)\nCountries_f4\n#&gt; [1] Romania &lt;NA&gt;    Morocco Iran    France \n#&gt; Levels: Morocco France Romania Iran\n\nSee how the different factors we made so far change the order of the levels and the data when a case is made absent from the list:\nTo compare one to one, we use to column-binding function cbind() (compare with c()). You also see here that the values of the factors are integers, not characters as for the first vector:\n\ncbind(Countries,Countries_f,Countries_f2,Countries_f3, Countries_f4)\n#&gt;      Countries Countries_f Countries_f2 Countries_f3 Countries_f4\n#&gt; [1,] \"Romania\" \"4\"         \"4\"          \"3\"          \"3\"         \n#&gt; [2,] \"Russia\"  \"5\"         \"5\"          NA           NA          \n#&gt; [3,] \"Morocco\" \"3\"         \"1\"          \"1\"          \"1\"         \n#&gt; [4,] \"Iran\"    \"2\"         \"3\"          \"4\"          \"4\"         \n#&gt; [5,] \"France\"  \"1\"         \"2\"          \"2\"          \"2\"\n\n\n\n6.3.2 Ordering a factor based on occurrence:\nTo make a more realistic case, we have scraped the Wikipedia table indicating the Tour de France winners since 1903. See “TourDeFrance” in the data folder for the data and scraping script (from R). The data is in the form of a data.frame and saved as a RDS (an effective way to save R objects onto your disc). See the relevant chapters for data.frame and saving to file later.\nThere was no Tour de France during World War I and II and no winner from 1999 to 2005 because Lance Armstrong cheated. Hence we have a series of ” - ” in our levels, which are not interesting, and thus not considered when we factor our vector. This doesn’t remove the vector elements but creates several NA’s.\n\nLeTour_df&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nWinners&lt;-factor(LeTour_df$Country, exclude = \"—\")\nWinners\n#&gt;   [1] France        France        France        France        France       \n#&gt;   [6] France        Luxembourg    France        France        Belgium      \n#&gt;  [11] Belgium       Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [16] &lt;NA&gt;          Belgium       Belgium       Belgium       Belgium      \n#&gt;  [21] France        Italy         Italy         Belgium       Luxembourg   \n#&gt;  [26] Luxembourg    Belgium       France        France        France       \n#&gt;  [31] France        France        Belgium       Belgium       France       \n#&gt;  [36] Italy         Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [41] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          France       \n#&gt;  [46] Italy         Italy         Switzerland   Switzerland   Italy        \n#&gt;  [51] France        France        France        France        France       \n#&gt;  [56] Luxembourg    Spain         Italy         France        France       \n#&gt;  [61] France        France        Italy         France        France       \n#&gt;  [66] Netherlands   Belgium       Belgium       Belgium       Belgium      \n#&gt;  [71] Spain         Belgium       France        Belgium       France       \n#&gt;  [76] France        France        Netherlands   France        France       \n#&gt;  [81] France        France        France        United States Ireland      \n#&gt;  [86] Spain         United States United States Spain         Spain        \n#&gt;  [91] Spain         Spain         Spain         Denmark       Germany      \n#&gt;  [96] Italy         &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt; [101] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          Spain         Spain        \n#&gt; [106] Spain         Spain         Luxembourg    Australia     Great Britain\n#&gt; [111] Great Britain Italy         Great Britain Great Britain Great Britain\n#&gt; [116] Great Britain Colombia      Slovenia      Slovenia      Denmark      \n#&gt; [121] Denmark       Slovenia     \n#&gt; 15 Levels: Australia Belgium Colombia Denmark France Germany ... United States\n\nlength(Winners) #this includes NA's!\n#&gt; [1] 122\nlevels(Winners)\n#&gt;  [1] \"Australia\"     \"Belgium\"       \"Colombia\"      \"Denmark\"      \n#&gt;  [5] \"France\"        \"Germany\"       \"Great Britain\" \"Ireland\"      \n#&gt;  [9] \"Italy\"         \"Luxembourg\"    \"Netherlands\"   \"Slovenia\"     \n#&gt; [13] \"Spain\"         \"Switzerland\"   \"United States\"\n\nSuppose you want to to know and plot how many times each of the 15 countries (who won Le Tour at least once) won.\nFrequencies can be observed directly from a basic plot:\n\nplot(Winners,las=2)\n\n\n\n\n\n\n\n\nGet the victories counts in a vector can be made with table(), which applies to anything (numeric or character) that can be coerced to a factor\n\ntable(Winners)\n#&gt; Winners\n#&gt;     Australia       Belgium      Colombia       Denmark        France \n#&gt;             1            18             1             3            36 \n#&gt;       Germany Great Britain       Ireland         Italy    Luxembourg \n#&gt;             1             6             1            10             5 \n#&gt;   Netherlands      Slovenia         Spain   Switzerland United States \n#&gt;             2             3            12             2             3\n\nBoth the plot and counts however follow the order of the levels, which is alphabetical\nOne possibility to improve the graph is to change the order of the levels based on the count table and redoing the graph\n\n#Vector of levels in a new order\nLevelsFrq&lt;-levels(Winners)[order(table(Winners), decreasing = TRUE)]\n  #note the order function and the square brackets\nLevelsFrq\n#&gt;  [1] \"France\"        \"Belgium\"       \"Spain\"         \"Italy\"        \n#&gt;  [5] \"Great Britain\" \"Luxembourg\"    \"Denmark\"       \"Slovenia\"     \n#&gt;  [9] \"United States\" \"Netherlands\"   \"Switzerland\"   \"Australia\"    \n#&gt; [13] \"Colombia\"      \"Germany\"       \"Ireland\"\n\n#Use that order when creating the factor from the character vector\nWinnersFrq&lt;-factor(Winners, levels = LevelsFrq) \n\n#plot\nplot(WinnersFrq,las=2)\n\n\n\n\n\n\n\n\n\n\n6.3.3 Ordered factors\nSometimes you will want to make a categorical data explicitly ordinal for graphical or statistical purpose . For example if you have used a Likert scale within a survey, it can be interesting it is stored as an ordered factor. You can order an already existing factor, using the function ordered(), which adds a logical flag to the factor to indicate it is order (see first example), or at the moment you create the factor using factor() (second example). Once the factor is ordered, the levels are displayed in order and separated by a ” &lt; “. Also a new class,”ordered”, is added to the object.\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") #in the expected order\nResponses&lt;-c(\"Neither agree nor disagree\", \"Disagree\", \"Disagree\", \"Agree\", \"Agree\", \"Neither agree nor disagree\", \"Strongly agree\", \"Strongly Agree\", \"Disagree\")\nResponses_f&lt;-factor(Responses, levels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree Disagree Neither agree nor disagree ... Strongly agree\n\nResponses_f2&lt;-ordered(Responses_f)\nResponses_f2 #Notice that if we don't provide again the levels, it is made from the existing ones, so there is only 4 levels now (\"Strongly Disagree\" not being answered)\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; Levels: Disagree &lt; Neither agree nor disagree &lt; Agree &lt; Strongly agree\n\nResponses_f3&lt;-factor(Responses, levels=LikertScale, ordered = TRUE)\nResponses_f3\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nclass(Responses_f3) #see the additional class\n#&gt; [1] \"ordered\" \"factor\"\n\nIn the above example you will have noticed a NA, due to some misspelling. This is a good case to remind that categorical data should not necessarily rely on character strings. In geography we often need to treat countries, regions, municipalities whose names can be very complicated, especially in multilingual context. It is good advise to rather use codes for geographical units (e.g. BE351, BE352, BE353, for the NUTS 3 classification of Belgium), land use (e.g. https://land.copernicus.eu/content/corine-land-cover-nomenclature-guidelines/html) and relate them to a corresponding table with proper labels.\nIt is also true for other types of categories, including Likert scale. An option is to use integer values (or characters for geographical codes) together with a series of labels. You add the labels at the moment of creating the factor:\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") \nLikertLevels&lt;-c(1L,2L,3L,4L,5L) #would work as well with numeric c(1,2,3,4,5)\nResponses&lt;-c(3L, 2L, 2L, 4L, 4L, 3L, 5L, 5L, 2L)\n\nResponses_f&lt;-factor(Responses, ordered=TRUE, levels=LikertLevels, labels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             Strongly agree            \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nThe result is the same as previously, but you avoid typos when inputing the responses and can also adapt the labels at will, without changing the data itself.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#operations-on-vectors",
    "href": "022_vectors.html#operations-on-vectors",
    "title": "6  Vectors",
    "section": "6.4 Operations on vectors",
    "text": "6.4 Operations on vectors\n\n6.4.1 Arithmetic operations and recycling\nR is a calculator and perform the arithmetic operations + - * / ^\nTwo numeric vectors of the same length can be added, multiplied, etc. When vectors of different length are provided, the shorter one is recycled until the end of the longer vector. Usually a vector of length 1 is recycled, thus allowing R to add, divide,etc. a scalar to a vector in the same way as between two vectors (in a “parallel way”). But any shorter vector is also recycled (you get a warning though). See:\n\nX&lt;-c(0,1,2,3,4,5)\nX^2\n#&gt; [1]  0  1  4  9 16 25\nY&lt;-c(10,9,8,7,6,5)\n\nX+Y\n#&gt; [1] 10 10 10 10 10 10\nY/X\n#&gt; [1]      Inf 9.000000 4.000000 2.333333 1.500000 1.000000\nX*c(0,1)\n#&gt; [1] 0 1 0 3 0 5\n\nYou can also apply a series of common mathematical functions to any numeric vector.\n\na&lt;-c(-2,-1,0,1,2,6,10)\nabs(a) #absolute\n#&gt; [1]  2  1  0  1  2  6 10\n\na^(-1) #inverse\n#&gt; [1] -0.5000000 -1.0000000        Inf  1.0000000  0.5000000  0.1666667  0.1000000\n\nlog(a) #ln or Natural logarithm (or Napierian logarithm)\n#&gt; Warning in log(a): NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.6931472 1.7917595 2.3025851\nlog10(a) # base 10 logarithm\n#&gt; Warning: NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.3010300 0.7781513 1.0000000\n\nexp(a) #base e exponential\n#&gt; [1] 1.353353e-01 3.678794e-01 1.000000e+00 2.718282e+00 7.389056e+00\n#&gt; [6] 4.034288e+02 2.202647e+04\n10^a #base 10 exponential\n#&gt; [1] 1e-02 1e-01 1e+00 1e+01 1e+02 1e+06 1e+10\n\nsqrt(a) #square root\n#&gt; Warning in sqrt(a): NaNs produced\n#&gt; [1]      NaN      NaN 0.000000 1.000000 1.414214 2.449490 3.162278\n\nSee Crawley (2012) p.11 for more examples:\n\n\n\nCrawley Table 2.1\n\n\nBe careful that these functions must lead to proper results. As you can see from the warnings. “NaN” stands for “Not a Number” and is considered as NA (“not available”). Which is not the case of infinity, which is a numeric\n\na&lt;-c(-2,-1,0,1,2,6,10)\nis.na(NaN)\n#&gt; [1] TRUE\nis.na(-Inf)\n#&gt; [1] FALSE\nis.finite(log10(a))\n#&gt; Warning: NaNs produced\n#&gt; [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\n6.4.2 Euclidean division and rounding\nThe modulo operation for integers is obtained with %/% to get the integral part of the Euclidean division and %% to get the remainder. This the way you will know an integer is odd or even. The function can also be applied to numeric vectors with decimals.\n\nc(1,3,5,44,444,4444)%/%2\n#&gt; [1]    0    1    2   22  222 2222\nc(1,3,5,44,444,4444)%%2 #remainder of dividing by 2, thus indicating odd/even\n#&gt; [1] 1 1 1 0 0 0\n99.99%/%2\n#&gt; [1] 49\n99.99%%2\n#&gt; [1] 1.99\n\nFor ratio and interval numbers with decimals you sometimes need to get rid of the decimals or display only a part of them Examine the following:\n\nx&lt;-c(33.33, 666.166, 50.5, 49.5)\ntrunc(x) #compares with as.integer() but does not change the class to integer\n#&gt; [1]  33 666  50  49\nround(x) #note the rounding of a 5 to the even digit (international standard) \n#&gt; [1]  33 666  50  50\nround(x, digits=2)\n#&gt; [1]  33.33 666.17  50.50  49.50\nceiling(x)\n#&gt; [1]  34 667  51  50\nfloor(x)\n#&gt; [1]  33 666  50  49\n\nsignif(x, digits = 5)\n#&gt; [1]  33.33 666.17  50.50  49.50\n\nWith geographical data, ceiling() can for example be applied to latitudes and longitudes in order to make grids. A simple example is how to obtain (theoretical) time zones from a set of longitudes.\nLet’s take the opportunity to learn about set.seed() and about uniform random number generation runif():\n\nset.seed(101)\nLongitudes&lt;-runif(n=10,min=-180, max=180)\nLongitudes\n#&gt;  [1]  -46.00858 -164.22307   75.48625   56.76854  -90.05194  -71.98026\n#&gt;  [7]   30.55199  -59.95183   43.92431   16.49828\n\nceiling(Longitudes*24/360)\n#&gt;  [1]  -3 -10   6   4  -6  -4   3  -3   3   2\n\n\n\n\nWikipedia, Time Zones\n\n\n\n\n6.4.3 Logical and Boolean operations\nA series of operations return a logical vector, i.e. a vector of Boolean values TRUE and FALSE.\nThose operations are &gt;, &gt;=, &lt;, &lt;=, ==, !=\nSome examples with numeric and character vectors:\nIMPORTANT: Use == for comparison, not =, which is an assignment!\n\nc(1,2,3) &lt; c(2,3,3)\n#&gt; [1]  TRUE  TRUE FALSE\nc(1,2,3) &gt;= 3 #see right hand side is recycled and applied \"one to one\"\n#&gt; [1] FALSE FALSE  TRUE\n\"Bernadette, elle est très chouette\" == \"Bernadette, elle est très chouette\"\n#&gt; [1] TRUE\n\"Bernadette, elle est très chouette\" != \"Mais sa cousine, elle est divine\"\n#&gt; [1] TRUE\n\"Julian\" &gt;= \"Julien\" #alphabetical order\n#&gt; [1] FALSE\n\nBy the way we see here vectors of the logical class:\n\nclass(\"Julian\" &gt;= \"Julien\")\n#&gt; [1] \"logical\"\n\nIn addition to those comparisons, many functions, especially structured as “is.xxx” return a TRUE or FALSE and are very useful within you data management process. You may make quite some use also of the %in% function, when you have a long vector, to check whether a particular value (or several) is present within a vector.\n\nis.numeric(\"a\")\n#&gt; [1] FALSE\nis.numeric(\"2\")\n#&gt; [1] FALSE\nis.numeric(3L)\n#&gt; [1] TRUE\nis.numeric(FALSE)\n#&gt; [1] FALSE\n!is.numeric(FALSE) #Note the negation here!\n#&gt; [1] TRUE\n\n\"urban\" %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1] TRUE\nc(\"urban\",\"industry\") %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1]  TRUE FALSE\n\nBoolean values are usually (always?) coerced to a 1 and 0, in case an arithmetic operation is then demanded.\n\nmean(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 0.75\nsum(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 3\n3*(c(TRUE,FALSE)+TRUE)\n#&gt; [1] 6 3\n\nA set of functions then apply specifically to the Boolean values TRUE or FALSE, i.e. to logical vectors.\nThese logical operators are: !, &, |, xor and typically used to select elements that match 2 or more conditions.\nThe graphic below, reproduced from R for Data Science (highly recommended!), demonstrate their outcome for 2 sets and an example is provided after creating 2 logical vectors, x and y from the LETTERS character vector.\n\n\n\nsource: Fig.12.1 from https://r4ds.hadley.nz/logicals\n\n\n\nLETTERS[1:6]\n#&gt; [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\nx&lt;-LETTERS[1:6]&lt;\"E\"\ny&lt;-LETTERS[1:6]&gt;\"B\"\nx\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\ny\n#&gt; [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx & !y\n#&gt; [1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n!x & y\n#&gt; [1] FALSE FALSE FALSE FALSE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx | y\n#&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE\nxor(x,y)\n#&gt; [1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#vectors-functions",
    "href": "022_vectors.html#vectors-functions",
    "title": "6  Vectors",
    "section": "6.5 Vectors’ functions",
    "text": "6.5 Vectors’ functions\nA number of functions are evaluated over an entire vector (vectorisation is there to avoid loops) and used to describe and understand the distribution of values. They apply mostly to numeric vectors but also to logicals (being 0 or 1 anyway) as well as characters, where possible (based on alphabetical order).\n\n6.5.1 Range, cumulative values, positions and sorting\n\nExamples of functions evaluated over an entire vector\n\n\nmin(x)\ncummin(x)\nwhich.min(x)\nsort(x)\n\n\nmax(x)\ncummax(x)\nwhich.max(x)\norder(x)\n\n\nrange(x)\n\n\nrank(x)\n\n\nsum(x)\ncumsum(x)\n\n\n\n\n\nLet’s explore some of those for x being a numeric, a logical and character. In class exploration.\nIt is sometimes difficult to remember differences between sort(x),order(x) and rank(x)\nSee two examples below\n\nset.seed(101)\nx&lt;-round(runif(10,100,200))\nx\n#&gt;  [1] 137 104 171 166 125 130 158 133 162 155\n\ncbind(Original=x,Sorted=sort(x),Rank=rank(x),Order=order(x))\n#&gt;       Original Sorted Rank Order\n#&gt;  [1,]      137    104    5     2\n#&gt;  [2,]      104    125    1     5\n#&gt;  [3,]      171    130   10     6\n#&gt;  [4,]      166    133    9     8\n#&gt;  [5,]      125    137    2     1\n#&gt;  [6,]      130    155    3    10\n#&gt;  [7,]      158    158    7     7\n#&gt;  [8,]      133    162    4     9\n#&gt;  [9,]      162    166    8     4\n#&gt; [10,]      155    171    6     3\n\n\nM&lt;-month.name[1:6]\nM\n#&gt; [1] \"January\"  \"February\" \"March\"    \"April\"    \"May\"      \"June\"\n\ncbind(Months=M,Sorted=sort(M),Rank=rank(M),Order=order(M))\n#&gt;      Months     Sorted     Rank Order\n#&gt; [1,] \"January\"  \"April\"    \"3\"  \"4\"  \n#&gt; [2,] \"February\" \"February\" \"2\"  \"2\"  \n#&gt; [3,] \"March\"    \"January\"  \"5\"  \"1\"  \n#&gt; [4,] \"April\"    \"June\"     \"1\"  \"6\"  \n#&gt; [5,] \"May\"      \"March\"    \"6\"  \"3\"  \n#&gt; [6,] \"June\"     \"May\"      \"4\"  \"5\"\n\nTo those operations, we should add all univariate statistics such as mean(), median(), var(), quantile(), but we leave them aside for now as they are introduced later with univariate statistics and distributions.\nSpecific to logical vectors, the any() and all() functions are particularly useful in the case you check a very long vector with only very few having a TRUE or a FALSE.\nSee basic examples:\n\nany(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] TRUE\nany(c(TRUE,TRUE,TRUE,TRUE))\n#&gt; [1] TRUE\nall(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] FALSE\n\nAnd an example where we suppose a random set of values from a normal distribution and we want to check manually whether there is an upper outlier. Typically (as in boxplots) this outlier is calculated as being any value above the 3rd quartile plus 1.5 times the interquartile range.\n\nset.seed(102)\nx&lt;-rnorm(100)\nq75&lt;-quantile(x, p=0.75)\niqr&lt;-IQR(x)\nany(x&gt;(q75+1.5*iqr))\n#&gt; [1] TRUE\nboxplot(x) #\n\n\n\n\n\n\n\nx[x&gt;(q75+1.5*iqr)] #to identify them\n#&gt; [1] 3.114333\n\nset.seed(101) #Check with this seed!\n\n\n\n6.5.2 Summary\nOne of the most used function for analysis (if not THE most used) is summary(). We will see its result may change substantially based on the object class it is applied to. Its basic functioning for a simple numeric vector and character vector is:\n\nsummary(c(1,2,3,4,NA,6))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;     1.0     2.0     3.0     3.2     4.0     6.0       1\nsummary(c(\"A\",\"B\",\"C\",NA,NA,\"F\"))\n#&gt;    Length     Class      Mode \n#&gt;         6 character character\n\n\n\n6.5.3 Element-wise functions\nOther functions apply to two or more vectors. They need an x and a y vector as input, e.g. cor(x,y). Most of those you will encounter use two vectors of the same length and type, i.e. no recycling, which leads us to the notion of a data frame (see next chapter).\nAt this time, we show only two simple “parallel” or “element-wise” computations: pmin() and pmax(), which can be useful for example when there is a repeated measure for a given set of individuals. Cases are not rare when you make a new vector (or column in a data frame) based on such element-wise calculations. (Yet you would probably assemble the data into a data frame first and then make a row-wise calculation).\n\nt1&lt;-c(25,35,45,55)\nt2&lt;-c(26,36,44,54)\nt3&lt;-c(27,34,43,56)\n\npmin(t1,t2,t3)\n#&gt; [1] 25 34 43 54\npmax(t1,t2,t3)\n#&gt; [1] 27 36 45 56",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#indexing-and-subsetting",
    "href": "022_vectors.html#indexing-and-subsetting",
    "title": "6  Vectors",
    "section": "6.6 Indexing and subsetting",
    "text": "6.6 Indexing and subsetting\nNow you have seen that vectors are the key elements in R, you will still want to access its elements individually or parts of it based on positions or conditions\nSquare brackets I used to get into vector elements by their position (or by their name if named).\nYou can supply one position but also several positions and ranges. Examine the following:\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx #all\n#&gt;  [1] 3 1 9 7 8 2 3 6 3 4\nx[4] #4th element\n#&gt; [1] 7\nx[6:8] #a sequence\n#&gt; [1] 2 3 6\nx[c(4,6:8)] #both\n#&gt; [1] 7 2 3 6\nx[-4] #all but the 4th element\n#&gt; [1] 3 1 9 8 2 3 6 3 4\nx[-length(x)] #all but the last one\n#&gt; [1] 3 1 9 7 8 2 3 6 3\nx[-((length(x)-1):length(x))]  #all but the last two\n#&gt; [1] 3 1 9 7 8 2 3 6\n\nThis is very flexible because you can use any vector representing an index (position) to make a new vector.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nchicken&lt;-c(10,10,10,4,2,3)\nx[chicken]\n#&gt; [1] 4 4 4 7 1 9\n\nSuch a vector can be a logical, then the elements where the logical is TRUE are returned.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx&gt;3\n#&gt;  [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\nx[x&gt;3]\n#&gt; [1] 9 7 8 6 4\n\nx %% 2 == 0\n#&gt;  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\nx[x %% 2 == 0] #even numbers only\n#&gt; [1] 8 2 6 4\n\nx[x&gt;mean(x)]\n#&gt; [1] 9 7 8 6\n\nx[as.logical(c(0,1))] #?? recyling is in effect here\n#&gt; [1] 1 7 2 6 4\n\nPossibilities are endless, especially if you think the indexing vector can come from another vector than x.\nVectors are sometimes named, and names the used to retrieve the data:\n\ny&lt;-c(1000,1500,900,1200)\nnames(y)&lt;-c(\"UK\",\"BE\",\"LU\",\"FR\")\n\ny[\"LU\"]\n#&gt;  LU \n#&gt; 900\n\nLast but not least, once selected an element can then be assigned a new value:\n\nz&lt;-c(0,0,0,4,2,3,0)\nz\n#&gt; [1] 0 0 0 4 2 3 0\n\nz[2]&lt;-5\nz\n#&gt; [1] 0 5 0 4 2 3 0\n\nz[z==0]&lt;-NA  #a very much used use case\nz\n#&gt; [1] NA  5 NA  4  2  3 NA",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sequences-and-repetitions",
    "href": "022_vectors.html#sequences-and-repetitions",
    "title": "6  Vectors",
    "section": "6.7 Sequences and repetitions",
    "text": "6.7 Sequences and repetitions\nThere are many instances where you will need to create a repeated set of values or sequence of values. The functions seq() and rep() are used extensively.\nImagine as a first example you have 5 regions named 2,4,6,8,10 and each of them send a flow of cars to each other, thus building a 5 x 5 matrix =25 records of flows.\n\nregions&lt;-seq(from=2, to=10, by=2)\nregions\n#&gt; [1]  2  4  6  8 10\norigins&lt;-rep(regions, 5)\norigins\n#&gt;  [1]  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10\ndestinations&lt;-rep(regions, each=5)\ndestinations\n#&gt;  [1]  2  2  2  2  2  4  4  4  4  4  6  6  6  6  6  8  8  8  8  8 10 10 10 10 10\nflows&lt;-paste(\"from \",origins, \" to \", destinations)\nflows\n#&gt;  [1] \"from  2  to  2\"   \"from  4  to  2\"   \"from  6  to  2\"   \"from  8  to  2\"  \n#&gt;  [5] \"from  10  to  2\"  \"from  2  to  4\"   \"from  4  to  4\"   \"from  6  to  4\"  \n#&gt;  [9] \"from  8  to  4\"   \"from  10  to  4\"  \"from  2  to  6\"   \"from  4  to  6\"  \n#&gt; [13] \"from  6  to  6\"   \"from  8  to  6\"   \"from  10  to  6\"  \"from  2  to  8\"  \n#&gt; [17] \"from  4  to  8\"   \"from  6  to  8\"   \"from  8  to  8\"   \"from  10  to  8\" \n#&gt; [21] \"from  2  to  10\"  \"from  4  to  10\"  \"from  6  to  10\"  \"from  8  to  10\" \n#&gt; [25] \"from  10  to  10\"\n\nIf you like to give a unique number to identify each flow with a number starting at 100 and don’t calculate that you have 25 of them, seq_along`is your tool:\n\nseq_along(flows)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\ncbind(seq_along(flows),flows)\n#&gt;            flows             \n#&gt;  [1,] \"1\"  \"from  2  to  2\"  \n#&gt;  [2,] \"2\"  \"from  4  to  2\"  \n#&gt;  [3,] \"3\"  \"from  6  to  2\"  \n#&gt;  [4,] \"4\"  \"from  8  to  2\"  \n#&gt;  [5,] \"5\"  \"from  10  to  2\" \n#&gt;  [6,] \"6\"  \"from  2  to  4\"  \n#&gt;  [7,] \"7\"  \"from  4  to  4\"  \n#&gt;  [8,] \"8\"  \"from  6  to  4\"  \n#&gt;  [9,] \"9\"  \"from  8  to  4\"  \n#&gt; [10,] \"10\" \"from  10  to  4\" \n#&gt; [11,] \"11\" \"from  2  to  6\"  \n#&gt; [12,] \"12\" \"from  4  to  6\"  \n#&gt; [13,] \"13\" \"from  6  to  6\"  \n#&gt; [14,] \"14\" \"from  8  to  6\"  \n#&gt; [15,] \"15\" \"from  10  to  6\" \n#&gt; [16,] \"16\" \"from  2  to  8\"  \n#&gt; [17,] \"17\" \"from  4  to  8\"  \n#&gt; [18,] \"18\" \"from  6  to  8\"  \n#&gt; [19,] \"19\" \"from  8  to  8\"  \n#&gt; [20,] \"20\" \"from  10  to  8\" \n#&gt; [21,] \"21\" \"from  2  to  10\" \n#&gt; [22,] \"22\" \"from  4  to  10\" \n#&gt; [23,] \"23\" \"from  6  to  10\" \n#&gt; [24,] \"24\" \"from  8  to  10\" \n#&gt; [25,] \"25\" \"from  10  to  10\"\n\nWith seq you can also partition a range of values into a given number of intervals without knowing what the value of each interval is. Suppose you have 14 teams to provide water to Marathonians. At which distance are you going to place those teams along the path, knowing you need one at the end and one at the start?\n\nStands&lt;-seq(0,42195,length=14)\n\nAnd suppose that at every third stand you provide some snacks, not just water. So you repeat the pattern “water,water,food” until the end of the vector of Stands\n\nSnacks&lt;-rep(c(FALSE,FALSE,TRUE), length.out = length(Stands))\nSnacks\n#&gt;  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n#&gt; [13] FALSE FALSE\n\nYou also use sequences to display some functions you like to understand better, such as a polynomial or quadratic.\n\nx&lt;-seq(from=1, to=500, length = 100) #m from school\ny&lt;-1000 - 0.001*x^2 + 0.3*x #house rental value\nplot(x=x,y=y)\n\n\n\n\n\n\n\n\n\n#although you could use curve() here as well:\ncurve(1000 - 0.001*x^2 + 0.3*x, from=1, to=500)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html",
    "href": "023_df_lst.html",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1 Data frames",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#data-frames",
    "href": "023_df_lst.html#data-frames",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1.1 A Data frame … your beloved spreadsheet\n\ndf&lt;-data.frame() #an empty one\n\ndf&lt;-data.frame(a = 1:5,\n           b = letters[1:5],\n           c = rnorm(n = 5))\ndf\n#&gt;   a b           c\n#&gt; 1 1 a  0.46340599\n#&gt; 2 2 b -1.30047198\n#&gt; 3 3 c  0.21847251\n#&gt; 4 4 d -0.63928307\n#&gt; 5 5 e -0.02116675\n\nSee how it is summarized. Basically summarizing each vector in columns.\n\nsummary(df)\n#&gt;        a          b                   c           \n#&gt;  Min.   :1   Length:5           Min.   :-1.30047  \n#&gt;  1st Qu.:2   Class :character   1st Qu.:-0.63928  \n#&gt;  Median :3   Mode  :character   Median :-0.02117  \n#&gt;  Mean   :3                      Mean   :-0.25581  \n#&gt;  3rd Qu.:4                      3rd Qu.: 0.21847  \n#&gt;  Max.   :5                      Max.   : 0.46341\n\n\n\n7.1.2 Accessing and subsetting\n!!! THIS IS EXTREMELY IMPORTANT !!! !!! Don’t forget the comma\nIdentifying\n\ndf[,\"b\"] #all rows but only the column named \"b\"\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\ndf$b #same !\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nSubsetting based on position\n\ndf$c[1:2] #subsetting a vector: only first 2 values\n#&gt; [1]  0.463406 -1.300472\ndf[1:2,\"b\"]\n#&gt; [1] \"a\" \"b\"\ndf[1:2,] #first 2 records, all variables. Don't forget the comma !\n#&gt;   a b         c\n#&gt; 1 1 a  0.463406\n#&gt; 2 2 b -1.300472\n\nSubsetting specific rows (not range as above):\n\ndf[1:3,\"b\"]\n#&gt; [1] \"a\" \"b\" \"c\"\ndf[c(1,3),\"b\"]\n#&gt; [1] \"a\" \"c\"\n\nEach dataframe column must have the same number of elements.\n(Enjoy this condition after thinking of how many times you had misaligned and columns of different lengths in Ms Exc..)\n\nz&lt;-c(\"Mom\",\"Dad\",3) #remember each vector as only one type of data, so this is coerced to character\n\ndf$z&lt;-z #Should not work becouse of different length\n#&gt; Error in `$&lt;-.data.frame`(`*tmp*`, z, value = c(\"Mom\", \"Dad\", \"3\")): replacement has 3 rows, data has 5\n\n\nlength(df$a)\n#&gt; [1] 5\nlength(z)\n#&gt; [1] 3\n\n#Beware. Length applied to the whole df means the number of columns!\nlength(df) #i.e. a b and c\n#&gt; [1] 3\n\nWhile length is general, for a data frame you can rather compute number of rows and columns this way\n\nnrow(df)\n#&gt; [1] 5\nncol(df)\n#&gt; [1] 3\ndim(df)\n#&gt; [1] 5 3\nnrow(df)==dim(df)[2] #What do you think?\n#&gt; [1] FALSE\n\n#But these won't work for a vector:\ndim(df$a)\n#&gt; NULL\nnrow(df$a)\n#&gt; NULL\n\nOften times data frames have more complicated column names. It is useful to access those directly.\nThis is also the way you change the name of a column:\n\nnames(df)[2]&lt;-\"blabla\"\ndf\n#&gt;   a blabla           c\n#&gt; 1 1      a  0.46340599\n#&gt; 2 2      b -1.30047198\n#&gt; 3 3      c  0.21847251\n#&gt; 4 4      d -0.63928307\n#&gt; 5 5      e -0.02116675",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#lists",
    "href": "023_df_lst.html#lists",
    "title": "7  Data frames and lists",
    "section": "7.2 Lists",
    "text": "7.2 Lists\nA List is a more general object than a dataframe.\nAll “columns” are not necessarily\n\nof the same length\nnor of the same class\n\n\nmylist&lt;-list(a = 1:5,\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3 4 5\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -0.1343675 -0.1375877 -0.5744943  0.4510188  1.1053019\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 5      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nmylist&lt;-list(a = 1:3,#we removed 2 elements here\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -1.7104115 -2.1850318  0.1866226  0.3223594 -1.0090578\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 3      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nlength(mylist) #number of elements in list\n#&gt; [1] 3\nlength(mylist$b) #number of objects within that element of the list\n#&gt; [1] 5\n\n\n7.2.1 Into the lists: [[ ]] vs [ ]\n\nmylist[2] # getting the list element displayed\n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[2])  # you see it is a list element\n#&gt; [1] \"list\"\nmylist[[2]] # getting the corresponding vector\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[[2]]) # you now see it is a character vector\n#&gt; [1] \"character\"\n\nAnd now subsetting:\n\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -1.7104115 -2.1850318  0.1866226  0.3223594 -1.0090578\nmylist[[2]][1] #1st element of the 2nd element of the list\n#&gt; [1] \"a\"\nmylist[[1]][2] #2nd element of the 1st element of the list\n#&gt; [1] 2\nM&lt;-mylist[[3]]\nM[1] #Subsetting just as any vector\n#&gt; [1] -1.710411",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html",
    "href": "024_df_functions.html",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "8.1 What is in a function? BMI example\npaste.heightweight&lt;-function(h,w){\n  print(paste(h,w))\n  }\npaste.heightweight(1.8,80) #you provide the 2 arguments and get the output\n#&gt; [1] \"1.8 80\"\nNow let’s do the computation with the BMI calculation with a new function\nbmi.calc&lt;-function(h,w){w/h^2}\nwhich we apply\nbmi.calc(1.8,80)\n#&gt; [1] 24.69136\nA function can take a sequence of processes (e.g compute, rounds, concatenate a sentence,…) and then returns the result of the last process.\nExample\nbmi.calc.text&lt;-function(h,w){\n  b&lt;-w/h^2\n  brounded&lt;-round(b)\n  paste(\"My BMI is\", brounded, \"kg/m2\")\n}\nbmi.calc.text(1.8,80)\n#&gt; [1] \"My BMI is 25 kg/m2\"\nFor clarity the outcome of the function can be put in a return()\nbmi.calc&lt;-function(h,w){\n  return(round(w/h^2))\n}\nbmi.calc(1.8,80)\n#&gt; [1] 25",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "href": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "Let’s create a BMI function\nFirst a simple function that simply prints a given height and weight",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "href": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "title": "8  Working with data frames and functions",
    "section": "8.2 Applying a function to a data frame column",
    "text": "8.2 Applying a function to a data frame column\nLet’s create a 2nd function to transfors degrees from Celsius to Fahrenheit\nSimpler with a single argument (x):\n\ncelsius2fahrenheit&lt;-function(x){round(32+(x*9/5))}\n\ncelsius2fahrenheit(25) #25 celsius degree is thus \n#&gt; [1] 77\n\nWhich we now apply to a series of values stored in a column within a data frame\n\nmytable&lt;-data.frame(A=c(21,22,23,24,25,26,27))\nmytable$F&lt;-celsius2fahrenheit(mytable[,\"A\"])\nmytable\n#&gt;    A  F\n#&gt; 1 21 70\n#&gt; 2 22 72\n#&gt; 3 23 73\n#&gt; 4 24 75\n#&gt; 5 25 77\n#&gt; 6 26 79\n#&gt; 7 27 81",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#data-frames-and-nas",
    "href": "024_df_functions.html#data-frames-and-nas",
    "title": "8  Working with data frames and functions",
    "section": "8.3 Data frames and NA’s",
    "text": "8.3 Data frames and NA’s\nComputation of a new column from columns of a dataframe\n\nmytable$G&lt;-mytable$A+mytable$F #note: adding C and F temperature is nonsensical though\nmytable$Gsquare&lt;-mytable$G^2 #note how you write an exponent \"^\" in R\nmytable$A*mytable$F # or a multiplication \"*\"\n#&gt; [1] 1470 1584 1679 1800 1925 2054 2187\n\nSimilarly we can apply our BMI computation to a data frame with heights and weights\n\nbmidf&lt;-data.frame(\n  h=c(1.8,1.7,2,1.9),\n  w=c(70,70,95,100))\n\nWe add the result of computing BMI directly as a new column “BMI” in our data.frame\n\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n\nNA is for unknowns !\nSuppose the 2nd person of our sample didn’t share his/her weight with us\n\nbmidf$w[2]&lt;-NA #NA is for unknowns\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n#You see the BMI could therefore not be computed\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nFor some functions you would still want to compute a value while ignoring the NA’s\nThe mean is a classical example\n\nmean(bmidf$h) #works\n#&gt; [1] 1.85\nmean(bmidf$w) #but returns NA because of one value not reported\n#&gt; [1] NA\n\nYou can explicitly ask to compute without the NA’s:\n\nmean(bmidf$w, na.rm=TRUE) #now works!\n#&gt; [1] 88.33333\n\nUsing complete cases\nFor some data frame made of surveyed values where different variables are filled in sparsely, it is important you get access only to entirely completed individuals\n\ncomplete.cases(bmidf) #returns a logical indicating whether the row \n#&gt; [1]  TRUE FALSE  TRUE  TRUE\n# has not a singleNA\nclass(complete.cases(bmidf))\n#&gt; [1] \"logical\"\n#Note that with logicals, TRUE is 1 and FALSE is zero. Thus\n\nsum(complete.cases(bmidf))\n#&gt; [1] 3\n\n#You can use this logical to subset the rows\n# and have a \"clean\" df\nbmidf2&lt;-bmidf[complete.cases(bmidf),] #read this as \"select complete cases rows with all columns\nbmidf2\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#environment-listing-and-management",
    "href": "024_df_functions.html#environment-listing-and-management",
    "title": "8  Working with data frames and functions",
    "section": "8.4 Environment listing and management",
    "text": "8.4 Environment listing and management\nWe have now created a bunch of objects which we can see in the Environment window of RStudio.\nIn the console we can also see them with\n\nls()\n#&gt; [1] \"bmi.calc\"           \"bmi.calc.text\"      \"bmidf\"             \n#&gt; [4] \"bmidf2\"             \"celsius2fahrenheit\" \"mytable\"           \n#&gt; [7] \"paste.heightweight\"\n\nAnd any of these objects can be removed with\n\nrm()\n#for example\nrm(mytable)\n\nIn the environment window of RStudio you also see the structure of objects (when displayed as a list not a grid)\nFrom the console you use the structure function str() to get the same info\n\nstr(bmidf)\n#&gt; 'data.frame':    4 obs. of  3 variables:\n#&gt;  $ h  : num  1.8 1.7 2 1.9\n#&gt;  $ w  : num  70 NA 95 100\n#&gt;  $ BMI: num  22 NA 24 28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#viewing-data-frame",
    "href": "024_df_functions.html#viewing-data-frame",
    "title": "8  Working with data frames and functions",
    "section": "8.5 Viewing data frame",
    "text": "8.5 Viewing data frame\n\nView(bmidf)\n\nis the most pleasant interactive way to view a data frame\nBut be careful if many rows or columns !\nThe classical console way is simply\n\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nIn case of a large vector or df there will be a limited display of 1000 values (default) in console\nSuppose you have a vector of 1002 values\n\nmydf&lt;-data.frame(z=1:502, zrev=502:1)\nmydf\n#&gt;       z zrev\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; 4     4  499\n#&gt; 5     5  498\n#&gt; 6     6  497\n#&gt; 7     7  496\n#&gt; 8     8  495\n#&gt; 9     9  494\n#&gt; 10   10  493\n#&gt; 11   11  492\n#&gt; 12   12  491\n#&gt; 13   13  490\n#&gt; 14   14  489\n#&gt; 15   15  488\n#&gt; 16   16  487\n#&gt; 17   17  486\n#&gt; 18   18  485\n#&gt; 19   19  484\n#&gt; 20   20  483\n#&gt; 21   21  482\n#&gt; 22   22  481\n#&gt; 23   23  480\n#&gt; 24   24  479\n#&gt; 25   25  478\n#&gt; 26   26  477\n#&gt; 27   27  476\n#&gt; 28   28  475\n#&gt; 29   29  474\n#&gt; 30   30  473\n#&gt; 31   31  472\n#&gt; 32   32  471\n#&gt; 33   33  470\n#&gt; 34   34  469\n#&gt; 35   35  468\n#&gt; 36   36  467\n#&gt; 37   37  466\n#&gt; 38   38  465\n#&gt; 39   39  464\n#&gt; 40   40  463\n#&gt; 41   41  462\n#&gt; 42   42  461\n#&gt; 43   43  460\n#&gt; 44   44  459\n#&gt; 45   45  458\n#&gt; 46   46  457\n#&gt; 47   47  456\n#&gt; 48   48  455\n#&gt; 49   49  454\n#&gt; 50   50  453\n#&gt; 51   51  452\n#&gt; 52   52  451\n#&gt; 53   53  450\n#&gt; 54   54  449\n#&gt; 55   55  448\n#&gt; 56   56  447\n#&gt; 57   57  446\n#&gt; 58   58  445\n#&gt; 59   59  444\n#&gt; 60   60  443\n#&gt; 61   61  442\n#&gt; 62   62  441\n#&gt; 63   63  440\n#&gt; 64   64  439\n#&gt; 65   65  438\n#&gt; 66   66  437\n#&gt; 67   67  436\n#&gt; 68   68  435\n#&gt; 69   69  434\n#&gt; 70   70  433\n#&gt; 71   71  432\n#&gt; 72   72  431\n#&gt; 73   73  430\n#&gt; 74   74  429\n#&gt; 75   75  428\n#&gt; 76   76  427\n#&gt; 77   77  426\n#&gt; 78   78  425\n#&gt; 79   79  424\n#&gt; 80   80  423\n#&gt; 81   81  422\n#&gt; 82   82  421\n#&gt; 83   83  420\n#&gt; 84   84  419\n#&gt; 85   85  418\n#&gt; 86   86  417\n#&gt; 87   87  416\n#&gt; 88   88  415\n#&gt; 89   89  414\n#&gt; 90   90  413\n#&gt; 91   91  412\n#&gt; 92   92  411\n#&gt; 93   93  410\n#&gt; 94   94  409\n#&gt; 95   95  408\n#&gt; 96   96  407\n#&gt; 97   97  406\n#&gt; 98   98  405\n#&gt; 99   99  404\n#&gt; 100 100  403\n#&gt; 101 101  402\n#&gt; 102 102  401\n#&gt; 103 103  400\n#&gt; 104 104  399\n#&gt; 105 105  398\n#&gt; 106 106  397\n#&gt; 107 107  396\n#&gt; 108 108  395\n#&gt; 109 109  394\n#&gt; 110 110  393\n#&gt; 111 111  392\n#&gt; 112 112  391\n#&gt; 113 113  390\n#&gt; 114 114  389\n#&gt; 115 115  388\n#&gt; 116 116  387\n#&gt; 117 117  386\n#&gt; 118 118  385\n#&gt; 119 119  384\n#&gt; 120 120  383\n#&gt; 121 121  382\n#&gt; 122 122  381\n#&gt; 123 123  380\n#&gt; 124 124  379\n#&gt; 125 125  378\n#&gt; 126 126  377\n#&gt; 127 127  376\n#&gt; 128 128  375\n#&gt; 129 129  374\n#&gt; 130 130  373\n#&gt; 131 131  372\n#&gt; 132 132  371\n#&gt; 133 133  370\n#&gt; 134 134  369\n#&gt; 135 135  368\n#&gt; 136 136  367\n#&gt; 137 137  366\n#&gt; 138 138  365\n#&gt; 139 139  364\n#&gt; 140 140  363\n#&gt; 141 141  362\n#&gt; 142 142  361\n#&gt; 143 143  360\n#&gt; 144 144  359\n#&gt; 145 145  358\n#&gt; 146 146  357\n#&gt; 147 147  356\n#&gt; 148 148  355\n#&gt; 149 149  354\n#&gt; 150 150  353\n#&gt; 151 151  352\n#&gt; 152 152  351\n#&gt; 153 153  350\n#&gt; 154 154  349\n#&gt; 155 155  348\n#&gt; 156 156  347\n#&gt; 157 157  346\n#&gt; 158 158  345\n#&gt; 159 159  344\n#&gt; 160 160  343\n#&gt; 161 161  342\n#&gt; 162 162  341\n#&gt; 163 163  340\n#&gt; 164 164  339\n#&gt; 165 165  338\n#&gt; 166 166  337\n#&gt; 167 167  336\n#&gt; 168 168  335\n#&gt; 169 169  334\n#&gt; 170 170  333\n#&gt; 171 171  332\n#&gt; 172 172  331\n#&gt; 173 173  330\n#&gt; 174 174  329\n#&gt; 175 175  328\n#&gt; 176 176  327\n#&gt; 177 177  326\n#&gt; 178 178  325\n#&gt; 179 179  324\n#&gt; 180 180  323\n#&gt; 181 181  322\n#&gt; 182 182  321\n#&gt; 183 183  320\n#&gt; 184 184  319\n#&gt; 185 185  318\n#&gt; 186 186  317\n#&gt; 187 187  316\n#&gt; 188 188  315\n#&gt; 189 189  314\n#&gt; 190 190  313\n#&gt; 191 191  312\n#&gt; 192 192  311\n#&gt; 193 193  310\n#&gt; 194 194  309\n#&gt; 195 195  308\n#&gt; 196 196  307\n#&gt; 197 197  306\n#&gt; 198 198  305\n#&gt; 199 199  304\n#&gt; 200 200  303\n#&gt; 201 201  302\n#&gt; 202 202  301\n#&gt; 203 203  300\n#&gt; 204 204  299\n#&gt; 205 205  298\n#&gt; 206 206  297\n#&gt; 207 207  296\n#&gt; 208 208  295\n#&gt; 209 209  294\n#&gt; 210 210  293\n#&gt; 211 211  292\n#&gt; 212 212  291\n#&gt; 213 213  290\n#&gt; 214 214  289\n#&gt; 215 215  288\n#&gt; 216 216  287\n#&gt; 217 217  286\n#&gt; 218 218  285\n#&gt; 219 219  284\n#&gt; 220 220  283\n#&gt; 221 221  282\n#&gt; 222 222  281\n#&gt; 223 223  280\n#&gt; 224 224  279\n#&gt; 225 225  278\n#&gt; 226 226  277\n#&gt; 227 227  276\n#&gt; 228 228  275\n#&gt; 229 229  274\n#&gt; 230 230  273\n#&gt; 231 231  272\n#&gt; 232 232  271\n#&gt; 233 233  270\n#&gt; 234 234  269\n#&gt; 235 235  268\n#&gt; 236 236  267\n#&gt; 237 237  266\n#&gt; 238 238  265\n#&gt; 239 239  264\n#&gt; 240 240  263\n#&gt; 241 241  262\n#&gt; 242 242  261\n#&gt; 243 243  260\n#&gt; 244 244  259\n#&gt; 245 245  258\n#&gt; 246 246  257\n#&gt; 247 247  256\n#&gt; 248 248  255\n#&gt; 249 249  254\n#&gt; 250 250  253\n#&gt; 251 251  252\n#&gt; 252 252  251\n#&gt; 253 253  250\n#&gt; 254 254  249\n#&gt; 255 255  248\n#&gt; 256 256  247\n#&gt; 257 257  246\n#&gt; 258 258  245\n#&gt; 259 259  244\n#&gt; 260 260  243\n#&gt; 261 261  242\n#&gt; 262 262  241\n#&gt; 263 263  240\n#&gt; 264 264  239\n#&gt; 265 265  238\n#&gt; 266 266  237\n#&gt; 267 267  236\n#&gt; 268 268  235\n#&gt; 269 269  234\n#&gt; 270 270  233\n#&gt; 271 271  232\n#&gt; 272 272  231\n#&gt; 273 273  230\n#&gt; 274 274  229\n#&gt; 275 275  228\n#&gt; 276 276  227\n#&gt; 277 277  226\n#&gt; 278 278  225\n#&gt; 279 279  224\n#&gt; 280 280  223\n#&gt; 281 281  222\n#&gt; 282 282  221\n#&gt; 283 283  220\n#&gt; 284 284  219\n#&gt; 285 285  218\n#&gt; 286 286  217\n#&gt; 287 287  216\n#&gt; 288 288  215\n#&gt; 289 289  214\n#&gt; 290 290  213\n#&gt; 291 291  212\n#&gt; 292 292  211\n#&gt; 293 293  210\n#&gt; 294 294  209\n#&gt; 295 295  208\n#&gt; 296 296  207\n#&gt; 297 297  206\n#&gt; 298 298  205\n#&gt; 299 299  204\n#&gt; 300 300  203\n#&gt; 301 301  202\n#&gt; 302 302  201\n#&gt; 303 303  200\n#&gt; 304 304  199\n#&gt; 305 305  198\n#&gt; 306 306  197\n#&gt; 307 307  196\n#&gt; 308 308  195\n#&gt; 309 309  194\n#&gt; 310 310  193\n#&gt; 311 311  192\n#&gt; 312 312  191\n#&gt; 313 313  190\n#&gt; 314 314  189\n#&gt; 315 315  188\n#&gt; 316 316  187\n#&gt; 317 317  186\n#&gt; 318 318  185\n#&gt; 319 319  184\n#&gt; 320 320  183\n#&gt; 321 321  182\n#&gt; 322 322  181\n#&gt; 323 323  180\n#&gt; 324 324  179\n#&gt; 325 325  178\n#&gt; 326 326  177\n#&gt; 327 327  176\n#&gt; 328 328  175\n#&gt; 329 329  174\n#&gt; 330 330  173\n#&gt; 331 331  172\n#&gt; 332 332  171\n#&gt; 333 333  170\n#&gt; 334 334  169\n#&gt; 335 335  168\n#&gt; 336 336  167\n#&gt; 337 337  166\n#&gt; 338 338  165\n#&gt; 339 339  164\n#&gt; 340 340  163\n#&gt; 341 341  162\n#&gt; 342 342  161\n#&gt; 343 343  160\n#&gt; 344 344  159\n#&gt; 345 345  158\n#&gt; 346 346  157\n#&gt; 347 347  156\n#&gt; 348 348  155\n#&gt; 349 349  154\n#&gt; 350 350  153\n#&gt; 351 351  152\n#&gt; 352 352  151\n#&gt; 353 353  150\n#&gt; 354 354  149\n#&gt; 355 355  148\n#&gt; 356 356  147\n#&gt; 357 357  146\n#&gt; 358 358  145\n#&gt; 359 359  144\n#&gt; 360 360  143\n#&gt; 361 361  142\n#&gt; 362 362  141\n#&gt; 363 363  140\n#&gt; 364 364  139\n#&gt; 365 365  138\n#&gt; 366 366  137\n#&gt; 367 367  136\n#&gt; 368 368  135\n#&gt; 369 369  134\n#&gt; 370 370  133\n#&gt; 371 371  132\n#&gt; 372 372  131\n#&gt; 373 373  130\n#&gt; 374 374  129\n#&gt; 375 375  128\n#&gt; 376 376  127\n#&gt; 377 377  126\n#&gt; 378 378  125\n#&gt; 379 379  124\n#&gt; 380 380  123\n#&gt; 381 381  122\n#&gt; 382 382  121\n#&gt; 383 383  120\n#&gt; 384 384  119\n#&gt; 385 385  118\n#&gt; 386 386  117\n#&gt; 387 387  116\n#&gt; 388 388  115\n#&gt; 389 389  114\n#&gt; 390 390  113\n#&gt; 391 391  112\n#&gt; 392 392  111\n#&gt; 393 393  110\n#&gt; 394 394  109\n#&gt; 395 395  108\n#&gt; 396 396  107\n#&gt; 397 397  106\n#&gt; 398 398  105\n#&gt; 399 399  104\n#&gt; 400 400  103\n#&gt; 401 401  102\n#&gt; 402 402  101\n#&gt; 403 403  100\n#&gt; 404 404   99\n#&gt; 405 405   98\n#&gt; 406 406   97\n#&gt; 407 407   96\n#&gt; 408 408   95\n#&gt; 409 409   94\n#&gt; 410 410   93\n#&gt; 411 411   92\n#&gt; 412 412   91\n#&gt; 413 413   90\n#&gt; 414 414   89\n#&gt; 415 415   88\n#&gt; 416 416   87\n#&gt; 417 417   86\n#&gt; 418 418   85\n#&gt; 419 419   84\n#&gt; 420 420   83\n#&gt; 421 421   82\n#&gt; 422 422   81\n#&gt; 423 423   80\n#&gt; 424 424   79\n#&gt; 425 425   78\n#&gt; 426 426   77\n#&gt; 427 427   76\n#&gt; 428 428   75\n#&gt; 429 429   74\n#&gt; 430 430   73\n#&gt; 431 431   72\n#&gt; 432 432   71\n#&gt; 433 433   70\n#&gt; 434 434   69\n#&gt; 435 435   68\n#&gt; 436 436   67\n#&gt; 437 437   66\n#&gt; 438 438   65\n#&gt; 439 439   64\n#&gt; 440 440   63\n#&gt; 441 441   62\n#&gt; 442 442   61\n#&gt; 443 443   60\n#&gt; 444 444   59\n#&gt; 445 445   58\n#&gt; 446 446   57\n#&gt; 447 447   56\n#&gt; 448 448   55\n#&gt; 449 449   54\n#&gt; 450 450   53\n#&gt; 451 451   52\n#&gt; 452 452   51\n#&gt; 453 453   50\n#&gt; 454 454   49\n#&gt; 455 455   48\n#&gt; 456 456   47\n#&gt; 457 457   46\n#&gt; 458 458   45\n#&gt; 459 459   44\n#&gt; 460 460   43\n#&gt; 461 461   42\n#&gt; 462 462   41\n#&gt; 463 463   40\n#&gt; 464 464   39\n#&gt; 465 465   38\n#&gt; 466 466   37\n#&gt; 467 467   36\n#&gt; 468 468   35\n#&gt; 469 469   34\n#&gt; 470 470   33\n#&gt; 471 471   32\n#&gt; 472 472   31\n#&gt; 473 473   30\n#&gt; 474 474   29\n#&gt; 475 475   28\n#&gt; 476 476   27\n#&gt; 477 477   26\n#&gt; 478 478   25\n#&gt; 479 479   24\n#&gt; 480 480   23\n#&gt; 481 481   22\n#&gt; 482 482   21\n#&gt; 483 483   20\n#&gt; 484 484   19\n#&gt; 485 485   18\n#&gt; 486 486   17\n#&gt; 487 487   16\n#&gt; 488 488   15\n#&gt; 489 489   14\n#&gt; 490 490   13\n#&gt; 491 491   12\n#&gt; 492 492   11\n#&gt; 493 493   10\n#&gt; 494 494    9\n#&gt; 495 495    8\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n#[ reached getOption(\"max.print\") -- omitted 2 entries ]\n\nYou can change the default using\n\n options(max.print=1500)\n\nbut one rarely does this\nMost of the times you want a sneak preview in your data from the top\n\n# head() returns the first rows (5 default) rows:\nhead(mydf)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\nhead(mydf,8)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\n#&gt; 7 7  496\n#&gt; 8 8  495\n\nor the bottom:\n\n# tail() the last ones :\ntail(mydf)\n#&gt;       z zrev\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\ntail(mydf, 7)\n#&gt;       z zrev\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\nor some random records if you load the car package (Package related to book “Companion to Applied Regression” by Fox, J and Weisber, S (2024) )\n\ncar::some(mydf)\n#&gt;       z zrev\n#&gt; 111 111  392\n#&gt; 255 255  248\n#&gt; 295 295  208\n#&gt; 333 333  170\n#&gt; 336 336  167\n#&gt; 337 337  166\n#&gt; 440 440   63\n#&gt; 456 456   47\n#&gt; 464 464   39\n#&gt; 500 500    3\n\nor a brief:\n\ncar::brief(mydf)\n#&gt; 502 x 2 data.frame (497 rows omitted)\n#&gt;       z zrev\n#&gt;     [i]  [i]\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; . . .            \n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied Regression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "025_read_write.html",
    "href": "025_read_write.html",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "9.1 Reading delimited files\nText files and delimited files can easily be imported in R using the function read.table(). You can play with the values of the different arguments to adapt to the format of your file. You have the possibility to load a file\n?read.table\n\nheartSA &lt;- read.table(\"data/SAheart/SAheart.txt\", header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(file.choose(), header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", \n                      sep=\",\", head=T, row.names=1)\nThere exist also some preset functions to read tables with some specific formats. Here again you can play we the values of the different arguments to load your database in an accurate way.\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n          dec = \",\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n            dec = \",\", fill = TRUE, comment.char = \"\", ...)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#reading-delimited-files",
    "href": "025_read_write.html#reading-delimited-files",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "stored locally: writing the path to reach it\nchosen interactively and stored locally\nfrom the web using its url.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#files-form-specific-software",
    "href": "025_read_write.html#files-form-specific-software",
    "title": "9  Reading and writing data to and from R",
    "section": "9.2 Files form specific software",
    "text": "9.2 Files form specific software\nSome databases are exported from other software and have atypical formats.\n\nYou first need to install (once per machine) and load the R package foreign.\n\n\n# install.packages('foreign')\nlibrary(foreign)\n\n\nThen you can load the following different formats\n\n\n# Read the SPSS data\nread.spss(\"example.sav\")\n\n# Read Stata data into R\nread.dta(\"c:/mydata.dta\") \n\nlibrary(xlsx)\n\n# first row contains variable names\nread.xlsx(\"c:/myexcel.xlsx\", 1)\n\n# read in the worksheet named mysheet\nread.xlsx(\"c:/myexcel.xlsx\", sheetName = \"mysheet\") \n\n# The package `readxl` can also be helpful.\n\nlibrary(sas7bdat)\n\n# Read in the SAS data\nmySASData &lt;- read.sas7bdat(\"example.sas7bdat\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#writing-a-dataframe-to-a-file",
    "href": "025_read_write.html#writing-a-dataframe-to-a-file",
    "title": "9  Reading and writing data to and from R",
    "section": "9.3 Writing a dataframe to a file",
    "text": "9.3 Writing a dataframe to a file\nYou can also export a dataset\n\nwrite.table(heartSA, file = \"data/SAheart/NewSAheart.txt\")\nwrite.csv(heartSA, file = \"data/SAheart/NewSAheart.csv\")\nwrite.csv2(heartSA, file = \"data/SAheart/NewSAheart.csv\")\n\nHowever, in case it is for further use within R, we recommend you use the RDS format, which works with any type of R object and is often very effective in terms of file size.\n\nsaveRDS(heartSA,\"data/SAheart/NewSAheart.rds\")\nNewSAheart&lt;-readRDS(\"data/SAheart/NewSAheart.rds\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html",
    "href": "031_sample_population.html",
    "title": "10  Sample and Population",
    "section": "",
    "text": "10.1 Definitions\nSurvey\nElement, record, individual\nPopulation\nTarget population\nSampling units\nFrame\nSample",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#definitions",
    "href": "031_sample_population.html#definitions",
    "title": "10  Sample and Population",
    "section": "",
    "text": "Any activity that collects information in an organised and methodical manner about characteristics of interest from some or all units of a population using well-defined concepts, methods and procedures and compiles such information into a useful summary form.\n\n\n\nAn object on which a measurement is taken.\n\n\n\nA collection of elements.\n\n\n\nThe population for which information is required.\n\n\n\nNon-overlapping collection of elements from the population that covers the entire population.\n\n\n\nThe device which delimits, identifies, and allows access to the elements of the target population. The frame is also called the survey frame or the sampling frame.\n\n\n\nA collection of sampling units drawn from a frame.",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#usual-denotations",
    "href": "031_sample_population.html#usual-denotations",
    "title": "10  Sample and Population",
    "section": "10.2 Usual denotations",
    "text": "10.2 Usual denotations\nPopulation observations are usually denoted with capital letters: \\[X = \\{X_1, X_2,..., X_N\\}\\]\nTheir parameters are denoted with Greek letters, capital letters or a mix: \\[\\mu, \\mu_X, \\sigma, \\sigma_X, \\sigma_{X,Y}, E(X), Var(X), Cov(X,Y), \\rho_{X,Y}\\]\nSample observations are expressed with small letters: \\[x = (x_1, x_2,..., x_n)\\]\nTheir estimators associated are also expressed using small letters, i.e. \\[\\bar{x}, s, s_x, s_{x,y}, var(x), cov(x,y), r_{x,y}\\] …",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "032_univariate.html",
    "href": "032_univariate.html",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1 Measures of Center",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-center",
    "href": "032_univariate.html#measures-of-center",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1.1 Mean\nThe arithmetic mean of a vector x having n observations: x = (x1, x2,… , xi, …, xn) is given by the following formulae for the empirical and theoretical means:\n\\[\\bar{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\] \\[\\mu_X = E(X) = \\dfrac{1}{N} \\sum_{i=1}^N X_i\\]\nwith \\(n\\) the size of the sample and \\(N\\) the size of the population.\nLet’s take a variable with the ages of some individuals:\n\nAge &lt;- c (25, 27, 28, 23, 52, 27, 27, 26, 25, 30)\n\nThe empirical mean of this variable is given by:\n\\((1 / 10) * (25 + 27 + 28 + 23 + 52 + 27 + 27 + 26 + 25 + 30) = 290 / 10 = 29\\)\nYou can compute the mean of a vector as follows in R:\n\nsum(Age) / length(Age)\n#&gt; [1] 29\nmean(Age)\n#&gt; [1] 29\n\nBut in some cases, the mean is not a good indicator of the central value of a distribution. For the above variable, we can see that one individual is 52 years old. The mean is sensitive to extreme values or outliers.\n\n\n11.1.2 Median\nThe median corresponds to the value such that 50% of the individuals have a smaller or equal value and 50% of the individuals have a larger or equal value. It is also called the 50th percentile. We consider the variable Age and a second variable Age2 where the value 52 has been removed. They have respectively 10 and 9 elements.\n\nAge2 &lt;- c (25, 27, 28, 23, 27, 27, 26, 25, 30)\n\nFirst, sort the values of the vector considered from the smallest to the largest. For Age: \\({23, 25, 25, 26, 27, 27, 27, 28, 30, 52}\\)\nand for Age2:\n\\({23, 25, 25, 26, 27, 27, 27, 28, 30}\\)\nWe know already how to do this in R:\n\nsort(Age)\n#&gt;  [1] 23 25 25 26 27 27 27 28 30 52\nsort(Age2)\n#&gt; [1] 23 25 25 26 27 27 27 28 30\n\nSecond,\n\nFor an odd set of numbers (Age2), find the number in the middle of the vector, this is the empirical median. The number to take is also given by: \\((n + 1) / 2 = 10 / 2\\) , i.e. the 5th value, 27\n\n\nsort(Age2)[5]\n#&gt; [1] 27\n\n\nFor an even set of numbers (Age), find the two numbers in the middle and compute their average value, this is the empirical median, i.e. \\((27 + 27) / 2 = 27\\)\n\n\nsort(Age)[c(5,6)]\n#&gt; [1] 27 27\nsum(sort(Age)[c(5,6)])/2\n#&gt; [1] 27\n\nWe can see that the median is much less sensitive to extreme values. In both cases, the median is 27. Using R built-in functions, the median is :\n\nmedian(Age)\n#&gt; [1] 27\nmedian(Age2)\n#&gt; [1] 27\n\n\n\n11.1.3 Mode\nThe mode (not to be mistaken with R mode for vectors) corresponds to the value(s) which appears the most often. A vector can have 0, 1 or many modes. For our variable Age, the mode is 27 which appears 3 times.\n\ntable(Age)\n#&gt; Age\n#&gt; 23 25 26 27 28 30 52 \n#&gt;  1  2  1  3  1  1  1\nsort(table(Age), descending=TRUE)\n#&gt; Age\n#&gt; 23 26 28 30 52 25 27 \n#&gt;  1  1  1  1  1  2  3\n\nThe mode is an immediate output in R. But we can write what we have just done and extract the value after sorting, i.e.\n\nsort(table(Age),decreasing = TRUE)[1]\n#&gt; 27 \n#&gt;  3\n\nWhile this requires sorting (which can be long), an alternative would be to use the which.max() function:\n\nwhich.max(table(Age)) #returning the position of the max, i.e. 4th position here\n#&gt; 27 \n#&gt;  4\nAge[which.max(table(Age))] #then using that position into the original vector\n#&gt; [1] 23\n\nWe can also look at the mode for qualitative / categorical variables. If we take the example of the variable score, the mode is the value “C”.\n\nscore &lt;- as.factor ( c (\"C\",\"C\",\"A\",\"B\",\"A\",\"C\",\"B\",\"B\",\"A\",\"C\"))\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-dispersion",
    "href": "032_univariate.html#measures-of-dispersion",
    "title": "11  Univariate statistics",
    "section": "11.2 Measures of Dispersion",
    "text": "11.2 Measures of Dispersion\n\n11.2.1 Range\nRange is the simplest measure of the spread of a distribution and corresponds to the difference between the maximum and the minimum values: \\[Max(x) - Min(x)\\]\nFor the variable Age the minimum being 23 and the maximum 52, the range is: \\(52 - 23 = 29\\).\nIn R the function range returns the two extrema, not the difference, see\n\nmax(Age) - min(Age)\n#&gt; [1] 29\nrange(Age)\n#&gt; [1] 23 52\n\n\n\n11.2.2 Quantiles.\nExtending the concept of a median, quantiles (percentiles, deciles, quartiles,…) divide the distribution into equal slices. The i-th percentile corresponds to the value at which i% of the distribution is below that value. The median is when \\(i=50%\\), i.e. the ditribution is split into 2 half parts so that the probability of drawing a number below the median is 50%.\nPercentiles divide the distribution into 100 slices (probability = \\({0.01, 0.02, 0.03, ..., 1}\\)) ; deciles into 10 (probability = \\({0.1, 0.2, 0.3, ..., 1}\\)) ; quartiles into 4 (probability = \\({0.25, 0.5, 0.75, 1}\\)). You obtain all of these using the same function quantile() and the corresponding probability of picking up a number below:\n\n# quartiles are the default\nquantile(Age)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 23.00 25.25 27.00 27.75 52.00\n#deciles\nquantile(Age, probs = seq(0, 1, 0.1))\n#&gt;   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#&gt; 23.0 24.8 25.0 25.7 26.6 27.0 27.0 27.3 28.4 32.2 52.0\n\n#Suppose a larger set of 100000 values \"normally\" distributed around the mean 0\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n#the 1st , 5th, 95th and 99th % and some others\nquantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99))\n#&gt;         1%         5%        16%        84%        95%        99% \n#&gt; -2.3333179 -1.6488276 -0.9924832  1.0009836  1.6512973  2.3372571\n\n\nhist(x, breaks = 100)\nabline(v=quantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99)), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Inter-quartile range (IQR)\nIt is simply the difference between the 3rd and the 1st quartiles: \\[IQR = Q3(x) - Q1(x)\\].\nAgain for the variable Age, it equals: \\(27.75 - 25.25 = 2.5\\)\n\nquantile(Age, probs = 0.75) - quantile(Age, probs = 0.25)\n#&gt; 75% \n#&gt; 2.5\n\n#or\nIQR(Age)\n#&gt; [1] 2.5\n\n\n\n11.2.4 Variance and Standard Deviation\nAlthough quantiles and plots are very much in use to describe the spread of a distribution, statistical analysis relies most heavily on the notion of variance.\nFirst, think about the simplest way you can measure how a given observation is far from, (i.e. spread out of) a general expected value. A pretty effective way is to measure the difference between that observation and the mean of observations.\nThe Deviation to the mean for an individual i is \\[v_i=x_i-\\bar{x}\\]\nIt is then very tempting to say that the general spread of a variable is simply the sum of all those values. In order for the number not to grow with the number of obseravtions, we then compute an average deviation by dividing by \\(n\\)\n\\[\\Sigma_i(x_i-\\bar{x})/n\\]\nBut is this a good idea?\nTake the Age example:\n\nv&lt;- Age-mean(Age) #set of deviations to the mean\nsum(v)/length(v)\n#&gt; [1] 0\n\nIt seems there is no “spreading” ? In fact, all the negative deviations compensate (here exactly) the positive deviations.\nWe can rather remove the signs and use the absolute value of each deviation, sum them up and divide by \\(n\\).\nThis is called MAD, the Mean Absolute Deviation and is quite easy to interpret indeed.\n\nabs_v&lt;- abs(Age-mean(Age)) #set of deviations to the mean\nmean(abs_v)\n#&gt; [1] 4.8\n\nYet, one could argue that large deviations to the mean are more important than the smaller ones to describe the pattern of deviations, especially since in a normal population there are more values closer to the mean than farther.\nRather than using absolute deviations, (most of) statisticians have therefore opted for squaring the deviations, which is still symmetrical and has the same characteristic of turning every negative value into a positive one.\nWe therefore usually consider the sum of squared deviations to the mean:\n\\[\\Sigma_i^n(x_i-\\bar{x})^2\\] which, we then divide by the number of observations to avoid the value to grow with the number of observations, thus allowing comparisons. It is then called the variance. More precisely, if we use a sample, we still need to use one of our observation in order to estimate the mean, hence we are left with \\(n-1\\) degrees of freedom.\nThe empirical variance is then given by:\n\\[var_x = s^2_x = \\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2\\]\nand the theoretical variance:\n\\[var_X = \\sigma^2_X = \\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2\\] \\[= E(X-E(X))^2 = E(X-\\mu)^2 \\]\nwhere \\(E(X)\\) is the expected mean of the population (or “Esperance”).\nThe variance is thus a single number that gives insight on how the variable is spread around the mean value. A small value (close to 0) indicates a small variability: values are not very different from the mean value. A high value indicated a strong variability.\nThe variance cannot be negative.\nIn R, we use the var() function which we here first reconstruct:\n\n(Age-mean(Age))^2 #squared deviations to the mean\n#&gt;  [1]  16   4   1  36 529   4   4   9  16   1\nsum((Age-mean(Age))^2) #sum of squared deviations to the mean\n#&gt; [1] 620\nsum((Age-mean(Age))^2)/(length(Age)-1) #...divided by n-1\n#&gt; [1] 68.88889\n\nvar(Age)\n#&gt; [1] 68.88889\nvar(Age2)\n#&gt; [1] 4.027778\n\nWe see that the default in R for var() is to divide by \\(n-1\\), i.e. the sample variance.\nFinally, we like the “spread” to be expressed in the same units as the original variable, i.e. years in this case. It is already the case for the Mean Absolute Deviation. We need to take the square root of the variance to obtain a “standard deviation”:\nThe standard deviations correponding to the sample and population varianxe are then given by:\n\\[s_x = \\sqrt{\\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2}\\]\n\\[\\sigma_X = \\sqrt{\\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2}\\]\nAnd can be computed using:\n\nsqrt(var(Age))\n#&gt; [1] 8.299933\nsqrt(var(Age2))\n#&gt; [1] 2.006932\n#or simply\nsd(Age) #again remember it is the sample sd\n#&gt; [1] 8.299933\nsd(Age2)\n#&gt; [1] 2.006932",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#visualize-a-distribution",
    "href": "032_univariate.html#visualize-a-distribution",
    "title": "11  Univariate statistics",
    "section": "11.3 Visualize a distribution",
    "text": "11.3 Visualize a distribution\nVisualizing a distribution with graphics is important for both supporting the analysis and dissemination.\nWe have seen some graphics already above and we are going to produce improved graphics with ggplot later on. Without spending much time on design, the purpose here is show how graphics accompany the univariate statistics we introduced.\n\n11.3.1 Boxplots\nA boxplot visually provides a number of information about the distribution of a variable\n\nthe median value (thick black line),\nthe inter-quartile range (IQR) (the black box),\nthe minimum and maximum values or 1.5 times the IQR (horizontal lines),\nthe outlier(s) (dots out of the whiskers).\n\nValues are considered outliers when they fall outside the whiskers, that is outside a distance of 1.5 times the IQR. In absence of such outliers the horizontal lines show the extrema (min and max).\nWe have added the mean as a red point to clarify here the difference between the mean and median.\nExamine the difference again between Age and Age2\n\npar (mfrow = c(1, 2)) # to display multiple plots at once\nboxplot(Age, ylab = \"Age\", main = \"Boxplot of Age\")\npoints(mean(Age), col = 2, pch = 18)\n\nboxplot(Age2, ylab = \"Age2\", main = \"Boxplot of Age2\")\npoints(mean(Age2), col = 2, pch = 18)\n\n\n\n\n\n\n\n\n\n\n11.3.2 Stem and leaf\nProbably less in use for visual purpose and reporting, a stem and leaf graph is a very effective way to look into the distribution of a variable while you are exploring, analyzing your data.\nIt is not actually a plot but a presentation of the values into a “stem”, that is made of the values that are present across all case and then a “leaf” where the remaing parts of each numbers is shown and accumulated, thus showing a kind of frequency together with the values:\nExamine the case for Age and Age2:\n\nstem(Age)\n#&gt; \n#&gt;   The decimal point is 1 digit(s) to the right of the |\n#&gt; \n#&gt;   2 | 35567778\n#&gt;   3 | 0\n#&gt;   4 | \n#&gt;   5 | 2\nstem(Age2)\n#&gt; \n#&gt;   The decimal point is at the |\n#&gt; \n#&gt;   22 | 0\n#&gt;   24 | 00\n#&gt;   26 | 0000\n#&gt;   28 | 0\n#&gt;   30 | 0\n\n\n\n11.3.3 Histogram\nHistograms are probably the first go to graphic in order to visualize a distribution\nLet’s reuse the x normal variable we created earlier and plot both its boxplot and the histogram\n\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n\nsummary(x)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; -4.127442 -0.668690  0.004522  0.003651  0.682819  4.323841\nboxplot(x, main = \"Boxplot of a random variable following N(0,1) with n = 100,000\")\n\n\n\n\n\n\n\nhist(x)\n\n\n\n\n\n\n\n\nA histogram is more detailed than a boxplot because it shows every data but does not provide a central or dispersion measure. Key to using a histogram is to play with the number of bars, otherwise some information, gaps, or multimodalities may be not be seen. You adapt the number of bars using the option “breaks”\n\npar(mfrow=c(1,2))\nhist(x, breaks=5)\nhist(x, breaks=100)\n\n\n\n\n\n\n\n\nFor a categorical variable (factor), the function plot() gives the counts of each category (level). We have worked an example using Le Tour de France data earlier in the course. It is similar to a visualisation of the table() output and is equivalent to the function hist() for quantitative variables.\n\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4\nplot(score, main = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nFor a numeric variable, a call to plot, shows values along the vertical axis and the index of the rows along the horizontal axis, which is rarely a useful information.\n\nplot(x)",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "033_discretisation.html",
    "href": "033_discretisation.html",
    "title": "12  Discretisation",
    "section": "",
    "text": "Geographers, maybe more than others because they like to produce maps, are often tempted to cut a numerical vector into a set classes.\nA typical use is the cartography of a continuous variable in a set of 5, 6 or 7 groups, knowing the human eye has difficulties to disentangle more colours. GIS and mapping software all have a menu where “discretisation” is made using a number of manually defined limits of preset algorithms, such as “Natural breaks”, “quantiles”, etc.\nIn R, the function cut()` is the base function to divide a range of numeric values x into intervals by a number of break values. It outputs a new factor where each value x is given a level according to which interval they fall in. The breaks are either\n\na scalar greater or equal to 2 for the range to be cut into equal length pieces.\na set of breaks to be used as the upper and lower limits of each discrete category\n\n\nset.seed(101)\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf4&lt;-cut(x, breaks=4)\nhead(xf4)\n#&gt; [1] (94.1,98.4] (98.4,103]  (94.1,98.4] (98.4,103]  (98.4,103]  (103,107]  \n#&gt; Levels: (89.7,94.1] (94.1,98.4] (98.4,103] (103,107]\ntable(xf4)\n#&gt; xf4\n#&gt; (89.7,94.1] (94.1,98.4]  (98.4,103]   (103,107] \n#&gt;           2           5           9           4\n\nNotice how the label clearly indicates the (default) closing on the right of each interval\n\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf6&lt;-cut(x, breaks=c(min(x),-90,95,100,105,110, max(x)))\nhead(xf6)\n#&gt; [1] (95,100]  (100,105] (95,100]  (89.6,95] (100,105] (89.6,95]\n#&gt; Levels: (-90,89.6] (89.6,95] (95,100] (100,105] (105,106] (106,110]\ntable(xf6)\n#&gt; xf6\n#&gt; (-90,89.6]  (89.6,95]   (95,100]  (100,105]  (105,106]  (106,110] \n#&gt;          1          3          4         10          2          0\n\nSince we can choose any breaks, it is pretty easy to adapt and use any discretisation method one would find elsewhere, e.g. in mapping packages.\nThere is a wonderful package, classInt, that does so and where you can simply choose the discretisation methodology.\nLet’s explore!\nhttps://cran.r-project.org/web/packages/classInt/classInt.pdf\nWe refer to the help of the package and specifically the function classIntervals()to find out about the available methods\n\nx_quantile_5&lt;-classInt::classIntervals(x, n=5, style=\"quantile\") #Default style\nx_quantile_5\n#&gt; style: quantile\n#&gt;   one of 3,876 possible partitions of this variable into 5 classes\n#&gt; [89.63447,95.69211) [95.69211,100.2653) [100.2653,102.3357) [102.3357,103.8727) \n#&gt;                   4                   4                   4                   4 \n#&gt; [103.8727,105.9493] \n#&gt;                   4\n\nThe classIntervals() output has its own class and specific plotting method that works with a given colour palette\n\nclass(x_quantile_5)\n#&gt; [1] \"classIntervals\"\nmycolors&lt;-c(\"darkgreen\",\"lightgreen\",\"lightyellow\", \"orange\", \"orangered\")\nplot(x_quantile_5, pal=mycolors)\n\n\n\n\n\n\n\n\nGiven the normality of the distribution, and the use of quantiles, the central class logically needs a smaller range to host the same number of values.\nBelow another split based on standard deviations:\n\nmean(x)\n#&gt; [1] 99.97489\nsd(x)\n#&gt; [1] 4.865144\nx_sd_5&lt;-classInt::classIntervals(x, n=5, style=\"sd\")\nx_sd_5\n#&gt; style: sd\n#&gt;   one of 50,388 possible partitions of this variable into 8 classes\n#&gt;  [87.81203,90.2446)  [90.2446,92.67717) [92.67717,95.10974) [95.10974,97.54231) \n#&gt;                   1                   1                   2                   1 \n#&gt; [97.54231,99.97489) [99.97489,102.4075)   [102.4075,104.84)   [104.84,107.2726] \n#&gt;                   3                   5                   5                   2\nplot(x_sd_5, pal=mycolors)\n\n\n\n\n\n\n\n\nAnd with 7 classes using the “Jenks” method, similar to the one we find within Esri ArcGIS:\n\nx_jenks_7&lt;-classInt::classIntervals(x, n=7, style=\"jenks\")\nx_jenks_7\n#&gt; style: jenks\n#&gt;   one of 27,132 possible partitions of this variable into 7 classes\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2\nplot(x_jenks_7, pal=mycolors)\n\n\n\n\n\n\n\n\nNotice that ahead of plotting classInt expanded the number of colours, which we provided. In fact, 2 would be enough:\n\nplot(x_jenks_7, pal=c(\"yellow\",\"red\"))\n\n\n\n\n\n\n\n\nInterestingly, rather that specifying a number of classes, one could also use the same breaks as a standard boxplot:\n\nx_box&lt;-classInt::classIntervals(x,  style=\"box\")\nx_box\n#&gt; style: box\n#&gt;   one of 11,628 possible partitions of this variable into 6 classes\n#&gt; [89.63447,89.84276) [89.84276,98.08961) [98.08961,101.8191) [101.8191,103.5875) \n#&gt;                   1                   4                   5                   5 \n#&gt; [103.5875,111.8343)      [111.8343,Inf] \n#&gt;                   5                   0\nquantile(x,probs=c(0.25,0.5,0.75))\n#&gt;       25%       50%       75% \n#&gt;  98.08961 101.81905 103.58750\nc(quantile(x,probs=0.25)-1.5*IQR(x),\n  quantile(x,probs=0.75)+1.5*IQR(x))\n#&gt;       25%       75% \n#&gt;  89.84276 111.83435\nplot(x_box, pal=mycolors)\n\n\n\n\n\n\n\n\nOne then retrieves a vector of the categories in which each values fall using findCols(), which we can easily add to a dataframe as a new column, or even a vector of colours for use anywhere else using findColours().\nThis is shown with our Jenks example:\n\nclassInt::findCols(x_jenks_7)\n#&gt;  [1] 4 6 4 2 6 2 5 4 5 5 6 5 7 1 7 3 5 6 2 5\nclassInt::findColours(x_jenks_7, pal=c(\"yellow\",\"red\"))\n#&gt;  [1] \"#FF7F00\" \"#FF2A00\" \"#FF7F00\" \"#FFD400\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt;  [8] \"#FF7F00\" \"#FF5500\" \"#FF5500\" \"#FF2A00\" \"#FF5500\" \"#FF0000\" \"#FFFF00\"\n#&gt; [15] \"#FF0000\" \"#FFAA00\" \"#FF5500\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt; attr(,\"palette\")\n#&gt; [1] \"#FFFF00\" \"#FFD400\" \"#FFAA00\" \"#FF7F00\" \"#FF5500\" \"#FF2A00\" \"#FF0000\"\n#&gt; attr(,\"table\")\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discretisation</span>"
    ]
  },
  {
    "objectID": "034_distributions.html",
    "href": "034_distributions.html",
    "title": "13  Statistical distributions",
    "section": "",
    "text": "13.1 Continuous distributions:",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical distributions</span>"
    ]
  },
  {
    "objectID": "034_distributions.html#continuous-distributions",
    "href": "034_distributions.html#continuous-distributions",
    "title": "13  Statistical distributions",
    "section": "",
    "text": "13.1.1 Uniform distribution\nThe uniform distribution is one of the simplest distribution of a continuous (ratio or interval) variable. Although it is rare in practice that each value along a continuum has the same chance of occurring than all others, it is a base distribution to know of. Each value has the same probability of occurrence, which is a simple case also to show how we can compute density functions, probabilities and cumulative distributions.\nTo generate a uniform a function, we feed the runif() function with a number of observations. All generative distribution are of the form r...(). Like for the other generating functions, the mean is 0 and the standard deviation is 1 by default, which provides us with some control on the range:\n\nset.seed(101)\nu&lt;-runif(1000)\nhist(u)\n\n\n\n\n\n\n\n\nThe histogram show that each value (intervals) is similarly frequent. Rather than/or on top of a histogram, we often plot densities, which is a smoother representation using a local density (kernel density i.e. using a bandwidth instead of the strict silos of the histogram). In R we use the density() function and use its returned values for plotting:\n\ndensity(u)\n#&gt; \n#&gt; Call:\n#&gt;  density.default(x = u)\n#&gt; \n#&gt; Data: u (1000 obs.); Bandwidth 'bw' = 0.06579\n#&gt; \n#&gt;        x                 y           \n#&gt;  Min.   :-0.1970   Min.   :0.001447  \n#&gt;  1st Qu.: 0.1514   1st Qu.:0.375214  \n#&gt;  Median : 0.4998   Median :0.946124  \n#&gt;  Mean   : 0.4998   Mean   :0.716115  \n#&gt;  3rd Qu.: 0.8482   3rd Qu.:1.001957  \n#&gt;  Max.   : 1.1966   Max.   :1.055677\nplot(density(u))\n\n\n\n\n\n\n\n\nWith the uniform distribution, the probability of drawing a number below a given value will be given by the quantile of that particular value. We can see the empirical cumulative distribution function (ecdf) is a straight line where quantiles equal probabilities:\n\necdf(u)\n#&gt; Empirical CDF \n#&gt; Call: ecdf(u)\n#&gt;  x[1:1000] = 0.00037212, 0.0007435, 0.0023788,  ..., 0.99886, 0.99929\nplot(ecdf(u))\n\n\n\n\n\n\n\n\nInstead of simulating, R knows the theoretical functions as well. We can return the probabilities corresponding to some quantile values, using punif() without creating numbers before. We can use such p...() functions to compare the occurrence of an empirical outcome to its probability within a theorized distribution\n\npunif(q=c(0.01,0.25,0.5,0.75,0.99))\n#&gt; [1] 0.01 0.25 0.50 0.75 0.99\npunif(min=50, max=200, q=c(100)) #the probability of drawing a number below 100 knowing the min and max are 50 and 200 and the distribution uniform is 1/3 (the range being 150 and 100 being 50 beyond the max)\n#&gt; [1] 0.3333333\n\nSimilarly to our density plot, the density of a known distribution is obtained from d...(),i.e.\n\ndunif(c(0.01,0.25,0.5,0.75,0.99),min=0, max=1)\n#&gt; [1] 1 1 1 1 1\n\n\n\n13.1.2 Normal distribution\nCentral limit theorem\nnormal (explain +-1 sd +-2 sd)\n\n\n13.1.3 t-Student distribution",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical distributions</span>"
    ]
  },
  {
    "objectID": "034_distributions.html#discrete-distributions",
    "href": "034_distributions.html#discrete-distributions",
    "title": "13  Statistical distributions",
    "section": "13.2 Discrete distributions",
    "text": "13.2 Discrete distributions\n\n13.2.1 Discrete uniform distribution\n\n\n13.2.2 Discrete binomial distribution\nThrowing a coin holds two possible values: Bernoulli Bernoulli to Binomial https://math.stackexchange.com/questions/838107/what-is-the-difference-and-relationship-between-the-binomial-and-bernoulli-distr\n\n\n13.2.3 Discrete Poisson distribution",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical distributions</span>"
    ]
  },
  {
    "objectID": "034_distributions.html#shape-of-distributions",
    "href": "034_distributions.html#shape-of-distributions",
    "title": "13  Statistical distributions",
    "section": "13.3 Shape of distributions:",
    "text": "13.3 Shape of distributions:\n\n13.3.1 Skewness\n\n\n13.3.2 Kurtosis\n\n\n13.3.3 QQ plots\nComparing to a theoretical distribution",
    "crumbs": [
      "Part III - Univariate analysis and distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical distributions</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html",
    "href": "901_copy_paste.html",
    "title": "14  Copy-pasting",
    "section": "",
    "text": "14.1 clipr way\nA simple way to access to input some small data using the clipboard and that should work across all platforms is to use the read_clip() function from the clipr package.\nSuppose you have a series of numbers copied from a series like this one: 11 12 13 14 15 16 or this one: 11, 12, 13, 14, 15, 16\nget to the console and type clipr::read_clip()\nIn this case you will notice the whole set is a single character string entry, which then necessitates a split. See\na&lt;-clipr::read_clip()\na\n#&gt; [1] \"11, 12, 13, 14, 15, 16\"\nstrsplit(a,\", \")\n#&gt;[[1]]\n#&gt;[1] \"11\" \"12\" \"13\" \"14\" \"15\" \"16\"\nHowever, when it comes from a spreadsheet (e.g. open office)\nyou’ll get separate character entries directly:\nclipr::read_clip()\n#&gt; [1] \"0.7226332924\" \"0.5949296139\" \"0.0513909524\"\n#&gt; [4] \"0.2215940265\" \"0.8725634748\" \"0.0032392712\"\n#&gt; [7] \"0.774327883\"  \"0.1773198219\" \"0.1791877889\"\n#&gt; [10] \"0.004243708\"",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html#read.table-way",
    "href": "901_copy_paste.html#read.table-way",
    "title": "14  Copy-pasting",
    "section": "14.2 read.table way",
    "text": "14.2 read.table way\nYou can also use the read/write table approach after saying the clipboard in the source or output. The inconvenience is that the MacOSX and MS Window approach have a slightly different code:\nCopy from spreadsheet, then paste in R using\n\nb &lt;- read.table(pipe(\"pbpaste\"),header = TRUE) #on macOSX \nb &lt;- read.table(\"clipboard\",header = TRUE) #on MS Windows\n\nand similarly to write to a spreadsheet:\n\nb3&lt;-b^3\nwrite.table(b3, pipe(\"pbcopy\"),row.names = FALSE,sep = \"\\t\") #MACOSX\nwrite.table(A3, \"clipboard\",row.names = FALSE,sep = \"\\t\") #MS Windows\n\nThen paste in spreadsheet.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "902_making_data.html",
    "href": "902_making_data.html",
    "title": "15  Making data",
    "section": "",
    "text": "15.1 Scraping a wikipedia table\nExample of the Tour de France winners table from wikipedia, used in the Vectors chapter of the course: Section 6.3.\n#R script to scrape the tour de France winners table from wikipedia page:\n#https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\n#Geoffrey Caruso  Sept 18th 2024\n#\n# Script is adapted from\n# https://help.displayr.com/hc/en-us/articles/360003582875-How-to-Import-a-Wikipedia-Table-using-R\n# \n# Since there are different tables in Tour de France page and the one of interest is a sortable one,\n# I have adapted following suggestions in\n# \n# https://stackoverflow.com/questions/72380279/how-to-scrape-with-table-class-name-with-r\n# \n# I still had to look into the source of the table to find out what table class this is\n# In this case it was a \"wikitable plainrowheaders sortable'\n\nLeTour_url&lt;-\"https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\"\nLeTour_Page&lt;-rvest::read_html(LeTour_url)\nLeTour_Table&lt;-rvest::html_node(LeTour_Page, xpath=\"//table[@class='wikitable plainrowheaders sortable']\")\nLeTour_Tibble = rvest::html_table(LeTour_Table, fill = TRUE)\nLeTour_df&lt;-as.data.frame(LeTour_Tibble)\n\nsaveRDS(LeTour_df,\"data/TourDeFrance/LeTour_df.rds\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "href": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "title": "15  Making data",
    "section": "15.2 Scotland rain and elevation (by Ferguson)",
    "text": "15.2 Scotland rain and elevation (by Ferguson)\n\n#Ferguson Rob,\n#linear regression in geography, CATMOG 15. https://github.com/qmrg/CATMOG/blob/Main/15-linear-regression-in-geography.pdf\n\n#Data from Table 1 and 2, p8:\n#Average precipitation and elevation across southern Scotland\n#\n#Source caption: British Rainfall (HMSO),\n#selected raingauges between national grid lines 600 and 601 km N.\n#Sites are in West-East order\n#Elevation in m above OD\n#Rainfall in mm/yr\n#DistanceE in km from W coast\n#\nRainScotland&lt;-data.frame(\n  SiteNo=1:20,\n  Elevation=c(240,430,420,470,300,150,520,460,300,410,\n              140,540,280,240,200,210,160,270,320,230),\n  Rainfall=c(1720,2320,2050,1870,1690,1250,2130,2090, 1730,2040,\n             1460,1860,1670,1580,1490,1420,900,1250,1170,1170),\n  DistanceE=c(37,43,48,49,52,59,73,75,76,77,\n              86,97,100,103,104,114,138,152,153,154)\n    )\nwrite.csv(RainScotland, \"data/Ferguson/RainScotland.csv\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "903_summarizing.html",
    "href": "903_summarizing.html",
    "title": "Summary tables",
    "section": "",
    "text": "Descriptive statistics with summarytools\nTake the RainScotland dataset as a first example and use the dfSummary() function that provides a rather comprehensive set of statistics for entire dataframes:\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nsummarytools::dfSummary(RainScotland)\n#&gt; Data Frame Summary  \n#&gt; RainScotland  \n#&gt; Dimensions: 20 x 5  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------\n#&gt; No   Variable    Stats / Values              Freqs (% of Valid)   Graph               Valid      Missing  \n#&gt; ---- ----------- --------------------------- -------------------- ------------------- ---------- ---------\n#&gt; 1    X           Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 2    SiteNo      Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 3    Elevation   Mean (sd) : 314.5 (125.5)   18 distinct values       : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                     : :     :       (100.0%)   (0.0%)   \n#&gt;                  140 &lt; 290 &lt; 540                                  . . : :     : . .                       \n#&gt;                  IQR (CV) : 197.5 (0.4)                           : : : :     : : :                       \n#&gt;                                                                   : : : : :   : : :                       \n#&gt; \n#&gt; 4    Rainfall    Mean (sd) : 1643 (380.6)    18 distinct values         : :   :       20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                       : :   :       (100.0%)   (0.0%)   \n#&gt;                  900 &lt; 1680 &lt; 2320                                  . . : : . :                           \n#&gt;                  IQR (CV) : 535 (0.2)                               : : : : : :                           \n#&gt;                                                                   : : : : : : : :                         \n#&gt; \n#&gt; 5    DistanceE   Mean (sd) : 89.5 (37.7)     20 distinct values     :                 20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                   : :               (100.0%)   (0.0%)   \n#&gt;                  37 &lt; 81.5 &lt; 154                                    : : : :   :                           \n#&gt;                  IQR (CV) : 49.2 (0.4)                              : : : :   :                           \n#&gt;                                                                   : : : : : : :                           \n#&gt; ----------------------------------------------------------------------------------------------------------\nAn nice feature is that it can generate its own view (beware: view not View(), as an html file for display in any browser:\nsummarytools::view(summarytools::dfSummary(RainScotland), file = \"output/RainScotland.html\")\nIf we would a more complex dataset (including factors) such as the wikipedia table of the Tour de France winners, we would have:\nTDF&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nsummarytools::dfSummary(TDF)\n#&gt; Data Frame Summary  \n#&gt; TDF  \n#&gt; Dimensions: 122 x 8  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------------\n#&gt; No   Variable       Stats / Values                 Freqs (% of Valid)    Graph              Valid      Missing  \n#&gt; ---- -------------- ------------------------------ --------------------- ------------------ ---------- ---------\n#&gt; 1    Year           Mean (sd) : 1963.5 (35.4)      122 distinct values   . : : : : :        122        0        \n#&gt;      [integer]      min &lt; med &lt; max:               (Integer sequence)    : : : : : :        (100.0%)   (0.0%)   \n#&gt;                     1903 &lt; 1963.5 &lt; 2024                                 : : : : : :                            \n#&gt;                     IQR (CV) : 60.5 (0)                                  : : : : : :                            \n#&gt;                                                                          : : : : : : :                          \n#&gt; \n#&gt; 2    Country        1. France                      36 (29.5%)            IIIII              122        0        \n#&gt;      [character]    2. —                           18 (14.8%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Belgium                     18 (14.8%)            II                                     \n#&gt;                     4. Spain                       12 ( 9.8%)            I                                      \n#&gt;                     5. Italy                       10 ( 8.2%)            I                                      \n#&gt;                     6. Great Britain                6 ( 4.9%)                                                   \n#&gt;                     7. Luxembourg                   5 ( 4.1%)                                                   \n#&gt;                     8. Denmark                      3 ( 2.5%)                                                   \n#&gt;                     9. Slovenia                     3 ( 2.5%)                                                   \n#&gt;                     10. United States               3 ( 2.5%)                                                   \n#&gt;                     [ 6 others ]                    8 ( 6.6%)            I                                      \n#&gt; \n#&gt; 3    Cyclist        1. ~Not contested due to Wor    7 ( 5.7%)            I                  122        0        \n#&gt;      [character]    2. No winner[c]                 7 ( 5.7%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. Jacques Anquetil             5 ( 4.1%)                                                   \n#&gt;                     4. Miguel Indurain              5 ( 4.1%)                                                   \n#&gt;                     5. ~Not contested due to Wor    4 ( 3.3%)                                                   \n#&gt;                     6. Bernard Hinault              4 ( 3.3%)                                                   \n#&gt;                     7. Chris Froome                 3 ( 2.5%)                                                   \n#&gt;                     8. Greg LeMond                  3 ( 2.5%)                                                   \n#&gt;                     9. Louison Bobet                3 ( 2.5%)                                                   \n#&gt;                     10. Philippe Thys               3 ( 2.5%)                                                   \n#&gt;                     [ 67 others ]                  78 (63.9%)            IIIIIIIIIIII                           \n#&gt; \n#&gt; 4    Sponsor/Team   1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. France                      13 (10.7%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Alcyon–Dunlop                7 ( 5.7%)            I                                      \n#&gt;                     4. Peugeot–Wolber               7 ( 5.7%)            I                                      \n#&gt;                     5. Team Sky                     6 ( 4.9%)                                                   \n#&gt;                     6. Banesto                      5 ( 4.1%)                                                   \n#&gt;                     7. Italy                        5 ( 4.1%)                                                   \n#&gt;                     8. Automoto–Hutchinson          3 ( 2.5%)                                                   \n#&gt;                     9. Belgium                      3 ( 2.5%)                                                   \n#&gt;                     10. La Sportive                 3 ( 2.5%)                                                   \n#&gt;                     [ 40 others ]                  52 (42.6%)            IIIIIIII                               \n#&gt; \n#&gt; 5    Distance       1. —                           11 ( 9.0%)            I                  122        0        \n#&gt;      [character]    2. 2,428 km (1,509 mi)          2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 3,765 km (2,339 mi)          2 ( 1.6%)                                                   \n#&gt;                     4. 4,498 km (2,795 mi)          2 ( 1.6%)                                                   \n#&gt;                     5. 2,994 km (1,860 mi)          1 ( 0.8%)                                                   \n#&gt;                     6. 3,278 km (2,037 mi)          1 ( 0.8%)                                                   \n#&gt;                     7. 3,285 km (2,041 mi)          1 ( 0.8%)                                                   \n#&gt;                     8. 3,286 km (2,042 mi)          1 ( 0.8%)                                                   \n#&gt;                     9. 3,328 km (2,068 mi)          1 ( 0.8%)                                                   \n#&gt;                     10. 3,349 km (2,081 mi)         1 ( 0.8%)                                                   \n#&gt;                     [ 99 others ]                  99 (81.1%)            IIIIIIIIIIIIIIII                       \n#&gt; \n#&gt; 6    Time/Points    1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 100h 30′ 35″                 1 ( 0.8%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 100h 49′ 30″                 1 ( 0.8%)                                                   \n#&gt;                     4. 101h 01′ 20″                 1 ( 0.8%)                                                   \n#&gt;                     5. 103h 06′ 50″                 1 ( 0.8%)                                                   \n#&gt;                     6. 103h 38′ 38″                 1 ( 0.8%)                                                   \n#&gt;                     7. 105h 07′ 52″                 1 ( 0.8%)                                                   \n#&gt;                     8. 108h 17′ 18″                 1 ( 0.8%)                                                   \n#&gt;                     9. 108h 18′ 00″                 1 ( 0.8%)                                                   \n#&gt;                     10. 109h 19′ 14″                1 ( 0.8%)                                                   \n#&gt;                     [ 95 others ]                  95 (77.9%)            IIIIIIIIIIIIIII                        \n#&gt; \n#&gt; 7    Margin         1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. + 3′ 10″                     2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. + 3′ 21″                     2 ( 1.6%)                                                   \n#&gt;                     4. + 4′ 01″                     2 ( 1.6%)                                                   \n#&gt;                     5. + 4′ 35″                     2 ( 1.6%)                                                   \n#&gt;                     6. + 4′ 59″                     2 ( 1.6%)                                                   \n#&gt;                     7. + 1′ 07″                     1 ( 0.8%)                                                   \n#&gt;                     8. + 1′ 11″                     1 ( 0.8%)                                                   \n#&gt;                     9. + 1′ 12″                     1 ( 0.8%)                                                   \n#&gt;                     10. + 1′ 22″                    1 ( 0.8%)                                                   \n#&gt;                     [ 90 others ]                  90 (73.8%)            IIIIIIIIIIIIII                         \n#&gt; \n#&gt; 8    Stage wins     1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 0                            8 ( 6.6%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. 1                           20 (16.4%)            III                                    \n#&gt;                     4. 2                           27 (22.1%)            IIII                                   \n#&gt;                     5. 3                           19 (15.6%)            III                                    \n#&gt;                     6. 4                           12 ( 9.8%)            I                                      \n#&gt;                     7. 5                            8 ( 6.6%)            I                                      \n#&gt;                     8. 6                            6 ( 4.9%)                                                   \n#&gt;                     9. 7                            2 ( 1.6%)                                                   \n#&gt;                     10. 8                           2 ( 1.6%)                                                   \n#&gt; ----------------------------------------------------------------------------------------------------------------\nsummarytools::view(summarytools::dfSummary(TDF), file = \"output/LeTour.html\")\nA less visual but even more complete set (including range and indicators of the shape of the distributions) can be obtained with the descr()function.\nsummarytools::descr(RainScotland)\n#&gt; Descriptive Statistics  \n#&gt; RainScotland  \n#&gt; N: 20  \n#&gt; \n#&gt;                     DistanceE   Elevation   Rainfall   SiteNo        X\n#&gt; ----------------- ----------- ----------- ---------- -------- --------\n#&gt;              Mean       89.50      314.50    1643.00    10.50    10.50\n#&gt;           Std.Dev       37.74      125.51     380.62     5.92     5.92\n#&gt;               Min       37.00      140.00     900.00     1.00     1.00\n#&gt;                Q1       55.50      220.00    1335.00     5.50     5.50\n#&gt;            Median       81.50      290.00    1680.00    10.50    10.50\n#&gt;                Q3      109.00      425.00    1955.00    15.50    15.50\n#&gt;               Max      154.00      540.00    2320.00    20.00    20.00\n#&gt;               MAD       38.55      155.67     459.61     7.41     7.41\n#&gt;               IQR       49.25      197.50     535.00     9.50     9.50\n#&gt;                CV        0.42        0.40       0.23     0.56     0.56\n#&gt;          Skewness        0.40        0.33      -0.09     0.00     0.00\n#&gt;       SE.Skewness        0.51        0.51       0.51     0.51     0.51\n#&gt;          Kurtosis       -1.13       -1.30      -1.05    -1.38    -1.38\n#&gt;           N.Valid       20.00       20.00      20.00    20.00    20.00\n#&gt;         Pct.Valid      100.00      100.00     100.00   100.00   100.00\nIn the case we apply it to the Tour de France, only the year is available as a numeric, which is not quite interesting because there is only one Tour per year, thus providing basically no useful information.\nsummarytools::descr(TDF)\n#&gt; Non-numerical variable(s) ignored: Country, Cyclist, Sponsor/Team, Distance, Time/Points, Margin, Stage wins\n#&gt; Descriptive Statistics  \n#&gt; TDF$Year  \n#&gt; N: 122  \n#&gt; \n#&gt;                        Year\n#&gt; ----------------- ---------\n#&gt;              Mean   1963.50\n#&gt;           Std.Dev     35.36\n#&gt;               Min   1903.00\n#&gt;                Q1   1933.00\n#&gt;            Median   1963.50\n#&gt;                Q3   1994.00\n#&gt;               Max   2024.00\n#&gt;               MAD     45.22\n#&gt;               IQR     60.50\n#&gt;                CV      0.02\n#&gt;          Skewness      0.00\n#&gt;       SE.Skewness      0.22\n#&gt;          Kurtosis     -1.23\n#&gt;           N.Valid    122.00\n#&gt;         Pct.Valid    100.00\nThe specific function freq() is rather to be used here. Given most variables are categorical, we are interested in counts and percentages:\nsummarytools::freq(TDF)\n#&gt; Variable(s) ignored: Year\n#&gt; Frequencies  \n#&gt; TDF$Country  \n#&gt; Type: Character  \n#&gt; \n#&gt;                       Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------- ------ --------- -------------- --------- --------------\n#&gt;                   —     18     14.75          14.75     14.75          14.75\n#&gt;           Australia      1      0.82          15.57      0.82          15.57\n#&gt;             Belgium     18     14.75          30.33     14.75          30.33\n#&gt;            Colombia      1      0.82          31.15      0.82          31.15\n#&gt;             Denmark      3      2.46          33.61      2.46          33.61\n#&gt;              France     36     29.51          63.11     29.51          63.11\n#&gt;             Germany      1      0.82          63.93      0.82          63.93\n#&gt;       Great Britain      6      4.92          68.85      4.92          68.85\n#&gt;             Ireland      1      0.82          69.67      0.82          69.67\n#&gt;               Italy     10      8.20          77.87      8.20          77.87\n#&gt;          Luxembourg      5      4.10          81.97      4.10          81.97\n#&gt;         Netherlands      2      1.64          83.61      1.64          83.61\n#&gt;            Slovenia      3      2.46          86.07      2.46          86.07\n#&gt;               Spain     12      9.84          95.90      9.84          95.90\n#&gt;         Switzerland      2      1.64          97.54      1.64          97.54\n#&gt;       United States      3      2.46         100.00      2.46         100.00\n#&gt;                &lt;NA&gt;      0                               0.00         100.00\n#&gt;               Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Cyclist  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                            Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ---------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;        ~Not contested due to World War I      4      3.28           3.28      3.28           3.28\n#&gt;       ~Not contested due to World War II      7      5.74           9.02      5.74           9.02\n#&gt;                         Alberto Contador      1      0.82           9.84      0.82           9.84\n#&gt;                        Alberto Contador#      1      0.82          10.66      0.82          10.66\n#&gt;                             André Leducq      2      1.64          12.30      1.64          12.30\n#&gt;                         Andy Schleck#[e]      1      0.82          13.11      0.82          13.11\n#&gt;                            Antonin Magne      2      1.64          14.75      1.64          14.75\n#&gt;                          Bernard Hinault      4      3.28          18.03      3.28          18.03\n#&gt;                         Bernard Hinault†      1      0.82          18.85      0.82          18.85\n#&gt;                         Bernard Thévenet      2      1.64          20.49      1.64          20.49\n#&gt;                           Bjarne Riis[b]      1      0.82          21.31      0.82          21.31\n#&gt;                          Bradley Wiggins      1      0.82          22.13      0.82          22.13\n#&gt;                              Cadel Evans      1      0.82          22.95      0.82          22.95\n#&gt;                            Carlos Sastre      1      0.82          23.77      0.82          23.77\n#&gt;                              Charly Gaul      1      0.82          24.59      0.82          24.59\n#&gt;                             Chris Froome      3      2.46          27.05      2.46          27.05\n#&gt;                             Chris Froome      1      0.82          27.87      0.82          27.87\n#&gt;                              Eddy Merckx      1      0.82          28.69      0.82          28.69\n#&gt;                              Eddy Merckx      1      0.82          29.51      0.82          29.51\n#&gt;                             Eddy Merckx†      2      1.64          31.15      1.64          31.15\n#&gt;                             Eddy Merckx‡      1      0.82          31.97      0.82          31.97\n#&gt;                             Egan Bernal#      1      0.82          32.79      0.82          32.79\n#&gt;                             Fausto Coppi      2      1.64          34.43      1.64          34.43\n#&gt;                      Federico Bahamontes      1      0.82          35.25      0.82          35.25\n#&gt;                           Felice Gimondi      1      0.82          36.07      0.82          36.07\n#&gt;                         Ferdinand Kübler      1      0.82          36.89      0.82          36.89\n#&gt;                            Firmin Lambot      2      1.64          38.52      1.64          38.52\n#&gt;                           François Faber      1      0.82          39.34      0.82          39.34\n#&gt;                          Gastone Nencini      1      0.82          40.16      0.82          40.16\n#&gt;                         Georges Speicher      1      0.82          40.98      0.82          40.98\n#&gt;                           Geraint Thomas      1      0.82          41.80      0.82          41.80\n#&gt;                             Gino Bartali      2      1.64          43.44      1.64          43.44\n#&gt;                              Greg LeMond      3      2.46          45.90      2.46          45.90\n#&gt;                         Gustave Garrigou      1      0.82          46.72      0.82          46.72\n#&gt;                          Henri Cornet[a]      1      0.82          47.54      0.82          47.54\n#&gt;                          Henri Pélissier      1      0.82          48.36      0.82          48.36\n#&gt;                              Hugo Koblet      1      0.82          49.18      0.82          49.18\n#&gt;                         Jacques Anquetil      5      4.10          53.28      4.10          53.28\n#&gt;                              Jan Janssen      1      0.82          54.10      0.82          54.10\n#&gt;                             Jan Ullrich#      1      0.82          54.92      0.82          54.92\n#&gt;                               Jean Robic      1      0.82          55.74      0.82          55.74\n#&gt;                         Jonas Vingegaard      1      0.82          56.56      0.82          56.56\n#&gt;                         Jonas Vingegaard      1      0.82          57.38      0.82          57.38\n#&gt;                           Joop Zoetemelk      1      0.82          58.20      0.82          58.20\n#&gt;                           Laurent Fignon      1      0.82          59.02      0.82          59.02\n#&gt;                          Laurent Fignon#      1      0.82          59.84      0.82          59.84\n#&gt;                              Léon Scieur      1      0.82          60.66      0.82          60.66\n#&gt;                        Louis Trousselier      1      0.82          61.48      0.82          61.48\n#&gt;                            Louison Bobet      3      2.46          63.93      2.46          63.93\n#&gt;                             Lucien Aimar      1      0.82          64.75      0.82          64.75\n#&gt;                            Lucien Buysse      1      0.82          65.57      0.82          65.57\n#&gt;                      Lucien Petit-Breton      2      1.64          67.21      1.64          67.21\n#&gt;                          Lucien Van Impe      1      0.82          68.03      0.82          68.03\n#&gt;                               Luis Ocaña      1      0.82          68.85      0.82          68.85\n#&gt;                            Marco Pantani      1      0.82          69.67      0.82          69.67\n#&gt;                         Maurice De Waele      1      0.82          70.49      0.82          70.49\n#&gt;                            Maurice Garin      1      0.82          71.31      0.82          71.31\n#&gt;                          Miguel Indurain      5      4.10          75.41      4.10          75.41\n#&gt;                           Nicolas Frantz      2      1.64          77.05      1.64          77.05\n#&gt;                             No winner[c]      7      5.74          82.79      5.74          82.79\n#&gt;                            Octave Lapize      1      0.82          83.61      0.82          83.61\n#&gt;                            Odile Defraye      1      0.82          84.43      0.82          84.43\n#&gt;                         Óscar Pereiro[d]      1      0.82          85.25      0.82          85.25\n#&gt;                       Ottavio Bottecchia      2      1.64          86.89      1.64          86.89\n#&gt;                            Pedro Delgado      1      0.82          87.70      0.82          87.70\n#&gt;                            Philippe Thys      3      2.46          90.16      2.46          90.16\n#&gt;                             René Pottier      1      0.82          90.98      0.82          90.98\n#&gt;                            Roger Lapébie      1      0.82          91.80      0.82          91.80\n#&gt;                            Roger Pingeon      1      0.82          92.62      0.82          92.62\n#&gt;                          Roger Walkowiak      1      0.82          93.44      0.82          93.44\n#&gt;                              Romain Maes      1      0.82          94.26      0.82          94.26\n#&gt;                            Stephen Roche      1      0.82          95.08      0.82          95.08\n#&gt;                             Sylvère Maes      1      0.82          95.90      0.82          95.90\n#&gt;                             Sylvère Maes      1      0.82          96.72      0.82          96.72\n#&gt;                            Tadej Pogačar      1      0.82          97.54      0.82          97.54\n#&gt;                           Tadej Pogačar§      2      1.64          99.18      1.64          99.18\n#&gt;                          Vincenzo Nibali      1      0.82         100.00      0.82         100.00\n#&gt;                                     &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                    Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Sponsor/Team  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                           Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; --------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;                                       —     18     14.75          14.75     14.75          14.75\n#&gt;             AD Renting–W-Cup–Bottecchia      1      0.82          15.57      0.82          15.57\n#&gt;                           Alcyon–Dunlop      7      5.74          21.31      5.74          21.31\n#&gt;                                  Astana      2      1.64          22.95      1.64          22.95\n#&gt;                                Automoto      1      0.82          23.77      0.82          23.77\n#&gt;                     Automoto–Hutchinson      3      2.46          26.23      2.46          26.23\n#&gt;                                 Banesto      5      4.10          30.33      4.10          30.33\n#&gt;                                 Belgium      3      2.46          32.79      2.46          32.79\n#&gt;                                     Bic      1      0.82          33.61      0.82          33.61\n#&gt;                         BMC Racing Team      1      0.82          34.43      0.82          34.43\n#&gt;          Caisse d'Epargne–Illes Balears      1      0.82          35.25      0.82          35.25\n#&gt;                  Carrera Jeans–Vagabond      1      0.82          36.07      0.82          36.07\n#&gt;                                   Conte      1      0.82          36.89      0.82          36.89\n#&gt;                       Discovery Channel      1      0.82          37.70      0.82          37.70\n#&gt;                                   Faema      1      0.82          38.52      0.82          38.52\n#&gt;                           Faemino–Faema      1      0.82          39.34      0.82          39.34\n#&gt;                  Ford France–Hutchinson      1      0.82          40.16      0.82          40.16\n#&gt;                                  France     13     10.66          50.82     10.66          50.82\n#&gt;                       Gitane–Campagnolo      1      0.82          51.64      0.82          51.64\n#&gt;                                   Italy      5      4.10          55.74      4.10          55.74\n#&gt;                            La Française      1      0.82          56.56      0.82          56.56\n#&gt;                             La Sportive      3      2.46          59.02      2.46          59.02\n#&gt;                           La Vie Claire      2      1.64          60.66      1.64          60.66\n#&gt;                              Luxembourg      1      0.82          61.48      0.82          61.48\n#&gt;                   Mercatone Uno–Bianchi      1      0.82          62.30      0.82          62.30\n#&gt;                                 Molteni      3      2.46          64.75      2.46          64.75\n#&gt;                Pelforth–Sauvage–Lejeune      1      0.82          65.57      0.82          65.57\n#&gt;                     Peugeot–BP–Michelin      2      1.64          67.21      1.64          67.21\n#&gt;                   Peugeot–Esso–Michelin      1      0.82          68.03      0.82          68.03\n#&gt;                          Peugeot–Wolber      7      5.74          73.77      5.74          73.77\n#&gt;                             Renault–Elf      2      1.64          75.41      1.64          75.41\n#&gt;                      Renault–Elf–Gitane      2      1.64          77.05      1.64          77.05\n#&gt;                          Renault–Gitane      1      0.82          77.87      0.82          77.87\n#&gt;               Renault–Gitane–Campagnolo      1      0.82          78.69      0.82          78.69\n#&gt;                                Reynolds      1      0.82          79.51      0.82          79.51\n#&gt;             Saint-Raphaël–Gitane–Dunlop      1      0.82          80.33      0.82          80.33\n#&gt;       Saint-Raphaël–Gitane–R. Geminiani      1      0.82          81.15      0.82          81.15\n#&gt;        Saint-Raphaël–Helyett–Hutchinson      1      0.82          81.97      0.82          81.97\n#&gt;                               Salvarani      1      0.82          82.79      0.82          82.79\n#&gt;                                   Spain      1      0.82          83.61      0.82          83.61\n#&gt;                             Switzerland      2      1.64          85.25      1.64          85.25\n#&gt;                                Team CSC      1      0.82          86.07      0.82          86.07\n#&gt;                              Team Ineos      1      0.82          86.89      0.82          86.89\n#&gt;                        Team Jumbo–Visma      2      1.64          88.52      1.64          88.52\n#&gt;                          Team Saxo Bank      1      0.82          89.34      0.82          89.34\n#&gt;                                Team Sky      6      4.92          94.26      4.92          94.26\n#&gt;                            Team Telekom      2      1.64          95.90      1.64          95.90\n#&gt;                        TI–Raleigh–Creda      1      0.82          96.72      0.82          96.72\n#&gt;                       UAE Team Emirates      3      2.46          99.18      2.46          99.18\n#&gt;                               Z–Tomasso      1      0.82         100.00      0.82         100.00\n#&gt;                                    &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                   Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Distance  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                 Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------------------------- ------ --------- -------------- --------- --------------\n#&gt;                             —     11      9.02           9.02      9.02           9.02\n#&gt;           2,428 km (1,509 mi)      2      1.64          10.66      1.64          10.66\n#&gt;           2,994 km (1,860 mi)      1      0.82          11.48      0.82          11.48\n#&gt;           3,278 km (2,037 mi)      1      0.82          12.30      0.82          12.30\n#&gt;           3,285 km (2,041 mi)      1      0.82          13.11      0.82          13.11\n#&gt;           3,286 km (2,042 mi)      1      0.82          13.93      0.82          13.93\n#&gt;           3,328 km (2,068 mi)      1      0.82          14.75      0.82          14.75\n#&gt;           3,349 km (2,081 mi)      1      0.82          15.57      0.82          15.57\n#&gt;           3,359 km (2,087 mi)      1      0.82          16.39      0.82          16.39\n#&gt;       3,360.3 km (2,088.0 mi)      1      0.82          17.21      0.82          17.21\n#&gt;           3,366 km (2,092 mi)      1      0.82          18.03      0.82          18.03\n#&gt;           3,391 km (2,107 mi)      1      0.82          18.85      0.82          18.85\n#&gt;           3,404 km (2,115 mi)      1      0.82          19.67      0.82          19.67\n#&gt;           3,406 km (2,116 mi)      1      0.82          20.49      0.82          20.49\n#&gt;       3,414.4 km (2,121.6 mi)      1      0.82          21.31      0.82          21.31\n#&gt;           3,427 km (2,129 mi)      1      0.82          22.13      0.82          22.13\n#&gt;           3,430 km (2,130 mi)      1      0.82          22.95      0.82          22.95\n#&gt;           3,458 km (2,149 mi)      1      0.82          23.77      0.82          23.77\n#&gt;           3,459 km (2,149 mi)      1      0.82          24.59      0.82          24.59\n#&gt;           3,484 km (2,165 mi)      1      0.82          25.41      0.82          25.41\n#&gt;           3,496 km (2,172 mi)      1      0.82          26.23      0.82          26.23\n#&gt;           3,498 km (2,174 mi)      1      0.82          27.05      0.82          27.05\n#&gt;           3,504 km (2,177 mi)      1      0.82          27.87      0.82          27.87\n#&gt;           3,507 km (2,179 mi)      1      0.82          28.69      0.82          28.69\n#&gt;           3,529 km (2,193 mi)      1      0.82          29.51      0.82          29.51\n#&gt;           3,540 km (2,200 mi)      1      0.82          30.33      0.82          30.33\n#&gt;           3,559 km (2,211 mi)      1      0.82          31.15      0.82          31.15\n#&gt;           3,570 km (2,220 mi)      1      0.82          31.97      0.82          31.97\n#&gt;           3,608 km (2,242 mi)      1      0.82          32.79      0.82          32.79\n#&gt;           3,635 km (2,259 mi)      1      0.82          33.61      0.82          33.61\n#&gt;           3,642 km (2,263 mi)      1      0.82          34.43      0.82          34.43\n#&gt;           3,657 km (2,272 mi)      1      0.82          35.25      0.82          35.25\n#&gt;       3,660.5 km (2,274.5 mi)      1      0.82          36.07      0.82          36.07\n#&gt;           3,662 km (2,275 mi)      1      0.82          36.89      0.82          36.89\n#&gt;           3,687 km (2,291 mi)      1      0.82          37.70      0.82          37.70\n#&gt;           3,714 km (2,308 mi)      1      0.82          38.52      0.82          38.52\n#&gt;           3,753 km (2,332 mi)      1      0.82          39.34      0.82          39.34\n#&gt;           3,765 km (2,339 mi)      2      1.64          40.98      1.64          40.98\n#&gt;           3,809 km (2,367 mi)      1      0.82          41.80      0.82          41.80\n#&gt;           3,842 km (2,387 mi)      1      0.82          42.62      0.82          42.62\n#&gt;           3,846 km (2,390 mi)      1      0.82          43.44      0.82          43.44\n#&gt;           3,875 km (2,408 mi)      1      0.82          44.26      0.82          44.26\n#&gt;           3,908 km (2,428 mi)      1      0.82          45.08      0.82          45.08\n#&gt;           3,914 km (2,432 mi)      1      0.82          45.90      0.82          45.90\n#&gt;           3,950 km (2,450 mi)      1      0.82          46.72      0.82          46.72\n#&gt;           3,978 km (2,472 mi)      1      0.82          47.54      0.82          47.54\n#&gt;           3,983 km (2,475 mi)      1      0.82          48.36      0.82          48.36\n#&gt;           4,000 km (2,500 mi)      1      0.82          49.18      0.82          49.18\n#&gt;           4,017 km (2,496 mi)      1      0.82          50.00      0.82          50.00\n#&gt;           4,021 km (2,499 mi)      1      0.82          50.82      0.82          50.82\n#&gt;           4,090 km (2,540 mi)      1      0.82          51.64      0.82          51.64\n#&gt;           4,094 km (2,544 mi)      1      0.82          52.46      0.82          52.46\n#&gt;           4,096 km (2,545 mi)      1      0.82          53.28      0.82          53.28\n#&gt;           4,098 km (2,546 mi)      1      0.82          54.10      0.82          54.10\n#&gt;           4,109 km (2,553 mi)      1      0.82          54.92      0.82          54.92\n#&gt;           4,117 km (2,558 mi)      1      0.82          55.74      0.82          55.74\n#&gt;           4,138 km (2,571 mi)      1      0.82          56.56      0.82          56.56\n#&gt;           4,173 km (2,593 mi)      1      0.82          57.38      0.82          57.38\n#&gt;           4,188 km (2,602 mi)      1      0.82          58.20      0.82          58.20\n#&gt;           4,224 km (2,625 mi)      1      0.82          59.02      0.82          59.02\n#&gt;           4,231 km (2,629 mi)      1      0.82          59.84      0.82          59.84\n#&gt;           4,254 km (2,643 mi)      1      0.82          60.66      0.82          60.66\n#&gt;           4,274 km (2,656 mi)      1      0.82          61.48      0.82          61.48\n#&gt;           4,319 km (2,684 mi)      1      0.82          62.30      0.82          62.30\n#&gt;           4,329 km (2,690 mi)      1      0.82          63.11      0.82          63.11\n#&gt;           4,338 km (2,696 mi)      1      0.82          63.93      0.82          63.93\n#&gt;           4,358 km (2,708 mi)      1      0.82          64.75      0.82          64.75\n#&gt;           4,395 km (2,731 mi)      1      0.82          65.57      0.82          65.57\n#&gt;           4,397 km (2,732 mi)      1      0.82          66.39      0.82          66.39\n#&gt;           4,415 km (2,743 mi)      1      0.82          67.21      0.82          67.21\n#&gt;           4,442 km (2,760 mi)      1      0.82          68.03      0.82          68.03\n#&gt;           4,470 km (2,780 mi)      1      0.82          68.85      0.82          68.85\n#&gt;           4,476 km (2,781 mi)      1      0.82          69.67      0.82          69.67\n#&gt;           4,479 km (2,783 mi)      1      0.82          70.49      0.82          70.49\n#&gt;           4,488 km (2,789 mi)      1      0.82          71.31      0.82          71.31\n#&gt;           4,492 km (2,791 mi)      1      0.82          72.13      0.82          72.13\n#&gt;           4,495 km (2,793 mi)      1      0.82          72.95      0.82          72.95\n#&gt;           4,497 km (2,794 mi)      1      0.82          73.77      0.82          73.77\n#&gt;           4,498 km (2,795 mi)      2      1.64          75.41      1.64          75.41\n#&gt;           4,504 km (2,799 mi)      1      0.82          76.23      0.82          76.23\n#&gt;           4,637 km (2,881 mi)      1      0.82          77.05      0.82          77.05\n#&gt;           4,642 km (2,884 mi)      1      0.82          77.87      0.82          77.87\n#&gt;           4,656 km (2,893 mi)      1      0.82          78.69      0.82          78.69\n#&gt;           4,669 km (2,901 mi)      1      0.82          79.51      0.82          79.51\n#&gt;           4,690 km (2,910 mi)      1      0.82          80.33      0.82          80.33\n#&gt;           4,694 km (2,917 mi)      1      0.82          81.15      0.82          81.15\n#&gt;           4,734 km (2,942 mi)      1      0.82          81.97      0.82          81.97\n#&gt;           4,773 km (2,966 mi)      1      0.82          82.79      0.82          82.79\n#&gt;           4,779 km (2,970 mi)      1      0.82          83.61      0.82          83.61\n#&gt;           4,808 km (2,988 mi)      1      0.82          84.43      0.82          84.43\n#&gt;           4,822 km (2,996 mi)      1      0.82          85.25      0.82          85.25\n#&gt;           4,898 km (3,043 mi)      1      0.82          86.07      0.82          86.07\n#&gt;           4,922 km (3,058 mi)      1      0.82          86.89      0.82          86.89\n#&gt;           5,091 km (3,163 mi)      1      0.82          87.70      0.82          87.70\n#&gt;           5,286 km (3,285 mi)      1      0.82          88.52      0.82          88.52\n#&gt;           5,287 km (3,285 mi)      1      0.82          89.34      0.82          89.34\n#&gt;           5,289 km (3,286 mi)      1      0.82          90.16      0.82          90.16\n#&gt;           5,343 km (3,320 mi)      1      0.82          90.98      0.82          90.98\n#&gt;           5,375 km (3,340 mi)      1      0.82          91.80      0.82          91.80\n#&gt;           5,380 km (3,340 mi)      1      0.82          92.62      0.82          92.62\n#&gt;           5,386 km (3,347 mi)      1      0.82          93.44      0.82          93.44\n#&gt;           5,398 km (3,354 mi)      1      0.82          94.26      0.82          94.26\n#&gt;           5,425 km (3,371 mi)      1      0.82          95.08      0.82          95.08\n#&gt;           5,440 km (3,380 mi)      1      0.82          95.90      0.82          95.90\n#&gt;           5,476 km (3,403 mi)      1      0.82          96.72      0.82          96.72\n#&gt;           5,485 km (3,408 mi)      1      0.82          97.54      0.82          97.54\n#&gt;           5,503 km (3,419 mi)      1      0.82          98.36      0.82          98.36\n#&gt;           5,560 km (3,450 mi)      1      0.82          99.18      0.82          99.18\n#&gt;           5,745 km (3,570 mi)      1      0.82         100.00      0.82         100.00\n#&gt;                          &lt;NA&gt;      0                               0.00         100.00\n#&gt;                         Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Time/Points  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;       100h 30′ 35″      1      0.82          15.57      0.82          15.57\n#&gt;       100h 49′ 30″      1      0.82          16.39      0.82          16.39\n#&gt;       101h 01′ 20″      1      0.82          17.21      0.82          17.21\n#&gt;       103h 06′ 50″      1      0.82          18.03      0.82          18.03\n#&gt;       103h 38′ 38″      1      0.82          18.85      0.82          18.85\n#&gt;       105h 07′ 52″      1      0.82          19.67      0.82          19.67\n#&gt;       108h 17′ 18″      1      0.82          20.49      0.82          20.49\n#&gt;       108h 18′ 00″      1      0.82          21.31      0.82          21.31\n#&gt;       109h 19′ 14″      1      0.82          22.13      0.82          22.13\n#&gt;       110h 35′ 19″      1      0.82          22.95      0.82          22.95\n#&gt;       112h 03′ 40″      1      0.82          23.77      0.82          23.77\n#&gt;       112h 08′ 42″      1      0.82          24.59      0.82          24.59\n#&gt;       113h 24′ 23″      1      0.82          25.41      0.82          25.41\n#&gt;       113h 30′ 05″      1      0.82          26.23      0.82          26.23\n#&gt;       114h 31′ 54″      1      0.82          27.05      0.82          27.05\n#&gt;       114h 35′ 31″      1      0.82          27.87      0.82          27.87\n#&gt;       115h 27′ 42″      1      0.82          28.69      0.82          28.69\n#&gt;       115h 38′ 30″      1      0.82          29.51      0.82          29.51\n#&gt;       116h 16′ 02″      1      0.82          30.33      0.82          30.33\n#&gt;       116h 16′ 58″      1      0.82          31.15      0.82          31.15\n#&gt;       116h 22′ 23″      1      0.82          31.97      0.82          31.97\n#&gt;       116h 42′ 06″      1      0.82          32.79      0.82          32.79\n#&gt;       116h 59′ 05″      1      0.82          33.61      0.82          33.61\n#&gt;       117h 34′ 21″      1      0.82          34.43      0.82          34.43\n#&gt;       119h 31′ 49″      1      0.82          35.25      0.82          35.25\n#&gt;       122h 01′ 33″      1      0.82          36.07      0.82          36.07\n#&gt;       122h 25′ 34″      1      0.82          36.89      0.82          36.89\n#&gt;       123h 46′ 45″      1      0.82          37.70      0.82          37.70\n#&gt;       124h 01′ 16″      1      0.82          38.52      0.82          38.52\n#&gt;       127h 09′ 44″      1      0.82          39.34      0.82          39.34\n#&gt;       129h 23′ 25″      1      0.82          40.16      0.82          40.16\n#&gt;       130h 29′ 26″      1      0.82          40.98      0.82          40.98\n#&gt;       132h 03′ 17″      1      0.82          41.80      0.82          41.80\n#&gt;       133h 49′ 42″      1      0.82          42.62      0.82          42.62\n#&gt;       135h 44′ 42″      1      0.82          43.44      0.82          43.44\n#&gt;       136h 53′ 50″      1      0.82          44.26      0.82          44.26\n#&gt;       138h 58′ 31″      1      0.82          45.08      0.82          45.08\n#&gt;       140h 06′ 05″      1      0.82          45.90      0.82          45.90\n#&gt;       141h 23′ 00″      1      0.82          46.72      0.82          46.72\n#&gt;       142h 20′ 14″      1      0.82          47.54      0.82          47.54\n#&gt;       142h 47′ 32″      1      0.82          48.36      0.82          48.36\n#&gt;       145h 36′ 56″      1      0.82          49.18      0.82          49.18\n#&gt;       147h 10′ 36″      1      0.82          50.00      0.82          50.00\n#&gt;       147h 13′ 58″      1      0.82          50.82      0.82          50.82\n#&gt;       147h 51′ 37″      1      0.82          51.64      0.82          51.64\n#&gt;       148h 11′ 25″      1      0.82          52.46      0.82          52.46\n#&gt;       148h 29′ 12″      1      0.82          53.28      0.82          53.28\n#&gt;       149h 40′ 49″      1      0.82          54.10      0.82          54.10\n#&gt;       151h 57′ 20″      1      0.82          54.92      0.82          54.92\n#&gt;       154h 11′ 49″      1      0.82          55.74      0.82          55.74\n#&gt;       172h 12′ 16″      1      0.82          56.56      0.82          56.56\n#&gt;       177h 10′ 03″      1      0.82          57.38      0.82          57.38\n#&gt;       186h 39′ 15″      1      0.82          58.20      0.82          58.20\n#&gt;       192h 48′ 58″      1      0.82          59.02      0.82          59.02\n#&gt;       197h 54′ 00″      1      0.82          59.84      0.82          59.84\n#&gt;       198h 16′ 42″      1      0.82          60.66      0.82          60.66\n#&gt;       200h 28′ 48″      1      0.82          61.48      0.82          61.48\n#&gt;       219h 10′ 18″      1      0.82          62.30      0.82          62.30\n#&gt;       221h 50′ 26″      1      0.82          63.11      0.82          63.11\n#&gt;       222h 08′ 06″      1      0.82          63.93      0.82          63.93\n#&gt;       222h 15′ 30″      1      0.82          64.75      0.82          64.75\n#&gt;       226h 18′ 21″      1      0.82          65.57      0.82          65.57\n#&gt;       228h 36′ 13″      1      0.82          66.39      0.82          66.39\n#&gt;       231h 07′ 15″      1      0.82          67.21      0.82          67.21\n#&gt;       238h 44′ 25″      1      0.82          68.03      0.82          68.03\n#&gt;                 31      1      0.82          68.85      0.82          68.85\n#&gt;                 35      1      0.82          69.67      0.82          69.67\n#&gt;                 36      1      0.82          70.49      0.82          70.49\n#&gt;                 37      1      0.82          71.31      0.82          71.31\n#&gt;                 43      1      0.82          72.13      0.82          72.13\n#&gt;                 47      1      0.82          72.95      0.82          72.95\n#&gt;                 49      1      0.82          73.77      0.82          73.77\n#&gt;                 63      1      0.82          74.59      0.82          74.59\n#&gt;        79h 32′ 29″      1      0.82          75.41      0.82          75.41\n#&gt;        82h 05′ 42″      1      0.82          76.23      0.82          76.23\n#&gt;        82h 56′ 36″      1      0.82          77.05      0.82          77.05\n#&gt;        82h 57′ 00″      1      0.82          77.87      0.82          77.87\n#&gt;        83h 17′ 13″      1      0.82          78.69      0.82          78.69\n#&gt;        83h 38′ 56″      1      0.82          79.51      0.82          79.51\n#&gt;        83h 56′ 20″      1      0.82          80.33      0.82          80.33\n#&gt;        84h 27′ 53″      1      0.82          81.15      0.82          81.15\n#&gt;        84h 46′ 14″      1      0.82          81.97      0.82          81.97\n#&gt;        85h 48′ 35″      1      0.82          82.79      0.82          82.79\n#&gt;        86h 12′ 22″      1      0.82          83.61      0.82          83.61\n#&gt;        86h 20′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;        87h 20′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;        87h 34′ 47″      1      0.82          86.07      0.82          86.07\n#&gt;        87h 38′ 35″      1      0.82          86.89      0.82          86.89\n#&gt;        87h 52′ 52″      1      0.82          87.70      0.82          87.70\n#&gt;        89h 04′ 48″      1      0.82          88.52      0.82          88.52\n#&gt;        89h 40′ 27″      1      0.82          89.34      0.82          89.34\n#&gt;        89h 59′ 06″      1      0.82          90.16      0.82          90.16\n#&gt;        90h 43′ 20″      1      0.82          90.98      0.82          90.98\n#&gt;        91h 00′ 26″      1      0.82          91.80      0.82          91.80\n#&gt;        91h 59′ 27″      1      0.82          92.62      0.82          92.62\n#&gt;        92h 08′ 46″      1      0.82          93.44      0.82          93.44\n#&gt;        92h 44′ 59″      1      0.82          94.26      0.82          94.26\n#&gt;        92h 49′ 46″      1      0.82          95.08      0.82          95.08\n#&gt;        94h 33′ 14″      1      0.82          95.90      0.82          95.90\n#&gt;        95h 57′ 09″      1      0.82          96.72      0.82          96.72\n#&gt;        95h 57′ 16″      1      0.82          97.54      0.82          97.54\n#&gt;        96h 05′ 55″      1      0.82          98.36      0.82          98.36\n#&gt;        96h 19′ 38″      1      0.82          99.18      0.82          99.18\n#&gt;        96h 45′ 14″      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Margin  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;           + 1′ 07″      1      0.82          15.57      0.82          15.57\n#&gt;           + 1′ 11″      1      0.82          16.39      0.82          16.39\n#&gt;           + 1′ 12″      1      0.82          17.21      0.82          17.21\n#&gt;           + 1′ 22″      1      0.82          18.03      0.82          18.03\n#&gt;           + 1′ 25″      1      0.82          18.85      0.82          18.85\n#&gt;           + 1′ 34″      1      0.82          19.67      0.82          19.67\n#&gt;           + 1′ 41″      1      0.82          20.49      0.82          20.49\n#&gt;           + 1′ 42″      1      0.82          21.31      0.82          21.31\n#&gt;           + 1′ 50″      1      0.82          22.13      0.82          22.13\n#&gt;           + 1′ 51″      1      0.82          22.95      0.82          22.95\n#&gt;          + 10′ 32″      1      0.82          23.77      0.82          23.77\n#&gt;          + 10′ 41″      1      0.82          24.59      0.82          24.59\n#&gt;          + 10′ 55″      1      0.82          25.41      0.82          25.41\n#&gt;          + 12′ 14″      1      0.82          26.23      0.82          26.23\n#&gt;          + 12′ 41″      1      0.82          27.05      0.82          27.05\n#&gt;          + 12′ 56″      1      0.82          27.87      0.82          27.87\n#&gt;          + 13′ 07″      1      0.82          28.69      0.82          28.69\n#&gt;          + 14′ 13″      1      0.82          29.51      0.82          29.51\n#&gt;          + 14′ 18″      1      0.82          30.33      0.82          30.33\n#&gt;          + 14′ 34″      1      0.82          31.15      0.82          31.15\n#&gt;          + 14′ 56″      1      0.82          31.97      0.82          31.97\n#&gt;          + 15′ 49″      1      0.82          32.79      0.82          32.79\n#&gt;          + 15′ 51″      1      0.82          33.61      0.82          33.61\n#&gt;          + 17′ 52″      1      0.82          34.43      0.82          34.43\n#&gt;          + 17′ 54″      1      0.82          35.25      0.82          35.25\n#&gt;          + 18′ 27″      1      0.82          36.07      0.82          36.07\n#&gt;          + 18′ 36″      1      0.82          36.89      0.82          36.89\n#&gt;       + 1h 22′ 25″      1      0.82          37.70      0.82          37.70\n#&gt;       + 1h 42′ 54″      1      0.82          38.52      0.82          38.52\n#&gt;       + 1h 48′ 41″      1      0.82          39.34      0.82          39.34\n#&gt;           + 2′ 16″      1      0.82          40.16      0.82          40.16\n#&gt;           + 2′ 40″      1      0.82          40.98      0.82          40.98\n#&gt;           + 2′ 43″      1      0.82          41.80      0.82          41.80\n#&gt;           + 2′ 47″      1      0.82          42.62      0.82          42.62\n#&gt;          + 22′ 00″      1      0.82          43.44      0.82          43.44\n#&gt;              + 23″      1      0.82          44.26      0.82          44.26\n#&gt;          + 24′ 03″      1      0.82          45.08      0.82          45.08\n#&gt;          + 26′ 16″      1      0.82          45.90      0.82          45.90\n#&gt;          + 26′ 55″      1      0.82          46.72      0.82          46.72\n#&gt;          + 27′ 31″      1      0.82          47.54      0.82          47.54\n#&gt;          + 28′ 17″      1      0.82          48.36      0.82          48.36\n#&gt;       + 2h 16′ 14″      1      0.82          49.18      0.82          49.18\n#&gt;       + 2h 59′ 21″      1      0.82          50.00      0.82          50.00\n#&gt;           + 3′ 10″      2      1.64          51.64      1.64          51.64\n#&gt;           + 3′ 21″      2      1.64          53.28      1.64          53.28\n#&gt;           + 3′ 35″      1      0.82          54.10      0.82          54.10\n#&gt;           + 3′ 36″      1      0.82          54.92      0.82          54.92\n#&gt;           + 3′ 40″      1      0.82          55.74      0.82          55.74\n#&gt;           + 3′ 56″      1      0.82          56.56      0.82          56.56\n#&gt;           + 3′ 58″      1      0.82          57.38      0.82          57.38\n#&gt;          + 30 '41″      1      0.82          58.20      0.82          58.20\n#&gt;          + 30′ 38″      1      0.82          59.02      0.82          59.02\n#&gt;              + 32″      1      0.82          59.84      0.82          59.84\n#&gt;          + 35′ 36″      1      0.82          60.66      0.82          60.66\n#&gt;              + 38″      1      0.82          61.48      0.82          61.48\n#&gt;           + 4′ 01″      2      1.64          63.11      1.64          63.11\n#&gt;           + 4′ 04″      1      0.82          63.93      0.82          63.93\n#&gt;           + 4′ 05″      1      0.82          64.75      0.82          64.75\n#&gt;           + 4′ 11″      1      0.82          65.57      0.82          65.57\n#&gt;           + 4′ 14″      1      0.82          66.39      0.82          66.39\n#&gt;           + 4′ 20″      1      0.82          67.21      0.82          67.21\n#&gt;           + 4′ 35″      2      1.64          68.85      1.64          68.85\n#&gt;           + 4′ 53″      1      0.82          69.67      0.82          69.67\n#&gt;           + 4′ 59″      2      1.64          71.31      1.64          71.31\n#&gt;              + 40″      1      0.82          72.13      0.82          72.13\n#&gt;          + 41′ 15″      1      0.82          72.95      0.82          72.95\n#&gt;              + 48″      1      0.82          73.77      0.82          73.77\n#&gt;           + 5′ 02″      1      0.82          74.59      0.82          74.59\n#&gt;           + 5′ 20″      1      0.82          75.41      0.82          75.41\n#&gt;           + 5′ 39″      1      0.82          76.23      0.82          76.23\n#&gt;          + 50′ 07″      1      0.82          77.05      0.82          77.05\n#&gt;          + 54′ 20″      1      0.82          77.87      0.82          77.87\n#&gt;              + 54″      1      0.82          78.69      0.82          78.69\n#&gt;              + 55″      1      0.82          79.51      0.82          79.51\n#&gt;          + 57′ 21″      1      0.82          80.33      0.82          80.33\n#&gt;              + 58″      1      0.82          81.15      0.82          81.15\n#&gt;              + 59″      1      0.82          81.97      0.82          81.97\n#&gt;           + 6′ 17″      1      0.82          82.79      0.82          82.79\n#&gt;           + 6′ 21″      1      0.82          83.61      0.82          83.61\n#&gt;           + 6′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;           + 7′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;           + 7′ 17″      1      0.82          86.07      0.82          86.07\n#&gt;           + 7′ 29″      1      0.82          86.89      0.82          86.89\n#&gt;           + 7′ 37″      1      0.82          87.70      0.82          87.70\n#&gt;           + 8′ 04″      1      0.82          88.52      0.82          88.52\n#&gt;           + 8′ 37″      1      0.82          89.34      0.82          89.34\n#&gt;               + 8″      1      0.82          90.16      0.82          90.16\n#&gt;           + 9′ 09″      1      0.82          90.98      0.82          90.98\n#&gt;           + 9′ 30″      1      0.82          91.80      0.82          91.80\n#&gt;           + 9′ 51″      1      0.82          92.62      0.82          92.62\n#&gt;           +44′ 23″      1      0.82          93.44      0.82          93.44\n#&gt;                 18      1      0.82          94.26      0.82          94.26\n#&gt;                 19      1      0.82          95.08      0.82          95.08\n#&gt;                 20      1      0.82          95.90      0.82          95.90\n#&gt;                 26      1      0.82          96.72      0.82          96.72\n#&gt;                 32      1      0.82          97.54      0.82          97.54\n#&gt;                  4      1      0.82          98.36      0.82          98.36\n#&gt;                 59      1      0.82          99.18      0.82          99.18\n#&gt;                  8      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Stage wins  \n#&gt; Type: Character  \n#&gt; \n#&gt;               Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------- ------ --------- -------------- --------- --------------\n#&gt;           —     18     14.75          14.75     14.75          14.75\n#&gt;           0      8      6.56          21.31      6.56          21.31\n#&gt;           1     20     16.39          37.70     16.39          37.70\n#&gt;           2     27     22.13          59.84     22.13          59.84\n#&gt;           3     19     15.57          75.41     15.57          75.41\n#&gt;           4     12      9.84          85.25      9.84          85.25\n#&gt;           5      8      6.56          91.80      6.56          91.80\n#&gt;           6      6      4.92          96.72      4.92          96.72\n#&gt;           7      2      1.64          98.36      1.64          98.36\n#&gt;           8      2      1.64         100.00      1.64         100.00\n#&gt;        &lt;NA&gt;      0                               0.00         100.00\n#&gt;       Total    122    100.00         100.00    100.00         100.00\nBut here again we need to be careful. For example the distance variable is coded as a character, and its reporting with frequencies is thus quite stupid.\nThere is nothing automatic, you still need to think about, transform or subset your data.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "Summary tables"
    ]
  },
  {
    "objectID": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "href": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "title": "Summary tables",
    "section": "Descriptive statistics with modelsummary",
    "text": "Descriptive statistics with modelsummary\nto be done",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "Summary tables"
    ]
  },
  {
    "objectID": "999_references.html",
    "href": "999_references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear\nRegression Models with an Introduction to Spatial Econometrics:\nRegression Models with an Anselin Bera i. INTRODUCTION.” In. CRC\nPress.\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An\nAxiomatic Approach to\nGeographical Space.” Geographical\nAnalysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied\nRegression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced\nData.” In, edited by Manfred M. Fischer and Arthur Getis,\n197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd\nEdition. https://rc2e.com/.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem.\nConcepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth\nin the Detroit Region in Economic Geography 46: 2,\n234240.” Clark University, Worcester, MA.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to\nr.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d.\n“R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Bibliography",
      "References"
    ]
  },
  {
    "objectID": "010_intro.html",
    "href": "010_intro.html",
    "title": "Part I - Introduction",
    "section": "",
    "text": "1  Statistical data analysis for geographers\n2  Spatial data analysis: a definition\n3  Geographical space\n4  Spatial data issues",
    "crumbs": [
      "Part I - Introduction"
    ]
  },
  {
    "objectID": "020_getting_started.html",
    "href": "020_getting_started.html",
    "title": "Part II - Getting Started with R",
    "section": "",
    "text": "5  Getting started with RStudio\n6  Vectors\n7  Data frames and lists\n8  Working with data frames and functions\n9  Reading and writing data to and from R\nFor better introductory material, we recommend\n\nVenables, Bill and Smith, David, M (n.d.)\nLong and Teetor (n.d.)\nWickham, Çetinkaya-Rundel, and Grolemund (n.d.) (including RStudio introduction) (Note it uses a tidyverse approach while we rather stick to base R where possible, except for graphics (ggplot))\n\n\n\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd Edition. https://rc2e.com/.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to r.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d. “R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part II - Getting Started with R"
    ]
  }
]