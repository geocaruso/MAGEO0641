[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and R for geographers",
    "section": "",
    "text": "Preface\nThis is the syllabus for the course Introduction to Data Analysis for Geographers with R (MAGEO0641) at the Department of Geography and Spatial Planning at the University of Luxembourg. It has been produced as a Quarto book by Geoffrey Caruso and Léandre Fabri with the aim of aggregating and homogenizing different material accumulated over the past few years.\nAs of September 2024, the assemblage is still a work in progress. We kindly ask you to refer to your notes during class to prepare for the examination and assignments.\nThe general structure and base material shown here have been organized by Geoffrey Caruso, who originally inherited teaching material from Dominique Peeters. We are grateful to the previous teaching assistants, Mirjam Schindler and Marlène Boura, as well as to David Dabin and Jonathan Jones, for their contributions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001_timetable.html",
    "href": "001_timetable.html",
    "title": "Timetable",
    "section": "",
    "text": "The course is made of 13 sessions. Each session comprises 5 (teaching) units of 45 minutes.\nWe have attempted to balance theoretical explanations and R practice in each session.\nThe 2024-2025 schedule is set as follows. However, please refer to your student guichet and the Moodle platform for potential changes during the semester.\n\nScheduled: 16 Sep 2024 at 14:00 to 18:00, CEST\nScheduled: 23 Sep 2024 at 14:00 to 18:00, CEST\nNo session on 30 Sep 2024\nScheduled: 7 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 14 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 21 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 28 Oct 2024 at 14:00 to 18:00, CET\nScheduled: 4 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 11 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 18 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 25 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 2 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 9 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 16 Dec 2024 at 14:00 to 18:00, CET",
    "crumbs": [
      "Timetable"
    ]
  },
  {
    "objectID": "002_learning.html",
    "href": "002_learning.html",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Objectives:",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#objectives",
    "href": "002_learning.html#objectives",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Overview of key concepts and formalisation in data analysis in geographical contexts\nUnderstanding and describing the statistical and spatial distribution of data with univariate statistical analysis\nUnderstanding how a geographical pattern relate or can be understood from others with bivariate and regression analysis\nRaising awareness as to the characteristics and difficulties of statistical and econometric (regression) analysis with geographical data.\nMastering essential R software skills for tabular data management, uni and bi-variate analysis and regression, and producing graphics",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#expected-outcomes",
    "href": "002_learning.html#expected-outcomes",
    "title": "Learning outcomes and evaluation",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nOn completion, each student should be able to\n\nDescribe the key concepts in spatial statistical analysis and the specificities of geographical space and spatial data\nDemonstrate a good command of R to handle statistical datasets and perform univariate, bivariate and multiple regressions analyses with good diagnostics\nExplain and use common univariate and bivariate statistics\nExplain and use exploratory methods\nExplain and apply standard regression methods and diagnostics, and discuss limits and problems when applied to geographical data\nExplain the principles and methods used to identify local effects and spatial autocorrelation\nRead and discuss detailed results of an empirical research article that deal with data analysis including a multivariate regression in a spatial or non-spatial context\nExplain and use mixed methods (Q-Methodology: hybrid approach between quantitative and qualitative methods)",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "002_learning.html#evaluation",
    "href": "002_learning.html#evaluation",
    "title": "Learning outcomes and evaluation",
    "section": "Evaluation",
    "text": "Evaluation\n\nIndividual\n20% Continuous assessment: small tests in class and weekly exercises:\n40% R exam (3h) in GIS room, perform R analysis, answer a questionnaire and provide script\n40% Oral exam (30min): presenting a paper and answering questions about its details and the course",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "011_scope.html",
    "href": "011_scope.html",
    "title": "1  Statistical data analysis for geographers",
    "section": "",
    "text": "1.1 Scope\nThis course is an introduction to standard statistical techniques that geographers often encounter. Being at the crossroads of different fields, it is necessary for geographers to have a basic set of tools with which to interact with other experts and modelers who, although they may use different wording and have their own technical habits, use a common set of concepts and tools to analyze data and test their hypotheses.\nIn their own work and in their interactions with others, it is also important for geographers to keep in mind that the data they use are quite specific because they are about located objects or subjects, about places and their interactions. Most of the time, the data they use is georeferenced in some way (accurately or not). This inherently geographic aspect brings with it a number of challenges. In this course, we will highlight these challenges when performing standard data analysis. However, we won’t solve any of these geographic problems, and we won’t even explicitly use geographic features, i.e., no mapping, no use of georeferencing as such. Our goal is to equip students with standard statistical methods also used in related fields, with some critical thinking about their geographical nature or underlying spatial processes. In a nutshell, this is a journey from elementary statistics to spatial autocorrelation with a standard regression detour.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "href": "011_scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "title": "1  Statistical data analysis for geographers",
    "section": "1.2 Transparent geographical analysis empowered with R",
    "text": "1.2 Transparent geographical analysis empowered with R\nThe course is a blend of theory and practice, which we believe enhances intuition and understanding. Direct practice also helps, especially for human geographers with little training in quantitative methods, to demystify statistical concepts and provide confidence after repeated applications and interpretations.\nSoftware for statistical analysis has evolved rapidly and R is prominent in many disciplines. It is open and free. It is simply fantastic for spatial data analysis and may well be the only tool geographers really need in their data undertaking, even replacing GIS (Geographic Information Systems) software. You just need to get started with R.\n\n\n\n\n\nMost of our students have had some sort of theoretical statistics course so far in their studies, and have probably seen most of the content. Sometimes our students have had some practice with SPSS (or similar) software, but most have not used any real statistical software at all, and have a spreadsheet (e.g., Excel) as their only reference for data management, analysis, and graphing.\nWe chose R for its openness, leadership, large community, and later for its spatial features, but also because it forces students to be transparent and think about every step they take. Data analysis and visualization can be misleading and dangerous, it is necessary to think and facilitate replication for oneself and for others. R, and more generally the use of scripts and command lines, is absolutely necessary to bring robustness and trust.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html",
    "href": "012_spatial_analysis.html",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "2.1 Interdisciplinarity and perspective\nData analysis and statistics are used in many scientific fields. When the focus is on geographic objects, subjects, their patterns or relationships, we like to talk about spatial analysis. However, because geographic data is relevant to many fields and statistical methods are widely used, spatial analysis can be defined and approached differently.\nWithin geography, we understand the term spatial analysis to refer to all quantitative approaches, as opposed to qualitative approaches (although these can also be spatial and analytical). Within the quantitative part of the discipline, however, spatial analysis would mostly refer to applied statistical approaches, in contrast to GIS modeling, geosimulation, transportation modeling, or mathematical models. In this sense, this course is a spatial analysis course.\nHowever, for those researchers involved in spatial analysis close to regional science and economic geography (and perhaps close to landscape ecology and GIScience), the terms spatial data analysis or spatial statistics would more strictly refer to the explicit use of geographic information in the modeling process, not just the consideration of geographic elements. See for example the handbook of Fischer and Getis for discussion and examples.\nSimilarly, Goodchild and Longley (https://www.geos.ed.ac.uk/~gisteac/gis_book_abridged/files/ch40.pdf) suggest more broadly that spatial analysis could simply be a set of methods useful when the data are spatial (i.e. referenced in 2D frame). This definition however as they suggest would be too broad, if it does not address the question of whether the 2D frame actually matters. Rather spatial analysis is",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "href": "012_spatial_analysis.html#interdisciplinarity-and-perspective",
    "title": "2  Spatial data analysis: a definition",
    "section": "",
    "text": "the subset of analytic techniques whose results will change if the frame changes, or if objects are repositioned within it.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "012_spatial_analysis.html#links-with-theory",
    "href": "012_spatial_analysis.html#links-with-theory",
    "title": "2  Spatial data analysis: a definition",
    "section": "2.2 Links with theory",
    "text": "2.2 Links with theory\nScience progresses with tools and techniques but also by testing hypotheses and updating models and theory. How spatial analysis is linked to theory also depends on fields or sub-fields.\nFrom a quantitative geography viewpoint (adapted from Denise Pumain https://hypergeo.eu/theories-of-spatial-analysis/?lang=en), spatial analysis focuses on uncovering spatial structures and organizations. These structures can often be generalized into models, such as center-periphery relationships, gravity models, and urban hierarchies and networks.\nThe ultimate goal of spatial analysis is then to understand the processes that lead to the formation of these spatial structures.\nFrom a spatial economic or regional science viewpoint (as understood by a European quantitative geographer) spatial analysis consists of a set of techniques designed to:\n\nDescribe the location of activities and how they change over time\nEstimate reduced form models\n\nUnlike structural form models, which are direct representations or formulations of theoretical concepts, reduced form models are designed to better align with and fit the data.\nThere is probably no such a reduced or structural form model in quantitative geography, but in both case anyway, spatial analysis ultimately aims at testing and updating theories.\nWe very much agree with this perspective here, leading to giving more importance to the falsification of ideas and the interpretation of estimated coefficients than to prediction using as many data as possible.\nIf a variable is used it is because we have some idea of its importance and influence on others or its relevance, not just to obtain a fit. Hence we won’t use automatic models constructions (no stepwise regression for example) and leave out all the methods (neural networks, random forests, etc.) from which coefficients (if any) are difficult to interpret, even if these methods can be considered to belong to spatial analysis and use geographic data. This course is not about data mining or data crunching. We use a statistical lens to examine variations across space and how spatial relationships influence socio-economic patterns and behaviors or environmental effects.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: a definition</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html",
    "href": "013_space_axiom.html",
    "title": "3  Geographical space",
    "section": "",
    "text": "3.1 Absolute or pre-geographical space:",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "href": "013_space_axiom.html#absolute-or-pre-geographical-space",
    "title": "3  Geographical space",
    "section": "",
    "text": "A set of places or locations \\(S\\)\nIdentified by their coordinates \\(x,y\\)\nSeparated by a distance \\(d(L)\\)\nDistance being measured along a given metric \\(L\\)",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "013_space_axiom.html#geographical-space",
    "href": "013_space_axiom.html#geographical-space",
    "title": "3  Geographical space",
    "section": "3.2 Geographical space:",
    "text": "3.2 Geographical space:\nS can be endowed with various attributes to form a geographical space:\n\nThe surface attribute \\(m\\), measured along a given metric related to coordinates\nVarious attributes \\(Z\\)\nDensity measures, i.e. any \\(Z/m\\)\n\n\n\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An Axiomatic Approach to Geographical Space.” Geographical Analysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html",
    "href": "014_space_issues.html",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "4.1 Equivalence and Independence\n(Based on discussions in Jayet, p. 2-13)\nStatistical analysis is based on two key principles, or invariants:\nHowever, both of these principles are challenged when applied in a spatial context. Spatial data often exhibit dependencies due to geographic proximity, which violates the assumption of independence. Similarly, the notion of statistical equivalence becomes problematic as spatial heterogeneity introduces variability across observations in different locations and because the spatial definition of objects may vary and their sampling irregular.\nThese challenges highlight the need for specialized approaches in spatial analysis, where standard statistical methods must be adapted to account for the structure and dependencies present in the spatial data.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence-and-independence",
    "href": "014_space_issues.html#equivalence-and-independence",
    "title": "4  Spatial data issues",
    "section": "",
    "text": "All observations must be statistically equivalent: This means that no individual observation should be systematically different from others in the sample set. Each data point must have the same probability distribution, ensuring uniformity and comparability.\nAll observations must be independent from each other: In any statistical model, the assumption is that the occurrence of one observation does not influence or depend on another. Independence ensures the integrity of statistical results.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#equivalence",
    "href": "014_space_issues.html#equivalence",
    "title": "4  Spatial data issues",
    "section": "4.2 Equivalence",
    "text": "4.2 Equivalence\n\n4.2.1 Irregularity of observations and the nature of data\nWhile there are time cycles, making repetitive data logging along the time dimension doable, there is no such think as a spatial cycle for geographical data recording.\nMost spatial data has a irregular covering in space, which already challenges the equivalence of observations\n\n(source to be added, apologies if you are the author, I am happy to adapt)\n\nObservations (countries) are of different size, i.e. the surface attribute \\(m\\) matters here.\nSuppose \\(Z_{pop}\\) is the country population. One can expect \\(Z_{pop}\\) to relate to \\(m\\) if processes are homogeneous across space. However a \\(Z/m\\) density variables would still show these objects are very different.\nYet, other \\(Z_i\\) variables could still be compared using that \\(Z_{pop}\\) attributes. For example the active population of the place (country) as a percentage of its total population, or using other transformations (linear or not).\nNote that variations in volume/mass/size such as \\(Z_{pop}\\) are very common, with very few objects having a very large size compared to most others. Such a size effect\n\nimpacts on the total and central (mean, median) value of variables\nthe distribution of values when made in different observations’ regions\ntypically leads to outliers problems or heteroskedasticity (non constant variance)\n\nHowever, there are raster maps and some information is “regular”, such a precipitation, or can be “regularized”, such as population grids.\n\n(https://human-settlement.emergency.copernicus.eu/)\nThe discretization of geographic space should however be internally homogeneous, meaning the attributes within each grid cell (or other nwe objects) supposed to apply to every part of that cell.\nA difference is often made between continuous field data and discrete space objects.\nSee below the tabulation of these against the types of measurements by Haining (2010)\n\n\n\n4.2.2 Modifiable Areal unit Problem - MAUP\nOpenshaw (1984)\n\nAreal units = spatial objects such as zones or places or towns or regions\n\n\nGeography has consistently and dismally failed to tackle its entitation problems, and in that more than anything else lies the root of so many of its problems (Chapman 77)\n\n\nInsufficient thought is given to precisely what it is that is being studied. […] Little concern has been expressed about the nature and definition of the spatial objects under study\n\n\nFor many purposes the zones in a zoning system constitute the objects, or geographical individuals, that are the basic units for the observation and measurement of spatial phenomena.\n\n\nWith areal data, the spatial objects only exist after data collected for one set of entities (e.g. people) are subjected to an arbitrary aggregation (see also regularity discussion above) to produce a set of spatial units.\n\n\nHowever, there are no rules for areal aggregation, no standards, and no international conventions to guide the spatial aggregation process.\n\n\nThe areal units (zonal objects) used in many geographical studies are arbitrary, modifiable. Census areas have rarely an intrinsic geographical meaning\n\n\nA unmanageable combinatorial problem: There are approximately 10^12 different aggregations of 1,000 objects into 20 groups. If the aggregation process is constrained so that the groups consist of internally contiguous objects (i.e. all the objects assigned to the same group are geographical neighbours) then this huge number is reduced, but only by a few orders of magnitude.\n\nStan Openshaw distingues 2 interrelated issues, within the MAUP:\n\nThe scale problem: the variation in results that can often be obtained when data for one set of areal units are progressively aggregated into fewer and larger units for analysis.\nThe aggregation problem: the problem of alternative combinations of areal units at equal or similar scales. Any variation in results due to the use of alternative units of analysis when the number of units is held constant\n\n\nExample of effects on correlation coefficients:\n\nCorrelation between percentage vote for Republican candidates in the congressional election of 1968 and the percentage of the population over 60 years\nCorrelation at the 99 county level is 0.34\nAfter aggregation into six zones: 0.26 for the 6 congressional districts and 0.86 for a simple typology of Iowa into 6 rural-urban types (Openshaw and Taylor 77)\nCompare mean and dispersion of correlation coefficient after random zoning (using contiguity) and random sampling (grouping)\n\n\n\nNo systematic scale effect on correlation mean\nConsiderable variability about the mean values but reduces with increasing numbers of units\nThe standard deviations of the zoning distributions are considerably smaller than the corresponding sampling distributions but exhibit a greater degree of bias &gt; spatial autocorrelation effect",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "014_space_issues.html#spatial-in-dependence",
    "href": "014_space_issues.html#spatial-in-dependence",
    "title": "4  Spatial data issues",
    "section": "4.3 Spatial (in-)dependence",
    "text": "4.3 Spatial (in-)dependence\n\n4.3.1 Interactions between observations\n\nNot only the dimensions and structures of observations is of importance but also their relative position in space\nThe distance (between objects), \\(d(L)\\), is at the very heart of geographical analysis AND the source of statistical difficulties\nThe level of interactions increases with proximity (distance functions or contiguities) (see gravity-based theories)\n\n\n\n4.3.2 Tobler’s first law of geography\nTobler (1970)\n\n\n\n4.3.3 Spatial autocorrelation\nAnselin, Luc and Bera,I (1998)\n\n\n\n\n\n\nAnselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear Regression Models with an Introduction to Spatial Econometrics: Regression Models with an Anselin Bera i. INTRODUCTION.” In. CRC Press.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced Data.” In, edited by Manfred M. Fischer and Arthur Getis, 197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem. Concepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth in the Detroit Region in Economic Geography 46: 2, 234240.” Clark University, Worcester, MA.",
    "crumbs": [
      "Part I - Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial data issues</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html",
    "href": "021_Rstudio.html",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "5.1 R, RStudio and its interface\nIn class demonstration of how to",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#r-rstudio-and-its-interface",
    "href": "021_Rstudio.html#r-rstudio-and-its-interface",
    "title": "5  Getting started with RStudio",
    "section": "",
    "text": "Get R and RStudio installed\nNavigating the R studio interface\nScripting area, console, files,…\nUsing colors and TOC outline in RStudio\nAuto-completion using tabs\nNavigating history with the up/down arrows\nUnderstanding help (necessary arguments and default options)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#projects-and-workflows",
    "href": "021_Rstudio.html#projects-and-workflows",
    "title": "5  Getting started with RStudio",
    "section": "5.2 Projects and workflows",
    "text": "5.2 Projects and workflows\nLet’s compute a value in the console and store it to an object first\n\n#This is a comment\n1+2 #This is also a comment\n#&gt; [1] 3\na&lt;-3+4\na\n#&gt; [1] 7\n\nSee that we now have an object in the environment!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#objects",
    "href": "021_Rstudio.html#objects",
    "title": "5  Getting started with RStudio",
    "section": "5.3 Objects",
    "text": "5.3 Objects\nAn object is not defined ex-ante and is automatically overwritten\n\nX&lt;-3 #This X will soon be replaced\nX&lt;-1:10 \nY&lt;-X^2\n\nR is case sensitive. The following returns an Error\n\nx # x is lower case and does not exist\n\nIMPORTANT:\n\nAlways worry about Errors. They stop your process.\nAlways read and try to understand Warnings. They do not stop your process but usually indicate the result may not be as expected!",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#working-directory",
    "href": "021_Rstudio.html#working-directory",
    "title": "5  Getting started with RStudio",
    "section": "5.4 Working directory!!!",
    "text": "5.4 Working directory!!!\nSuppose we want to produce a text from the above and save it to a file:\n\na_sentence&lt;-paste(\"I have computed a sum, which equals\", a)\na_sentence #Let's see this in the console\n#&gt; [1] \"I have computed a sum, which equals 7\"\ncat(a_sentence,file=\"brol/a_sentence.txt\")\n\nWhere is the file? the directory? Have you been able to run this?\nAlways indicate where you work!\nThe classical way is\n\ngetwd() #get (default) working directory\nsetwd(\"/Users/geoffrey.caruso/Dropbox/GitHub/MAGEO0641/brol\") #set working directory to YOUR OWN NETWORK SPACE HERE!\n\nThere is a more practical way in RStudio: Make an .Rproj from/to a directory\nYou can create a directory in your finder/file explorer, or create a directory or subdirectory from within the R console. This will be very useful at a more advanced level when you create many outputs and directory names result from some data processing. Think of processing something across many countries/cities.\n\ndir.create(\"brol\")\ndir.create(\"Today\")\n\n#and for removing a directory\nunlink(\"Today\", recursive = TRUE) #see help: If recursive = FALSE directories are not deleted, not even empty ones.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "href": "021_Rstudio.html#commented-r-scripts-vs-markdown-documents",
    "title": "5  Getting started with RStudio",
    "section": "5.5 Commented R scripts vs markdown documents",
    "text": "5.5 Commented R scripts vs markdown documents\nThe above is a commented script, using #, which you can save as an .R file and re-run later.\nThe problem with this approach is that you won’t see results of the codes until you run it. So you can’t really comment your output (although many, and I, would still do it) thus mixing explanation of what is done and interpretation.\nA more advanced approach is to make a document where you integrate text, code chunks and results of the code.The text can thus document what is going to be done and the results, while the code chunks can thus document both the code itself and its result as it is processed by the Console.\nLet’s have a look at the structure of this syllabus, written using Quarto markdown.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "href": "021_Rstudio.html#good-practice-with-files-handling-and-objects-naming",
    "title": "5  Getting started with RStudio",
    "section": "5.6 Good practice with files handling and objects naming",
    "text": "5.6 Good practice with files handling and objects naming\nRproj in an excellent way to keep things at the same place.\nWhen using an Rproj, use relative path only, i.e. from the root folder of the project, not from your machine. This is the way you can easily transfer your project to friends. Only external data (e.g. from the web) should be referenced in full.\nIt is also good practice to have a specific “data” folder or subfolders for all data so you clearly differentiate your outputs with the inputs. Similarly a R folder with your scripts when you have many.\nAlthough there is no naming convention agreed by everyone, it is important to apply a consistent style for yourself and colleagues. Also you would avoid spaces and rarer characters, especially in a multilingual environment.\nA folder of file named “source data” or “data_für_rémi” are not great ideas. (This is also true for variable names in data frame, see later)\nIn general, I personally like files to be all lowercase with underscores and using action verbs to explain what is done in the R file, such as\n\nestimate_model.R\nget_statec_data.R\n\nFiles numbering can be of good help for heavier projects where there is a logical sequence (time)\n\n01_estimate_model.R\n02_get_statec_data.R\n\nFor variable names and objects I tend to use the “UpperCamelCase” form especially for vectors\n\nLuxCities\nGrowthRates\n\nand tend to add an “df” or “_lst” to disambiguate where needs be between some classes\n\nLuxCities_df\nLuxCities_lst\n\nI also like to use lowercase single letters for input parameters, such as a pvalue or number of neighbours, e.g.\n\np&lt;-0.05\nk&lt;-2\n\nor Greec symbols written in full, especially when there is a theoretical link\n\nbeta &lt;-model$coefficient[1]\nrho &lt;- 0.5\n\nFor functions (see later) I also like to use action verbs and include dots, such as:\n\nplot.bmi()\nextract.boundary()\n\nIn all case, be concise but specific and consistent within a project or even across.\n\nNeighboursCompute_Europe.Paris_project3 would be very long and inconsistent\ndf_new2bis not to use both as an object or a file\n\n…but to to be honest I have quite a number of tests.r and plot.test2.jpg files peppered in my machine.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting started with RStudio</span>"
    ]
  },
  {
    "objectID": "022_vectors.html",
    "href": "022_vectors.html",
    "title": "6  Vectors",
    "section": "",
    "text": "6.1 Introducing vectors\nVectors are the basic units of information in R, and many functions apply to vectors. If you are coming from the “spreadsheet” world (MS Excel or Open Office), where the basic unit is a cell, the change in perspective is quite important: while a cell has a single value (information), a vector contains multiple values.\nVectors, being a combination of values, are created by the combine (or concatenate) function c() or obtained from external sources.\nThere are two kinds of vectors:\nThis chapter is about atomic vectors, we will introduce lists later.\nAs a first example, see below a character (atomic) vector and a numeric (atomic) vector:\nCountries&lt;-c(\"Romania\",\"Russia\",\"Morocco\",\"Iran\",\"France\")\nmode(Countries)\n#&gt; [1] \"character\"\nAges&lt;-c(20,25,22,22,49)\nmode(Ages)\n#&gt; [1] \"numeric\"\nNote that in case you would have a long list to input manually, the scan() function is a little more interactive. Try! (I also have heard there are ways to copy-paste from external sources…(if you fancy less transparent clicking approaches, be curious and find out, for example here: Chapter 20 ;-), most of the times anyway we rather read values from readable text files).",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#introducing-vectors",
    "href": "022_vectors.html#introducing-vectors",
    "title": "6  Vectors",
    "section": "",
    "text": "Atomic vectors (which we tend to refer to simply as vectors), which are homogeneous in the sense that they contain only one “type” of data, such as characters or numbers.\nLists, which can have heterogeneous contents.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#attributes-of-vectors",
    "href": "022_vectors.html#attributes-of-vectors",
    "title": "6  Vectors",
    "section": "6.2 Attributes of vectors",
    "text": "6.2 Attributes of vectors\nA vector is always characterized by its type and length.\n\n6.2.1 Types\nA vector is a combination of values from the same type (or mode). So if you combine data from different types they will be coerced to the less demanding one, i.e. “character” in the following cases:\n\nA&lt;-c(1,\"two\",3)\nA\n#&gt; [1] \"1\"   \"two\" \"3\"\nmode(A)\n#&gt; [1] \"character\"\n\nc(Countries, Ages)\n#&gt;  [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\"  \"20\"      \"25\"     \n#&gt;  [8] \"22\"      \"22\"      \"49\"\nmode(c(Countries, Ages))\n#&gt; [1] \"character\"\n\nYou can also see from these examples that mode()` gets a vector’s “type”. R vectors have one and only one “mode”, either “numeric”, “character” or “logical” (plus “raw” and “complex”, which we don’t consider here).\nYou can also use typeof() in case you are interested to know how the data is actually encrypted, which essentially differentiates the “numeric” mode into “integer” and “double”. R vectors have one and only one “typof”, either “integer”, “double”, “character” or “logical”\nCoercion to the same type applies to the “typeof” as you can see below\n\nBNum&lt;-c(2,10,99)\ntypeof(BNum)\n#&gt; [1] \"double\"\nmode(BNum)\n#&gt; [1] \"numeric\"\nBInt&lt;-c(2L,10L,99L)\ntypeof(BInt)\n#&gt; [1] \"integer\"\nmode(BInt)\n#&gt; [1] \"numeric\"\nB&lt;-c(BNum,BInt)\ntypeof(B)\n#&gt; [1] \"double\"\nmode(B)\n#&gt; [1] \"numeric\"\n\nIn practice, you will rarely use mode() or typeof(), which distinguish well between vectors, but not between most other objects. Instead, you will use the class() function in order to know what kind of data you have and what you can do with it. For most atomic vectors, class() is basically typeof(). The difference is that “class” is not a mutually exclusive property. A vector, or any object, can belong to several classes and thus be used with different functions\n\nclass(c(20L,50L,70L))\n#&gt; [1] \"integer\"\nclass(c(20,50,70))\n#&gt; [1] \"numeric\"\nclass(Countries)\n#&gt; [1] \"character\"\nclass(Ages)\n#&gt; [1] \"numeric\"\nclass(c(Countries,Ages)) #coercion\n#&gt; [1] \"character\"\n\nSometimes, depending on some calculations, you may need to transform the type of data, especially from numeric to character and vice versa. Coercion may apply automatically but not always, so you will need to explicitly transform the data type using “as.a type” or”as.a mode”: as.numeric(), as.character(), which you will use quite often, or as.integer(),as.double(), or as.logical().\n\nA&lt;-c(1,2,3)\nB&lt;-c(1L,2L,3L)\nC&lt;-c(\"1\",\"2\",\"3\")\nA+B\n#&gt; [1] 2 4 6\nA+C\n#&gt; Error in A + C: non-numeric argument to binary operator\nA+as.numeric(C)\n#&gt; [1] 2 4 6\nas.character(A)\n#&gt; [1] \"1\" \"2\" \"3\"\n\n\n\n6.2.2 Length\nWhile a numeric vector of length one is mathematically a scalar, the way you input a scalar in R is as a vector of length 1. A vector can also be empty, i.e. of length 0, in which case its type is unknown, unless you apply one of the above transformations.\n\nX&lt;-3 #equivalent to X&lt;-c(3)\nclass(X)\n#&gt; [1] \"numeric\"\n\nY&lt;-c()\nclass(Y)\n#&gt; [1] \"NULL\"\n\nY&lt;-as.numeric(c())\nclass(Y)\n#&gt; [1] \"numeric\"\n\nY&lt;-as.integer(c())\nclass(Y)\n#&gt; [1] \"integer\"\n\nYou get the length of a vector using the function length(), which you will also use quite a lot.`\n\nlength(Countries)\n#&gt; [1] 5\nlength(X)\n#&gt; [1] 1\nlength(Y)\n#&gt; [1] 0\n\nNote that if you manually change the length of an existing vector, this will trim the vector end or extend the vector with empty values.\n\nZ&lt;-c(2, 4, 6, 7 , 10)\nlength(Z)&lt;-3\nZ\n#&gt; [1] 2 4 6\nlength(Z)&lt;-12\nZ\n#&gt;  [1]  2  4  6 NA NA NA NA NA NA NA NA NA\n\nBesides their class() and length() attributes, vectors (and other objects) can be endowed with a number of other attributes, which you will obtain from attributes(). None of the vectors we used as example so far has additional attributes, but you can define any attribute yourself. See below how we add a “source” attribute to the vector of “Countries” we created earlier and assign a “character string” to it to describe the source. Retrieving a specific attribute is done with the attr(,\"Whatever attribute\") function.\n\nattributes(Countries)\n#&gt; NULL\nattr(Countries, \"source\")&lt;-\"MAGEO students input\"\nattributes(Countries)\n#&gt; $source\n#&gt; [1] \"MAGEO students input\"",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sec-factors",
    "href": "022_vectors.html#sec-factors",
    "title": "6  Vectors",
    "section": "6.3 Factors",
    "text": "6.3 Factors\nRemember from the introduction, that we (in geography and elsewhere) usually consider discrete and continuous data, each type being split into 2 measurement levels:\n\nDiscrete: nominal (blue, green) or ordinal (high, low)\nContinuous: interval (10°C, 20°C) or ratio (10 apples, 20 apples) (difference being that 20 apples is twice as much as much as 10 apples, but 20°C is not twice as hot as 10°C because zero is not the absolute zero, hence a ratio in this case makes little sense, only differences are sensical).\n\nThis vocabulary is not used directly in R:\n\nContinuous (ratio/interval) data is coded as numeric or integer\nDiscrete (categorical) data (nominal/ordinal) is preferably coded as a factor.\n\nNominal data can still be in the “character” type, but the idea of a “factor” is that there is only a limited set of characters’ strings that you will find in a vector (e.g. set of countries, set of land uses) and you are able (willing) to enumerate them. For ordinal data, the order is important, hence it gets closer to an integer set than to a character (e.g. education levels: primary, secondary tertiary education), yet you can sum integer data but should not sum ordinal data.\n\nFactors are designed to properly solve the use of discrete/categorical data. It is based on the integer type (in the sense of typeof) on top of which a “level” attribute is added, and potentially whether it is ordered or not. They are fabricated with the as.factor() or factor() functions.\n\nCountries\n#&gt; [1] \"Romania\" \"Russia\"  \"Morocco\" \"Iran\"    \"France\" \n#&gt; attr(,\"source\")\n#&gt; [1] \"MAGEO students input\"\ntypeof(Countries)\n#&gt; [1] \"character\"\n\nCountries_f&lt;-as.factor(Countries)\nCountries_f\n#&gt; [1] Romania Russia  Morocco Iran    France \n#&gt; Levels: France Iran Morocco Romania Russia\ntypeof(Countries_f)\n#&gt; [1] \"integer\"\nclass(Countries_f)\n#&gt; [1] \"factor\"\n\nWhile the categories are displayed with the factor, the levels() function returns those categories as a character vector.\n\nlevels(Countries_f)\n#&gt; [1] \"France\"  \"Iran\"    \"Morocco\" \"Romania\" \"Russia\"\n\nBy default the levels in a factor uses the alphabetical order. In many cases however, you will want to at least define the first one, in order to use it as a reference (typically in regression analysis with a categorical explanatory variable) or choose you own order for plotting or other purposes.\n\n6.3.1 Re-defining the reference level\nSuppose you want to use “Morocco” as the first, reference level instead of “France”.\nThe relevel() function re-orders the levels so that the one indicated as “ref” is used first and the others are moved down the series.\n\nCountries_f2&lt;-relevel(Countries_f, ref=\"Morocco\")\n\nYou could also set the order directly at the time of creating the factor using the “level” argument, using the factor() function, not as.factor(). We used as.factor() before because it is generally quicker, and there is not always a need to adapt the order of levels.\n\nCountries_f3&lt;-factor(Countries, level=c(\"Morocco\",\"France\",\"Romania\", \"Iran\",\"Belgium\"))\n\nBe careful, however, because if you don’t use the complete list of possibilities, the values that are not specified in the levels vector, will simply be ignored and turned into NA’s. In the above example we forgot “Russia” and therefore have now a NA within our vector. Conversely, we have been able to indicate a “Belgium” level, although it was not present. There is no automatic correspondence between the levels of a factor and its set of values. If you want a match, you can drop unused levels using droplevels() but for those characters (e.g. Russia) that were not taken at the moment of creating the factor, it is too late, they remain a NA.\n\nCountries_f4&lt;-droplevels(Countries_f3)\nCountries_f4\n#&gt; [1] Romania &lt;NA&gt;    Morocco Iran    France \n#&gt; Levels: Morocco France Romania Iran\n\nSee how the different factors we made so far change the order of the levels and the data when a case is made absent from the list:\nTo compare one to one, we use to column-binding function cbind() (compare with c()). You also see here that the values of the factors are integers, not characters as for the first vector:\n\ncbind(Countries,Countries_f,Countries_f2,Countries_f3, Countries_f4)\n#&gt;      Countries Countries_f Countries_f2 Countries_f3 Countries_f4\n#&gt; [1,] \"Romania\" \"4\"         \"4\"          \"3\"          \"3\"         \n#&gt; [2,] \"Russia\"  \"5\"         \"5\"          NA           NA          \n#&gt; [3,] \"Morocco\" \"3\"         \"1\"          \"1\"          \"1\"         \n#&gt; [4,] \"Iran\"    \"2\"         \"3\"          \"4\"          \"4\"         \n#&gt; [5,] \"France\"  \"1\"         \"2\"          \"2\"          \"2\"\n\n\n\n6.3.2 Ordering a factor based on occurrence:\nTo make a more realistic case, we have scraped the Wikipedia table indicating the Tour de France winners since 1903. See “TourDeFrance” in the data folder for the data and scraping script (from R). The data is in the form of a data.frame and saved as a RDS (an effective way to save R objects onto your disc). See the relevant chapters for data.frame and saving to file later.\nThere was no Tour de France during World War I and II and no winner from 1999 to 2005 because Lance Armstrong cheated. Hence we have a series of ” - ” in our levels, which are not interesting, and thus not considered when we factor our vector. This doesn’t remove the vector elements but creates several NA’s.\n\nLeTour_df&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nWinners&lt;-factor(LeTour_df$Country, exclude = \"—\")\nWinners\n#&gt;   [1] France        France        France        France        France       \n#&gt;   [6] France        Luxembourg    France        France        Belgium      \n#&gt;  [11] Belgium       Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [16] &lt;NA&gt;          Belgium       Belgium       Belgium       Belgium      \n#&gt;  [21] France        Italy         Italy         Belgium       Luxembourg   \n#&gt;  [26] Luxembourg    Belgium       France        France        France       \n#&gt;  [31] France        France        Belgium       Belgium       France       \n#&gt;  [36] Italy         Belgium       &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt;  [41] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          France       \n#&gt;  [46] Italy         Italy         Switzerland   Switzerland   Italy        \n#&gt;  [51] France        France        France        France        France       \n#&gt;  [56] Luxembourg    Spain         Italy         France        France       \n#&gt;  [61] France        France        Italy         France        France       \n#&gt;  [66] Netherlands   Belgium       Belgium       Belgium       Belgium      \n#&gt;  [71] Spain         Belgium       France        Belgium       France       \n#&gt;  [76] France        France        Netherlands   France        France       \n#&gt;  [81] France        France        France        United States Ireland      \n#&gt;  [86] Spain         United States United States Spain         Spain        \n#&gt;  [91] Spain         Spain         Spain         Denmark       Germany      \n#&gt;  [96] Italy         &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;         \n#&gt; [101] &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          Spain         Spain        \n#&gt; [106] Spain         Spain         Luxembourg    Australia     Great Britain\n#&gt; [111] Great Britain Italy         Great Britain Great Britain Great Britain\n#&gt; [116] Great Britain Colombia      Slovenia      Slovenia      Denmark      \n#&gt; [121] Denmark       Slovenia     \n#&gt; 15 Levels: Australia Belgium Colombia Denmark France Germany ... United States\n\nlength(Winners) #this includes NA's!\n#&gt; [1] 122\nlevels(Winners)\n#&gt;  [1] \"Australia\"     \"Belgium\"       \"Colombia\"      \"Denmark\"      \n#&gt;  [5] \"France\"        \"Germany\"       \"Great Britain\" \"Ireland\"      \n#&gt;  [9] \"Italy\"         \"Luxembourg\"    \"Netherlands\"   \"Slovenia\"     \n#&gt; [13] \"Spain\"         \"Switzerland\"   \"United States\"\n\nSuppose you want to to know and plot how many times each of the 15 countries (who won Le Tour at least once) won.\nFrequencies can be observed directly from a basic plot:\n\nplot(Winners,las=2)\n\n\n\n\n\n\n\n\nGet the victories counts in a vector can be made with table(), which applies to anything (numeric or character) that can be coerced to a factor\n\ntable(Winners)\n#&gt; Winners\n#&gt;     Australia       Belgium      Colombia       Denmark        France \n#&gt;             1            18             1             3            36 \n#&gt;       Germany Great Britain       Ireland         Italy    Luxembourg \n#&gt;             1             6             1            10             5 \n#&gt;   Netherlands      Slovenia         Spain   Switzerland United States \n#&gt;             2             3            12             2             3\n\nBoth the plot and counts however follow the order of the levels, which is alphabetical\nOne possibility to improve the graph is to change the order of the levels based on the count table and redoing the graph\n\n#Vector of levels in a new order\nLevelsFrq&lt;-levels(Winners)[order(table(Winners), decreasing = TRUE)]\n  #note the order function and the square brackets\nLevelsFrq\n#&gt;  [1] \"France\"        \"Belgium\"       \"Spain\"         \"Italy\"        \n#&gt;  [5] \"Great Britain\" \"Luxembourg\"    \"Denmark\"       \"Slovenia\"     \n#&gt;  [9] \"United States\" \"Netherlands\"   \"Switzerland\"   \"Australia\"    \n#&gt; [13] \"Colombia\"      \"Germany\"       \"Ireland\"\n\n#Use that order when creating the factor from the character vector\nWinnersFrq&lt;-factor(Winners, levels = LevelsFrq) \n\n#plot\nplot(WinnersFrq,las=2)\n\n\n\n\n\n\n\n\n\n\n6.3.3 Ordered factors\nSometimes you will want to make a categorical data explicitly ordinal for graphical or statistical purpose . For example if you have used a Likert scale within a survey, it can be interesting it is stored as an ordered factor. You can order an already existing factor, using the function ordered(), which adds a logical flag to the factor to indicate it is order (see first example), or at the moment you create the factor using factor() (second example). Once the factor is ordered, the levels are displayed in order and separated by a ” &lt; “. Also a new class,”ordered”, is added to the object.\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") #in the expected order\nResponses&lt;-c(\"Neither agree nor disagree\", \"Disagree\", \"Disagree\", \"Agree\", \"Agree\", \"Neither agree nor disagree\", \"Strongly agree\", \"Strongly Agree\", \"Disagree\")\nResponses_f&lt;-factor(Responses, levels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree Disagree Neither agree nor disagree ... Strongly agree\n\nResponses_f2&lt;-ordered(Responses_f)\nResponses_f2 #Notice that if we don't provide again the levels, it is made from the existing ones, so there is only 4 levels now (\"Strongly Disagree\" not being answered)\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; Levels: Disagree &lt; Neither agree nor disagree &lt; Agree &lt; Strongly agree\n\nResponses_f3&lt;-factor(Responses, levels=LikertScale, ordered = TRUE)\nResponses_f3\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             &lt;NA&gt;                      \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nclass(Responses_f3) #see the additional class\n#&gt; [1] \"ordered\" \"factor\"\n\nIn the above example you will have noticed a NA, due to some misspelling. This is a good case to remind that categorical data should not necessarily rely on character strings. In geography we often need to treat countries, regions, municipalities whose names can be very complicated, especially in multilingual context. It is good advise to rather use codes for geographical units (e.g. BE351, BE352, BE353, for the NUTS 3 classification of Belgium), land use (e.g. https://land.copernicus.eu/content/corine-land-cover-nomenclature-guidelines/html) and relate them to a corresponding table with proper labels.\nIt is also true for other types of categories, including Likert scale. An option is to use integer values (or characters for geographical codes) together with a series of labels. You add the labels at the moment of creating the factor:\n\nLikertScale&lt;-c(\"Strongly disagree\", \"Disagree\",\"Neither agree nor disagree\",\n\"Agree\",\"Strongly agree\") \nLikertLevels&lt;-c(1L,2L,3L,4L,5L) #would work as well with numeric c(1,2,3,4,5)\nResponses&lt;-c(3L, 2L, 2L, 4L, 4L, 3L, 5L, 5L, 2L)\n\nResponses_f&lt;-factor(Responses, ordered=TRUE, levels=LikertLevels, labels=LikertScale)\nResponses_f\n#&gt; [1] Neither agree nor disagree Disagree                  \n#&gt; [3] Disagree                   Agree                     \n#&gt; [5] Agree                      Neither agree nor disagree\n#&gt; [7] Strongly agree             Strongly agree            \n#&gt; [9] Disagree                  \n#&gt; 5 Levels: Strongly disagree &lt; Disagree &lt; ... &lt; Strongly agree\n\nThe result is the same as previously, but you avoid typos when inputing the responses and can also adapt the labels at will, without changing the data itself.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#operations-on-vectors",
    "href": "022_vectors.html#operations-on-vectors",
    "title": "6  Vectors",
    "section": "6.4 Operations on vectors",
    "text": "6.4 Operations on vectors\n\n6.4.1 Arithmetic operations and recycling\nR is a calculator and perform the arithmetic operations + - * / ^\nTwo numeric vectors of the same length can be added, multiplied, etc. When vectors of different length are provided, the shorter one is recycled until the end of the longer vector. Usually a vector of length 1 is recycled, thus allowing R to add, divide,etc. a scalar to a vector in the same way as between two vectors (in a “parallel way”). But any shorter vector is also recycled (you get a warning though). See:\n\nX&lt;-c(0,1,2,3,4,5)\nX^2\n#&gt; [1]  0  1  4  9 16 25\nY&lt;-c(10,9,8,7,6,5)\n\nX+Y\n#&gt; [1] 10 10 10 10 10 10\nY/X\n#&gt; [1]      Inf 9.000000 4.000000 2.333333 1.500000 1.000000\nX*c(0,1)\n#&gt; [1] 0 1 0 3 0 5\n\nYou can also apply a series of common mathematical functions to any numeric vector.\n\na&lt;-c(-2,-1,0,1,2,6,10)\nabs(a) #absolute\n#&gt; [1]  2  1  0  1  2  6 10\n\na^(-1) #inverse\n#&gt; [1] -0.5000000 -1.0000000        Inf  1.0000000  0.5000000  0.1666667  0.1000000\n\nlog(a) #ln or Natural logarithm (or Napierian logarithm)\n#&gt; Warning in log(a): NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.6931472 1.7917595 2.3025851\nlog10(a) # base 10 logarithm\n#&gt; Warning: NaNs produced\n#&gt; [1]       NaN       NaN      -Inf 0.0000000 0.3010300 0.7781513 1.0000000\n\nexp(a) #base e exponential\n#&gt; [1] 1.353353e-01 3.678794e-01 1.000000e+00 2.718282e+00 7.389056e+00\n#&gt; [6] 4.034288e+02 2.202647e+04\n10^a #base 10 exponential\n#&gt; [1] 1e-02 1e-01 1e+00 1e+01 1e+02 1e+06 1e+10\n\nsqrt(a) #square root\n#&gt; Warning in sqrt(a): NaNs produced\n#&gt; [1]      NaN      NaN 0.000000 1.000000 1.414214 2.449490 3.162278\n\nSee Crawley (2012) p.11 for more examples:\n\n\n\nCrawley Table 2.1\n\n\nBe careful that these functions must lead to proper results. As you can see from the warnings. “NaN” stands for “Not a Number” and is considered as NA (“not available”). Which is not the case of infinity, which is a numeric\n\na&lt;-c(-2,-1,0,1,2,6,10)\nis.na(NaN)\n#&gt; [1] TRUE\nis.na(-Inf)\n#&gt; [1] FALSE\nis.finite(log10(a))\n#&gt; Warning: NaNs produced\n#&gt; [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\n6.4.2 Euclidean division and rounding\nThe modulo operation for integers is obtained with %/% to get the integral part of the Euclidean division and %% to get the remainder. This the way you will know an integer is odd or even. The function can also be applied to numeric vectors with decimals.\n\nc(1,3,5,44,444,4444)%/%2\n#&gt; [1]    0    1    2   22  222 2222\nc(1,3,5,44,444,4444)%%2 #remainder of dividing by 2, thus indicating odd/even\n#&gt; [1] 1 1 1 0 0 0\n99.99%/%2\n#&gt; [1] 49\n99.99%%2\n#&gt; [1] 1.99\n\nFor ratio and interval numbers with decimals you sometimes need to get rid of the decimals or display only a part of them Examine the following:\n\nx&lt;-c(33.33, 666.166, 50.5, 49.5)\ntrunc(x) #compares with as.integer() but does not change the class to integer\n#&gt; [1]  33 666  50  49\nround(x) #note the rounding of a 5 to the even digit (international standard) \n#&gt; [1]  33 666  50  50\nround(x, digits=2)\n#&gt; [1]  33.33 666.17  50.50  49.50\nceiling(x)\n#&gt; [1]  34 667  51  50\nfloor(x)\n#&gt; [1]  33 666  50  49\n\nsignif(x, digits = 5)\n#&gt; [1]  33.33 666.17  50.50  49.50\n\nWith geographical data, ceiling() can for example be applied to latitudes and longitudes in order to make grids. A simple example is how to obtain (theoretical) time zones from a set of longitudes.\nLet’s take the opportunity to learn about set.seed() and about uniform random number generation runif():\n\nset.seed(101)\nLongitudes&lt;-runif(n=10,min=-180, max=180)\nLongitudes\n#&gt;  [1]  -46.00858 -164.22307   75.48625   56.76854  -90.05194  -71.98026\n#&gt;  [7]   30.55199  -59.95183   43.92431   16.49828\n\nceiling(Longitudes*24/360)\n#&gt;  [1]  -3 -10   6   4  -6  -4   3  -3   3   2\n\n\n\n\nWikipedia, Time Zones\n\n\n\n\n6.4.3 Logical and Boolean operations\nA series of operations return a logical vector, i.e. a vector of Boolean values TRUE and FALSE.\nThose operations are &gt;, &gt;=, &lt;, &lt;=, ==, !=\nSome examples with numeric and character vectors:\nIMPORTANT: Use == for comparison, not =, which is an assignment!\n\nc(1,2,3) &lt; c(2,3,3)\n#&gt; [1]  TRUE  TRUE FALSE\nc(1,2,3) &gt;= 3 #see right hand side is recycled and applied \"one to one\"\n#&gt; [1] FALSE FALSE  TRUE\n\"Bernadette, elle est très chouette\" == \"Bernadette, elle est très chouette\"\n#&gt; [1] TRUE\n\"Bernadette, elle est très chouette\" != \"Mais sa cousine, elle est divine\"\n#&gt; [1] TRUE\n\"Julian\" &gt;= \"Julien\" #alphabetical order\n#&gt; [1] FALSE\n\nBy the way we see here vectors of the logical class:\n\nclass(\"Julian\" &gt;= \"Julien\")\n#&gt; [1] \"logical\"\n\nIn addition to those comparisons, many functions, especially structured as “is.xxx” return a TRUE or FALSE and are very useful within you data management process. You may make quite some use also of the %in% function, when you have a long vector, to check whether a particular value (or several) is present within a vector.\n\nis.numeric(\"a\")\n#&gt; [1] FALSE\nis.numeric(\"2\")\n#&gt; [1] FALSE\nis.numeric(3L)\n#&gt; [1] TRUE\nis.numeric(FALSE)\n#&gt; [1] FALSE\n!is.numeric(FALSE) #Note the negation here!\n#&gt; [1] TRUE\n\n\"urban\" %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1] TRUE\nc(\"urban\",\"industry\") %in% c(\"urban\",\"agriculture\",\"water\",\"forest\")\n#&gt; [1]  TRUE FALSE\n\nBoolean values are usually (always?) coerced to a 1 and 0, in case an arithmetic operation is then demanded.\n\nmean(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 0.75\nsum(c(TRUE,FALSE,TRUE,TRUE))\n#&gt; [1] 3\n3*(c(TRUE,FALSE)+TRUE)\n#&gt; [1] 6 3\n\nA set of functions then apply specifically to the Boolean values TRUE or FALSE, i.e. to logical vectors.\nThese logical operators are: !, &, |, xor and typically used to select elements that match 2 or more conditions.\nThe graphic below, reproduced from R for Data Science (highly recommended!), demonstrate their outcome for 2 sets and an example is provided after creating 2 logical vectors, x and y from the LETTERS character vector.\n\n\n\nsource: Fig.12.1 from https://r4ds.hadley.nz/logicals\n\n\n\nLETTERS[1:6]\n#&gt; [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\nx&lt;-LETTERS[1:6]&lt;\"E\"\ny&lt;-LETTERS[1:6]&gt;\"B\"\nx\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\ny\n#&gt; [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx & !y\n#&gt; [1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n!x & y\n#&gt; [1] FALSE FALSE FALSE FALSE  TRUE  TRUE\nx & y\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE FALSE\nx | y\n#&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE\nxor(x,y)\n#&gt; [1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#vectors-functions",
    "href": "022_vectors.html#vectors-functions",
    "title": "6  Vectors",
    "section": "6.5 Vectors’ functions",
    "text": "6.5 Vectors’ functions\nA number of functions are evaluated over an entire vector (vectorisation is there to avoid loops) and used to describe and understand the distribution of values. They apply mostly to numeric vectors but also to logicals (being 0 or 1 anyway) as well as characters, where possible (based on alphabetical order).\n\n6.5.1 Range, cumulative values, positions and sorting\n\nExamples of functions evaluated over an entire vector\n\n\nmin(x)\ncummin(x)\nwhich.min(x)\nsort(x)\n\n\nmax(x)\ncummax(x)\nwhich.max(x)\norder(x)\n\n\nrange(x)\n\n\nrank(x)\n\n\nsum(x)\ncumsum(x)\n\n\n\n\n\nLet’s explore some of those for x being a numeric, a logical and character. In class exploration.\nIt is sometimes difficult to remember differences between sort(x),order(x) and rank(x)\nSee two examples below\n\nset.seed(101)\nx&lt;-round(runif(10,100,200))\nx\n#&gt;  [1] 137 104 171 166 125 130 158 133 162 155\n\ncbind(Original=x,Sorted=sort(x),Rank=rank(x),Order=order(x))\n#&gt;       Original Sorted Rank Order\n#&gt;  [1,]      137    104    5     2\n#&gt;  [2,]      104    125    1     5\n#&gt;  [3,]      171    130   10     6\n#&gt;  [4,]      166    133    9     8\n#&gt;  [5,]      125    137    2     1\n#&gt;  [6,]      130    155    3    10\n#&gt;  [7,]      158    158    7     7\n#&gt;  [8,]      133    162    4     9\n#&gt;  [9,]      162    166    8     4\n#&gt; [10,]      155    171    6     3\n\n\nM&lt;-month.name[1:6]\nM\n#&gt; [1] \"January\"  \"February\" \"March\"    \"April\"    \"May\"      \"June\"\n\ncbind(Months=M,Sorted=sort(M),Rank=rank(M),Order=order(M))\n#&gt;      Months     Sorted     Rank Order\n#&gt; [1,] \"January\"  \"April\"    \"3\"  \"4\"  \n#&gt; [2,] \"February\" \"February\" \"2\"  \"2\"  \n#&gt; [3,] \"March\"    \"January\"  \"5\"  \"1\"  \n#&gt; [4,] \"April\"    \"June\"     \"1\"  \"6\"  \n#&gt; [5,] \"May\"      \"March\"    \"6\"  \"3\"  \n#&gt; [6,] \"June\"     \"May\"      \"4\"  \"5\"\n\nTo those operations, we should add all univariate statistics such as mean(), median(), var(), quantile(), but we leave them aside for now as they are introduced later with univariate statistics and distributions.\nSpecific to logical vectors, the any() and all() functions are particularly useful in the case you check a very long vector with only very few having a TRUE or a FALSE.\nSee basic examples:\n\nany(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] TRUE\nany(c(TRUE,TRUE,TRUE,TRUE))\n#&gt; [1] TRUE\nall(c(TRUE,TRUE,TRUE,FALSE))\n#&gt; [1] FALSE\n\nAnd an example where we suppose a random set of values from a normal distribution and we want to check manually whether there is an upper outlier. Typically (as in boxplots) this outlier is calculated as being any value above the 3rd quartile plus 1.5 times the interquartile range.\n\nset.seed(102)\nx&lt;-rnorm(100)\nq75&lt;-quantile(x, p=0.75)\niqr&lt;-IQR(x)\nany(x&gt;(q75+1.5*iqr))\n#&gt; [1] TRUE\nboxplot(x) #\n\n\n\n\n\n\n\nx[x&gt;(q75+1.5*iqr)] #to identify them\n#&gt; [1] 3.114333\n\nset.seed(101) #Check with this seed!\n\n\n\n6.5.2 Summary\nOne of the most used function for analysis (if not THE most used) is summary(). We will see its result may change substantially based on the object class it is applied to. Its basic functioning for a simple numeric vector and character vector is:\n\nsummary(c(1,2,3,4,NA,6))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;     1.0     2.0     3.0     3.2     4.0     6.0       1\nsummary(c(\"A\",\"B\",\"C\",NA,NA,\"F\"))\n#&gt;    Length     Class      Mode \n#&gt;         6 character character\n\n\n\n6.5.3 Element-wise functions\nOther functions apply to two or more vectors. They need an x and a y vector as input, e.g. cor(x,y). Most of those you will encounter use two vectors of the same length and type, i.e. no recycling, which leads us to the notion of a data frame (see next chapter).\nAt this time, we show only two simple “parallel” or “element-wise” computations: pmin() and pmax(), which can be useful for example when there is a repeated measure for a given set of individuals. Cases are not rare when you make a new vector (or column in a data frame) based on such element-wise calculations. (Yet you would probably assemble the data into a data frame first and then make a row-wise calculation).\n\nt1&lt;-c(25,35,45,55)\nt2&lt;-c(26,36,44,54)\nt3&lt;-c(27,34,43,56)\n\npmin(t1,t2,t3)\n#&gt; [1] 25 34 43 54\npmax(t1,t2,t3)\n#&gt; [1] 27 36 45 56",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#indexing-and-subsetting",
    "href": "022_vectors.html#indexing-and-subsetting",
    "title": "6  Vectors",
    "section": "6.6 Indexing and subsetting",
    "text": "6.6 Indexing and subsetting\nNow you have seen that vectors are the key elements in R, you will still want to access its elements individually or parts of it based on positions or conditions\nSquare brackets I used to get into vector elements by their position (or by their name if named).\nYou can supply one position but also several positions and ranges. Examine the following:\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx #all\n#&gt;  [1] 3 1 9 7 8 2 3 6 3 4\nx[4] #4th element\n#&gt; [1] 7\nx[6:8] #a sequence\n#&gt; [1] 2 3 6\nx[c(4,6:8)] #both\n#&gt; [1] 7 2 3 6\nx[-4] #all but the 4th element\n#&gt; [1] 3 1 9 8 2 3 6 3 4\nx[-length(x)] #all but the last one\n#&gt; [1] 3 1 9 7 8 2 3 6 3\nx[-((length(x)-1):length(x))]  #all but the last two\n#&gt; [1] 3 1 9 7 8 2 3 6\n\nThis is very flexible because you can use any vector representing an index (position) to make a new vector.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nchicken&lt;-c(10,10,10,4,2,3)\nx[chicken]\n#&gt; [1] 4 4 4 7 1 9\n\nSuch a vector can be a logical, then the elements where the logical is TRUE are returned.\n\nx&lt;-c(3,1,9,7,8,2,3,6,3,4)\nx&gt;3\n#&gt;  [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\nx[x&gt;3]\n#&gt; [1] 9 7 8 6 4\n\nx %% 2 == 0\n#&gt;  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\nx[x %% 2 == 0] #even numbers only\n#&gt; [1] 8 2 6 4\n\nx[x&gt;mean(x)]\n#&gt; [1] 9 7 8 6\n\nx[as.logical(c(0,1))] #?? recyling is in effect here\n#&gt; [1] 1 7 2 6 4\n\nPossibilities are endless, especially if you think the indexing vector can come from another vector than x.\nVectors are sometimes named, and names the used to retrieve the data:\n\ny&lt;-c(1000,1500,900,1200)\nnames(y)&lt;-c(\"UK\",\"BE\",\"LU\",\"FR\")\n\ny[\"LU\"]\n#&gt;  LU \n#&gt; 900\n\nLast but not least, once selected an element can then be assigned a new value:\n\nz&lt;-c(0,0,0,4,2,3,0)\nz\n#&gt; [1] 0 0 0 4 2 3 0\n\nz[2]&lt;-5\nz\n#&gt; [1] 0 5 0 4 2 3 0\n\nz[z==0]&lt;-NA  #a very much used use case\nz\n#&gt; [1] NA  5 NA  4  2  3 NA",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "022_vectors.html#sequences-and-repetitions",
    "href": "022_vectors.html#sequences-and-repetitions",
    "title": "6  Vectors",
    "section": "6.7 Sequences and repetitions",
    "text": "6.7 Sequences and repetitions\nThere are many instances where you will need to create a repeated set of values or sequence of values. The functions seq() and rep() are used extensively.\nImagine as a first example you have 5 regions named 2,4,6,8,10 and each of them send a flow of cars to each other, thus building a 5 x 5 matrix =25 records of flows.\n\nregions&lt;-seq(from=2, to=10, by=2)\nregions\n#&gt; [1]  2  4  6  8 10\norigins&lt;-rep(regions, 5)\norigins\n#&gt;  [1]  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10  2  4  6  8 10\ndestinations&lt;-rep(regions, each=5)\ndestinations\n#&gt;  [1]  2  2  2  2  2  4  4  4  4  4  6  6  6  6  6  8  8  8  8  8 10 10 10 10 10\nflows&lt;-paste(\"from \",origins, \" to \", destinations)\nflows\n#&gt;  [1] \"from  2  to  2\"   \"from  4  to  2\"   \"from  6  to  2\"   \"from  8  to  2\"  \n#&gt;  [5] \"from  10  to  2\"  \"from  2  to  4\"   \"from  4  to  4\"   \"from  6  to  4\"  \n#&gt;  [9] \"from  8  to  4\"   \"from  10  to  4\"  \"from  2  to  6\"   \"from  4  to  6\"  \n#&gt; [13] \"from  6  to  6\"   \"from  8  to  6\"   \"from  10  to  6\"  \"from  2  to  8\"  \n#&gt; [17] \"from  4  to  8\"   \"from  6  to  8\"   \"from  8  to  8\"   \"from  10  to  8\" \n#&gt; [21] \"from  2  to  10\"  \"from  4  to  10\"  \"from  6  to  10\"  \"from  8  to  10\" \n#&gt; [25] \"from  10  to  10\"\n\nIf you like to give a unique number to identify each flow with a number starting at 100 and don’t calculate that you have 25 of them, seq_along`is your tool:\n\nseq_along(flows)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\ncbind(seq_along(flows),flows)\n#&gt;            flows             \n#&gt;  [1,] \"1\"  \"from  2  to  2\"  \n#&gt;  [2,] \"2\"  \"from  4  to  2\"  \n#&gt;  [3,] \"3\"  \"from  6  to  2\"  \n#&gt;  [4,] \"4\"  \"from  8  to  2\"  \n#&gt;  [5,] \"5\"  \"from  10  to  2\" \n#&gt;  [6,] \"6\"  \"from  2  to  4\"  \n#&gt;  [7,] \"7\"  \"from  4  to  4\"  \n#&gt;  [8,] \"8\"  \"from  6  to  4\"  \n#&gt;  [9,] \"9\"  \"from  8  to  4\"  \n#&gt; [10,] \"10\" \"from  10  to  4\" \n#&gt; [11,] \"11\" \"from  2  to  6\"  \n#&gt; [12,] \"12\" \"from  4  to  6\"  \n#&gt; [13,] \"13\" \"from  6  to  6\"  \n#&gt; [14,] \"14\" \"from  8  to  6\"  \n#&gt; [15,] \"15\" \"from  10  to  6\" \n#&gt; [16,] \"16\" \"from  2  to  8\"  \n#&gt; [17,] \"17\" \"from  4  to  8\"  \n#&gt; [18,] \"18\" \"from  6  to  8\"  \n#&gt; [19,] \"19\" \"from  8  to  8\"  \n#&gt; [20,] \"20\" \"from  10  to  8\" \n#&gt; [21,] \"21\" \"from  2  to  10\" \n#&gt; [22,] \"22\" \"from  4  to  10\" \n#&gt; [23,] \"23\" \"from  6  to  10\" \n#&gt; [24,] \"24\" \"from  8  to  10\" \n#&gt; [25,] \"25\" \"from  10  to  10\"\n\nWith seq you can also partition a range of values into a given number of intervals without knowing what the value of each interval is. Suppose you have 14 teams to provide water to Marathonians. At which distance are you going to place those teams along the path, knowing you need one at the end and one at the start?\n\nStands&lt;-seq(0,42195,length=14)\n\nAnd suppose that at every third stand you provide some snacks, not just water. So you repeat the pattern “water,water,food” until the end of the vector of Stands\n\nSnacks&lt;-rep(c(FALSE,FALSE,TRUE), length.out = length(Stands))\nSnacks\n#&gt;  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n#&gt; [13] FALSE FALSE\n\nYou also use sequences to display some functions you like to understand better, such as a polynomial or quadratic.\n\nx&lt;-seq(from=1, to=500, length = 100) #m from school\ny&lt;-1000 - 0.001*x^2 + 0.3*x #house rental value\nplot(x=x,y=y)\n\n\n\n\n\n\n\n\n\n#although you could use curve() here as well:\ncurve(1000 - 0.001*x^2 + 0.3*x, from=1, to=500)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html",
    "href": "023_df_lst.html",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1 Data frames",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#data-frames",
    "href": "023_df_lst.html#data-frames",
    "title": "7  Data frames and lists",
    "section": "",
    "text": "7.1.1 A Data frame … your beloved spreadsheet\n\ndf&lt;-data.frame() #an empty one\n\ndf&lt;-data.frame(a = 1:5,\n           b = letters[1:5],\n           c = rnorm(n = 5))\ndf\n#&gt;   a b          c\n#&gt; 1 1 a -1.2756248\n#&gt; 2 2 b -0.9169839\n#&gt; 3 3 c  1.0106596\n#&gt; 4 4 d  0.1210808\n#&gt; 5 5 e  0.7231901\n\nSee how it is summarized. Basically summarizing each vector in columns.\n\nsummary(df)\n#&gt;        a          b                   c           \n#&gt;  Min.   :1   Length:5           Min.   :-1.27562  \n#&gt;  1st Qu.:2   Class :character   1st Qu.:-0.91698  \n#&gt;  Median :3   Mode  :character   Median : 0.12108  \n#&gt;  Mean   :3                      Mean   :-0.06754  \n#&gt;  3rd Qu.:4                      3rd Qu.: 0.72319  \n#&gt;  Max.   :5                      Max.   : 1.01066\n\n\n\n7.1.2 Accessing and subsetting\n!!! THIS IS EXTREMELY IMPORTANT !!! !!! Don’t forget the comma\nIdentifying\n\ndf[,\"b\"] #all rows but only the column named \"b\"\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\ndf$b #same !\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nSubsetting based on position\n\ndf$c[1:2] #subsetting a vector: only first 2 values\n#&gt; [1] -1.2756248 -0.9169839\ndf[1:2,\"b\"]\n#&gt; [1] \"a\" \"b\"\ndf[1:2,] #first 2 records, all variables. Don't forget the comma !\n#&gt;   a b          c\n#&gt; 1 1 a -1.2756248\n#&gt; 2 2 b -0.9169839\n\nSubsetting specific rows (not range as above):\n\ndf[1:3,\"b\"]\n#&gt; [1] \"a\" \"b\" \"c\"\ndf[c(1,3),\"b\"]\n#&gt; [1] \"a\" \"c\"\n\nEach dataframe column must have the same number of elements.\n(Enjoy this condition after thinking of how many times you had misaligned and columns of different lengths in Ms Exc..)\n\nz&lt;-c(\"Mom\",\"Dad\",3) #remember each vector as only one type of data, so this is coerced to character\n\ndf$z&lt;-z #Should not work becouse of different length\n#&gt; Error in `$&lt;-.data.frame`(`*tmp*`, z, value = c(\"Mom\", \"Dad\", \"3\")): replacement has 3 rows, data has 5\n\n\nlength(df$a)\n#&gt; [1] 5\nlength(z)\n#&gt; [1] 3\n\n#Beware. Length applied to the whole df means the number of columns!\nlength(df) #i.e. a b and c\n#&gt; [1] 3\n\nWhile length is general, for a data frame you can rather compute number of rows and columns this way\n\nnrow(df)\n#&gt; [1] 5\nncol(df)\n#&gt; [1] 3\ndim(df)\n#&gt; [1] 5 3\nnrow(df)==dim(df)[2] #What do you think?\n#&gt; [1] FALSE\n\n#But these won't work for a vector:\ndim(df$a)\n#&gt; NULL\nnrow(df$a)\n#&gt; NULL\n\nOften times data frames have more complicated column names. It is useful to access those directly.\nThis is also the way you change the name of a column:\n\nnames(df)[2]&lt;-\"blabla\"\ndf\n#&gt;   a blabla          c\n#&gt; 1 1      a -1.2756248\n#&gt; 2 2      b -0.9169839\n#&gt; 3 3      c  1.0106596\n#&gt; 4 4      d  0.1210808\n#&gt; 5 5      e  0.7231901",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "023_df_lst.html#lists",
    "href": "023_df_lst.html#lists",
    "title": "7  Data frames and lists",
    "section": "7.2 Lists",
    "text": "7.2 Lists\nA List is a more general object than a dataframe.\nAll “columns” are not necessarily\n\nof the same length\nnor of the same class\n\n\nmylist&lt;-list(a = 1:5,\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3 4 5\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -1.2901138 -0.6120341 -1.4536780 -0.4802886 -0.5749397\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 5      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nmylist&lt;-list(a = 1:3,#we removed 2 elements here\n             b = letters[1:5],\n             c = rnorm(n = 5))\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -0.2116789 -0.1707597  0.3304201  0.8995028 -0.3394525\nsummary(mylist)\n#&gt;   Length Class  Mode     \n#&gt; a 3      -none- numeric  \n#&gt; b 5      -none- character\n#&gt; c 5      -none- numeric\n\n\nlength(mylist) #number of elements in list\n#&gt; [1] 3\nlength(mylist$b) #number of objects within that element of the list\n#&gt; [1] 5\n\n\n7.2.1 Into the lists: [[ ]] vs [ ]\n\nmylist[2] # getting the list element displayed\n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[2])  # you see it is a list element\n#&gt; [1] \"list\"\nmylist[[2]] # getting the corresponding vector\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nclass(mylist[[2]]) # you now see it is a character vector\n#&gt; [1] \"character\"\n\nAnd now subsetting:\n\nmylist\n#&gt; $a\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $b\n#&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n#&gt; \n#&gt; $c\n#&gt; [1] -0.2116789 -0.1707597  0.3304201  0.8995028 -0.3394525\nmylist[[2]][1] #1st element of the 2nd element of the list\n#&gt; [1] \"a\"\nmylist[[1]][2] #2nd element of the 1st element of the list\n#&gt; [1] 2\nM&lt;-mylist[[3]]\nM[1] #Subsetting just as any vector\n#&gt; [1] -0.2116789",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data frames and lists</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html",
    "href": "024_df_functions.html",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "8.1 What is in a function? BMI example\npaste.heightweight&lt;-function(h,w){\n  print(paste(h,w))\n  }\npaste.heightweight(1.8,80) #you provide the 2 arguments and get the output\n#&gt; [1] \"1.8 80\"\nNow let’s do the computation with the BMI calculation with a new function\nbmi.calc&lt;-function(h,w){w/h^2}\nwhich we apply\nbmi.calc(1.8,80)\n#&gt; [1] 24.69136\nA function can take a sequence of processes (e.g compute, rounds, concatenate a sentence,…) and then returns the result of the last process.\nExample\nbmi.calc.text&lt;-function(h,w){\n  b&lt;-w/h^2\n  brounded&lt;-round(b)\n  paste(\"My BMI is\", brounded, \"kg/m2\")\n}\nbmi.calc.text(1.8,80)\n#&gt; [1] \"My BMI is 25 kg/m2\"\nFor clarity the outcome of the function can be put in a return()\nbmi.calc&lt;-function(h,w){\n  return(round(w/h^2))\n}\nbmi.calc(1.8,80)\n#&gt; [1] 25",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "href": "024_df_functions.html#what-is-in-a-function-bmi-example",
    "title": "8  Working with data frames and functions",
    "section": "",
    "text": "Let’s create a BMI function\nFirst a simple function that simply prints a given height and weight",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "href": "024_df_functions.html#applying-a-function-to-a-data-frame-column",
    "title": "8  Working with data frames and functions",
    "section": "8.2 Applying a function to a data frame column",
    "text": "8.2 Applying a function to a data frame column\nLet’s create a 2nd function to transfors degrees from Celsius to Fahrenheit\nSimpler with a single argument (x):\n\ncelsius2fahrenheit&lt;-function(x){round(32+(x*9/5))}\n\ncelsius2fahrenheit(25) #25 celsius degree is thus \n#&gt; [1] 77\n\nWhich we now apply to a series of values stored in a column within a data frame\n\nmytable&lt;-data.frame(A=c(21,22,23,24,25,26,27))\nmytable$F&lt;-celsius2fahrenheit(mytable[,\"A\"])\nmytable\n#&gt;    A  F\n#&gt; 1 21 70\n#&gt; 2 22 72\n#&gt; 3 23 73\n#&gt; 4 24 75\n#&gt; 5 25 77\n#&gt; 6 26 79\n#&gt; 7 27 81",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#data-frames-and-nas",
    "href": "024_df_functions.html#data-frames-and-nas",
    "title": "8  Working with data frames and functions",
    "section": "8.3 Data frames and NA’s",
    "text": "8.3 Data frames and NA’s\nComputation of a new column from columns of a dataframe\n\nmytable$G&lt;-mytable$A+mytable$F #note: adding C and F temperature is nonsensical though\nmytable$Gsquare&lt;-mytable$G^2 #note how you write an exponent \"^\" in R\nmytable$A*mytable$F # or a multiplication \"*\"\n#&gt; [1] 1470 1584 1679 1800 1925 2054 2187\n\nSimilarly we can apply our BMI computation to a data frame with heights and weights\n\nbmidf&lt;-data.frame(\n  h=c(1.8,1.7,2,1.9),\n  w=c(70,70,95,100))\n\nWe add the result of computing BMI directly as a new column “BMI” in our data.frame\n\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n\nNA is for unknowns !\nSuppose the 2nd person of our sample didn’t share his/her weight with us\n\nbmidf$w[2]&lt;-NA #NA is for unknowns\nbmidf$BMI&lt;-bmi.calc(h=bmidf$h,\n                    w=bmidf$w)\n#You see the BMI could therefore not be computed\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nFor some functions you would still want to compute a value while ignoring the NA’s\nThe mean is a classical example\n\nmean(bmidf$h) #works\n#&gt; [1] 1.85\nmean(bmidf$w) #but returns NA because of one value not reported\n#&gt; [1] NA\n\nYou can explicitly ask to compute without the NA’s:\n\nmean(bmidf$w, na.rm=TRUE) #now works!\n#&gt; [1] 88.33333\n\nUsing complete cases\nFor some data frame made of surveyed values where different variables are filled in sparsely, it is important you get access only to entirely completed individuals\n\ncomplete.cases(bmidf) #returns a logical indicating whether the row \n#&gt; [1]  TRUE FALSE  TRUE  TRUE\n# has not a singleNA\nclass(complete.cases(bmidf))\n#&gt; [1] \"logical\"\n#Note that with logicals, TRUE is 1 and FALSE is zero. Thus\n\nsum(complete.cases(bmidf))\n#&gt; [1] 3\n\n#You can use this logical to subset the rows\n# and have a \"clean\" df\nbmidf2&lt;-bmidf[complete.cases(bmidf),] #read this as \"select complete cases rows with all columns\nbmidf2\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#environment-listing-and-management",
    "href": "024_df_functions.html#environment-listing-and-management",
    "title": "8  Working with data frames and functions",
    "section": "8.4 Environment listing and management",
    "text": "8.4 Environment listing and management\nWe have now created a bunch of objects which we can see in the Environment window of RStudio.\nIn the console we can also see them with\n\nls()\n#&gt; [1] \"bmi.calc\"           \"bmi.calc.text\"      \"bmidf\"             \n#&gt; [4] \"bmidf2\"             \"celsius2fahrenheit\" \"mytable\"           \n#&gt; [7] \"paste.heightweight\"\n\nAnd any of these objects can be removed with\n\nrm()\n#for example\nrm(mytable)\n\nIn the environment window of RStudio you also see the structure of objects (when displayed as a list not a grid)\nFrom the console you use the structure function str() to get the same info\n\nstr(bmidf)\n#&gt; 'data.frame':    4 obs. of  3 variables:\n#&gt;  $ h  : num  1.8 1.7 2 1.9\n#&gt;  $ w  : num  70 NA 95 100\n#&gt;  $ BMI: num  22 NA 24 28",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "024_df_functions.html#viewing-data-frame",
    "href": "024_df_functions.html#viewing-data-frame",
    "title": "8  Working with data frames and functions",
    "section": "8.5 Viewing data frame",
    "text": "8.5 Viewing data frame\n\nView(bmidf)\n\nis the most pleasant interactive way to view a data frame\nBut be careful if many rows or columns !\nThe classical console way is simply\n\nbmidf\n#&gt;     h   w BMI\n#&gt; 1 1.8  70  22\n#&gt; 2 1.7  NA  NA\n#&gt; 3 2.0  95  24\n#&gt; 4 1.9 100  28\n\nIn case of a large vector or df there will be a limited display of 1000 values (default) in console\nSuppose you have a vector of 1002 values\n\nmydf&lt;-data.frame(z=1:502, zrev=502:1)\nmydf\n#&gt;       z zrev\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; 4     4  499\n#&gt; 5     5  498\n#&gt; 6     6  497\n#&gt; 7     7  496\n#&gt; 8     8  495\n#&gt; 9     9  494\n#&gt; 10   10  493\n#&gt; 11   11  492\n#&gt; 12   12  491\n#&gt; 13   13  490\n#&gt; 14   14  489\n#&gt; 15   15  488\n#&gt; 16   16  487\n#&gt; 17   17  486\n#&gt; 18   18  485\n#&gt; 19   19  484\n#&gt; 20   20  483\n#&gt; 21   21  482\n#&gt; 22   22  481\n#&gt; 23   23  480\n#&gt; 24   24  479\n#&gt; 25   25  478\n#&gt; 26   26  477\n#&gt; 27   27  476\n#&gt; 28   28  475\n#&gt; 29   29  474\n#&gt; 30   30  473\n#&gt; 31   31  472\n#&gt; 32   32  471\n#&gt; 33   33  470\n#&gt; 34   34  469\n#&gt; 35   35  468\n#&gt; 36   36  467\n#&gt; 37   37  466\n#&gt; 38   38  465\n#&gt; 39   39  464\n#&gt; 40   40  463\n#&gt; 41   41  462\n#&gt; 42   42  461\n#&gt; 43   43  460\n#&gt; 44   44  459\n#&gt; 45   45  458\n#&gt; 46   46  457\n#&gt; 47   47  456\n#&gt; 48   48  455\n#&gt; 49   49  454\n#&gt; 50   50  453\n#&gt; 51   51  452\n#&gt; 52   52  451\n#&gt; 53   53  450\n#&gt; 54   54  449\n#&gt; 55   55  448\n#&gt; 56   56  447\n#&gt; 57   57  446\n#&gt; 58   58  445\n#&gt; 59   59  444\n#&gt; 60   60  443\n#&gt; 61   61  442\n#&gt; 62   62  441\n#&gt; 63   63  440\n#&gt; 64   64  439\n#&gt; 65   65  438\n#&gt; 66   66  437\n#&gt; 67   67  436\n#&gt; 68   68  435\n#&gt; 69   69  434\n#&gt; 70   70  433\n#&gt; 71   71  432\n#&gt; 72   72  431\n#&gt; 73   73  430\n#&gt; 74   74  429\n#&gt; 75   75  428\n#&gt; 76   76  427\n#&gt; 77   77  426\n#&gt; 78   78  425\n#&gt; 79   79  424\n#&gt; 80   80  423\n#&gt; 81   81  422\n#&gt; 82   82  421\n#&gt; 83   83  420\n#&gt; 84   84  419\n#&gt; 85   85  418\n#&gt; 86   86  417\n#&gt; 87   87  416\n#&gt; 88   88  415\n#&gt; 89   89  414\n#&gt; 90   90  413\n#&gt; 91   91  412\n#&gt; 92   92  411\n#&gt; 93   93  410\n#&gt; 94   94  409\n#&gt; 95   95  408\n#&gt; 96   96  407\n#&gt; 97   97  406\n#&gt; 98   98  405\n#&gt; 99   99  404\n#&gt; 100 100  403\n#&gt; 101 101  402\n#&gt; 102 102  401\n#&gt; 103 103  400\n#&gt; 104 104  399\n#&gt; 105 105  398\n#&gt; 106 106  397\n#&gt; 107 107  396\n#&gt; 108 108  395\n#&gt; 109 109  394\n#&gt; 110 110  393\n#&gt; 111 111  392\n#&gt; 112 112  391\n#&gt; 113 113  390\n#&gt; 114 114  389\n#&gt; 115 115  388\n#&gt; 116 116  387\n#&gt; 117 117  386\n#&gt; 118 118  385\n#&gt; 119 119  384\n#&gt; 120 120  383\n#&gt; 121 121  382\n#&gt; 122 122  381\n#&gt; 123 123  380\n#&gt; 124 124  379\n#&gt; 125 125  378\n#&gt; 126 126  377\n#&gt; 127 127  376\n#&gt; 128 128  375\n#&gt; 129 129  374\n#&gt; 130 130  373\n#&gt; 131 131  372\n#&gt; 132 132  371\n#&gt; 133 133  370\n#&gt; 134 134  369\n#&gt; 135 135  368\n#&gt; 136 136  367\n#&gt; 137 137  366\n#&gt; 138 138  365\n#&gt; 139 139  364\n#&gt; 140 140  363\n#&gt; 141 141  362\n#&gt; 142 142  361\n#&gt; 143 143  360\n#&gt; 144 144  359\n#&gt; 145 145  358\n#&gt; 146 146  357\n#&gt; 147 147  356\n#&gt; 148 148  355\n#&gt; 149 149  354\n#&gt; 150 150  353\n#&gt; 151 151  352\n#&gt; 152 152  351\n#&gt; 153 153  350\n#&gt; 154 154  349\n#&gt; 155 155  348\n#&gt; 156 156  347\n#&gt; 157 157  346\n#&gt; 158 158  345\n#&gt; 159 159  344\n#&gt; 160 160  343\n#&gt; 161 161  342\n#&gt; 162 162  341\n#&gt; 163 163  340\n#&gt; 164 164  339\n#&gt; 165 165  338\n#&gt; 166 166  337\n#&gt; 167 167  336\n#&gt; 168 168  335\n#&gt; 169 169  334\n#&gt; 170 170  333\n#&gt; 171 171  332\n#&gt; 172 172  331\n#&gt; 173 173  330\n#&gt; 174 174  329\n#&gt; 175 175  328\n#&gt; 176 176  327\n#&gt; 177 177  326\n#&gt; 178 178  325\n#&gt; 179 179  324\n#&gt; 180 180  323\n#&gt; 181 181  322\n#&gt; 182 182  321\n#&gt; 183 183  320\n#&gt; 184 184  319\n#&gt; 185 185  318\n#&gt; 186 186  317\n#&gt; 187 187  316\n#&gt; 188 188  315\n#&gt; 189 189  314\n#&gt; 190 190  313\n#&gt; 191 191  312\n#&gt; 192 192  311\n#&gt; 193 193  310\n#&gt; 194 194  309\n#&gt; 195 195  308\n#&gt; 196 196  307\n#&gt; 197 197  306\n#&gt; 198 198  305\n#&gt; 199 199  304\n#&gt; 200 200  303\n#&gt; 201 201  302\n#&gt; 202 202  301\n#&gt; 203 203  300\n#&gt; 204 204  299\n#&gt; 205 205  298\n#&gt; 206 206  297\n#&gt; 207 207  296\n#&gt; 208 208  295\n#&gt; 209 209  294\n#&gt; 210 210  293\n#&gt; 211 211  292\n#&gt; 212 212  291\n#&gt; 213 213  290\n#&gt; 214 214  289\n#&gt; 215 215  288\n#&gt; 216 216  287\n#&gt; 217 217  286\n#&gt; 218 218  285\n#&gt; 219 219  284\n#&gt; 220 220  283\n#&gt; 221 221  282\n#&gt; 222 222  281\n#&gt; 223 223  280\n#&gt; 224 224  279\n#&gt; 225 225  278\n#&gt; 226 226  277\n#&gt; 227 227  276\n#&gt; 228 228  275\n#&gt; 229 229  274\n#&gt; 230 230  273\n#&gt; 231 231  272\n#&gt; 232 232  271\n#&gt; 233 233  270\n#&gt; 234 234  269\n#&gt; 235 235  268\n#&gt; 236 236  267\n#&gt; 237 237  266\n#&gt; 238 238  265\n#&gt; 239 239  264\n#&gt; 240 240  263\n#&gt; 241 241  262\n#&gt; 242 242  261\n#&gt; 243 243  260\n#&gt; 244 244  259\n#&gt; 245 245  258\n#&gt; 246 246  257\n#&gt; 247 247  256\n#&gt; 248 248  255\n#&gt; 249 249  254\n#&gt; 250 250  253\n#&gt; 251 251  252\n#&gt; 252 252  251\n#&gt; 253 253  250\n#&gt; 254 254  249\n#&gt; 255 255  248\n#&gt; 256 256  247\n#&gt; 257 257  246\n#&gt; 258 258  245\n#&gt; 259 259  244\n#&gt; 260 260  243\n#&gt; 261 261  242\n#&gt; 262 262  241\n#&gt; 263 263  240\n#&gt; 264 264  239\n#&gt; 265 265  238\n#&gt; 266 266  237\n#&gt; 267 267  236\n#&gt; 268 268  235\n#&gt; 269 269  234\n#&gt; 270 270  233\n#&gt; 271 271  232\n#&gt; 272 272  231\n#&gt; 273 273  230\n#&gt; 274 274  229\n#&gt; 275 275  228\n#&gt; 276 276  227\n#&gt; 277 277  226\n#&gt; 278 278  225\n#&gt; 279 279  224\n#&gt; 280 280  223\n#&gt; 281 281  222\n#&gt; 282 282  221\n#&gt; 283 283  220\n#&gt; 284 284  219\n#&gt; 285 285  218\n#&gt; 286 286  217\n#&gt; 287 287  216\n#&gt; 288 288  215\n#&gt; 289 289  214\n#&gt; 290 290  213\n#&gt; 291 291  212\n#&gt; 292 292  211\n#&gt; 293 293  210\n#&gt; 294 294  209\n#&gt; 295 295  208\n#&gt; 296 296  207\n#&gt; 297 297  206\n#&gt; 298 298  205\n#&gt; 299 299  204\n#&gt; 300 300  203\n#&gt; 301 301  202\n#&gt; 302 302  201\n#&gt; 303 303  200\n#&gt; 304 304  199\n#&gt; 305 305  198\n#&gt; 306 306  197\n#&gt; 307 307  196\n#&gt; 308 308  195\n#&gt; 309 309  194\n#&gt; 310 310  193\n#&gt; 311 311  192\n#&gt; 312 312  191\n#&gt; 313 313  190\n#&gt; 314 314  189\n#&gt; 315 315  188\n#&gt; 316 316  187\n#&gt; 317 317  186\n#&gt; 318 318  185\n#&gt; 319 319  184\n#&gt; 320 320  183\n#&gt; 321 321  182\n#&gt; 322 322  181\n#&gt; 323 323  180\n#&gt; 324 324  179\n#&gt; 325 325  178\n#&gt; 326 326  177\n#&gt; 327 327  176\n#&gt; 328 328  175\n#&gt; 329 329  174\n#&gt; 330 330  173\n#&gt; 331 331  172\n#&gt; 332 332  171\n#&gt; 333 333  170\n#&gt; 334 334  169\n#&gt; 335 335  168\n#&gt; 336 336  167\n#&gt; 337 337  166\n#&gt; 338 338  165\n#&gt; 339 339  164\n#&gt; 340 340  163\n#&gt; 341 341  162\n#&gt; 342 342  161\n#&gt; 343 343  160\n#&gt; 344 344  159\n#&gt; 345 345  158\n#&gt; 346 346  157\n#&gt; 347 347  156\n#&gt; 348 348  155\n#&gt; 349 349  154\n#&gt; 350 350  153\n#&gt; 351 351  152\n#&gt; 352 352  151\n#&gt; 353 353  150\n#&gt; 354 354  149\n#&gt; 355 355  148\n#&gt; 356 356  147\n#&gt; 357 357  146\n#&gt; 358 358  145\n#&gt; 359 359  144\n#&gt; 360 360  143\n#&gt; 361 361  142\n#&gt; 362 362  141\n#&gt; 363 363  140\n#&gt; 364 364  139\n#&gt; 365 365  138\n#&gt; 366 366  137\n#&gt; 367 367  136\n#&gt; 368 368  135\n#&gt; 369 369  134\n#&gt; 370 370  133\n#&gt; 371 371  132\n#&gt; 372 372  131\n#&gt; 373 373  130\n#&gt; 374 374  129\n#&gt; 375 375  128\n#&gt; 376 376  127\n#&gt; 377 377  126\n#&gt; 378 378  125\n#&gt; 379 379  124\n#&gt; 380 380  123\n#&gt; 381 381  122\n#&gt; 382 382  121\n#&gt; 383 383  120\n#&gt; 384 384  119\n#&gt; 385 385  118\n#&gt; 386 386  117\n#&gt; 387 387  116\n#&gt; 388 388  115\n#&gt; 389 389  114\n#&gt; 390 390  113\n#&gt; 391 391  112\n#&gt; 392 392  111\n#&gt; 393 393  110\n#&gt; 394 394  109\n#&gt; 395 395  108\n#&gt; 396 396  107\n#&gt; 397 397  106\n#&gt; 398 398  105\n#&gt; 399 399  104\n#&gt; 400 400  103\n#&gt; 401 401  102\n#&gt; 402 402  101\n#&gt; 403 403  100\n#&gt; 404 404   99\n#&gt; 405 405   98\n#&gt; 406 406   97\n#&gt; 407 407   96\n#&gt; 408 408   95\n#&gt; 409 409   94\n#&gt; 410 410   93\n#&gt; 411 411   92\n#&gt; 412 412   91\n#&gt; 413 413   90\n#&gt; 414 414   89\n#&gt; 415 415   88\n#&gt; 416 416   87\n#&gt; 417 417   86\n#&gt; 418 418   85\n#&gt; 419 419   84\n#&gt; 420 420   83\n#&gt; 421 421   82\n#&gt; 422 422   81\n#&gt; 423 423   80\n#&gt; 424 424   79\n#&gt; 425 425   78\n#&gt; 426 426   77\n#&gt; 427 427   76\n#&gt; 428 428   75\n#&gt; 429 429   74\n#&gt; 430 430   73\n#&gt; 431 431   72\n#&gt; 432 432   71\n#&gt; 433 433   70\n#&gt; 434 434   69\n#&gt; 435 435   68\n#&gt; 436 436   67\n#&gt; 437 437   66\n#&gt; 438 438   65\n#&gt; 439 439   64\n#&gt; 440 440   63\n#&gt; 441 441   62\n#&gt; 442 442   61\n#&gt; 443 443   60\n#&gt; 444 444   59\n#&gt; 445 445   58\n#&gt; 446 446   57\n#&gt; 447 447   56\n#&gt; 448 448   55\n#&gt; 449 449   54\n#&gt; 450 450   53\n#&gt; 451 451   52\n#&gt; 452 452   51\n#&gt; 453 453   50\n#&gt; 454 454   49\n#&gt; 455 455   48\n#&gt; 456 456   47\n#&gt; 457 457   46\n#&gt; 458 458   45\n#&gt; 459 459   44\n#&gt; 460 460   43\n#&gt; 461 461   42\n#&gt; 462 462   41\n#&gt; 463 463   40\n#&gt; 464 464   39\n#&gt; 465 465   38\n#&gt; 466 466   37\n#&gt; 467 467   36\n#&gt; 468 468   35\n#&gt; 469 469   34\n#&gt; 470 470   33\n#&gt; 471 471   32\n#&gt; 472 472   31\n#&gt; 473 473   30\n#&gt; 474 474   29\n#&gt; 475 475   28\n#&gt; 476 476   27\n#&gt; 477 477   26\n#&gt; 478 478   25\n#&gt; 479 479   24\n#&gt; 480 480   23\n#&gt; 481 481   22\n#&gt; 482 482   21\n#&gt; 483 483   20\n#&gt; 484 484   19\n#&gt; 485 485   18\n#&gt; 486 486   17\n#&gt; 487 487   16\n#&gt; 488 488   15\n#&gt; 489 489   14\n#&gt; 490 490   13\n#&gt; 491 491   12\n#&gt; 492 492   11\n#&gt; 493 493   10\n#&gt; 494 494    9\n#&gt; 495 495    8\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n#[ reached getOption(\"max.print\") -- omitted 2 entries ]\n\nYou can change the default using\n\n options(max.print=1500)\n\nbut one rarely does this\nMost of the times you want a sneak preview in your data from the top\n\n# head() returns the first rows (5 default) rows:\nhead(mydf)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\nhead(mydf,8)\n#&gt;   z zrev\n#&gt; 1 1  502\n#&gt; 2 2  501\n#&gt; 3 3  500\n#&gt; 4 4  499\n#&gt; 5 5  498\n#&gt; 6 6  497\n#&gt; 7 7  496\n#&gt; 8 8  495\n\nor the bottom:\n\n# tail() the last ones :\ntail(mydf)\n#&gt;       z zrev\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\ntail(mydf, 7)\n#&gt;       z zrev\n#&gt; 496 496    7\n#&gt; 497 497    6\n#&gt; 498 498    5\n#&gt; 499 499    4\n#&gt; 500 500    3\n#&gt; 501 501    2\n#&gt; 502 502    1\n\nor some random records if you load the car package (Package related to book “Companion to Applied Regression” by Fox, J and Weisber, S (2024) )\n\ncar::some(mydf)\n#&gt;       z zrev\n#&gt; 84   84  419\n#&gt; 93   93  410\n#&gt; 165 165  338\n#&gt; 220 220  283\n#&gt; 252 252  251\n#&gt; 269 269  234\n#&gt; 279 279  224\n#&gt; 324 324  179\n#&gt; 347 347  156\n#&gt; 386 386  117\n\nor a brief:\n\ncar::brief(mydf)\n#&gt; 502 x 2 data.frame (497 rows omitted)\n#&gt;       z zrev\n#&gt;     [i]  [i]\n#&gt; 1     1  502\n#&gt; 2     2  501\n#&gt; 3     3  500\n#&gt; . . .            \n#&gt; 501 501    2\n#&gt; 502 502    1\n\n\n\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied Regression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with data frames and functions</span>"
    ]
  },
  {
    "objectID": "025_read_write.html",
    "href": "025_read_write.html",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "9.1 Reading delimited files\nText files and delimited files can easily be imported in R using the function read.table(). You can play with the values of the different arguments to adapt to the format of your file. You have the possibility to load a file\n?read.table\n\nheartSA &lt;- read.table(\"data/SAheart/SAheart.txt\", header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(file.choose(), header=T, sep=',', row.names=1)\nheartSA &lt;- read.table(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", \n                      sep=\",\", head=T, row.names=1)\nThere exist also some preset functions to read tables with some specific formats. Here again you can play we the values of the different arguments to load your database in an accurate way.\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n          dec = \",\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n\nread.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n            dec = \",\", fill = TRUE, comment.char = \"\", ...)",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#reading-delimited-files",
    "href": "025_read_write.html#reading-delimited-files",
    "title": "9  Reading and writing data to and from R",
    "section": "",
    "text": "stored locally: writing the path to reach it\nchosen interactively and stored locally\nfrom the web using its url.",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#files-form-specific-software",
    "href": "025_read_write.html#files-form-specific-software",
    "title": "9  Reading and writing data to and from R",
    "section": "9.2 Files form specific software",
    "text": "9.2 Files form specific software\nSome databases are exported from other software and have atypical formats.\n\nYou first need to install (once per machine) and load the R package foreign.\n\n\n# install.packages('foreign')\nlibrary(foreign)\n\n\nThen you can load the following different formats\n\n\n# Read the SPSS data\nread.spss(\"example.sav\")\n\n# Read Stata data into R\nread.dta(\"c:/mydata.dta\") \n\nlibrary(xlsx)\n\n# first row contains variable names\nread.xlsx(\"c:/myexcel.xlsx\", 1)\n\n# read in the worksheet named mysheet\nread.xlsx(\"c:/myexcel.xlsx\", sheetName = \"mysheet\") \n\n# The package `readxl` can also be helpful.\n\nlibrary(sas7bdat)\n\n# Read in the SAS data\nmySASData &lt;- read.sas7bdat(\"example.sas7bdat\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "025_read_write.html#writing-a-dataframe-to-a-file",
    "href": "025_read_write.html#writing-a-dataframe-to-a-file",
    "title": "9  Reading and writing data to and from R",
    "section": "9.3 Writing a dataframe to a file",
    "text": "9.3 Writing a dataframe to a file\nYou can also export a dataset\n\nwrite.table(heartSA, file = \"data/SAheart/NewSAheart.txt\")\nwrite.csv(heartSA, file = \"data/SAheart/NewSAheart.csv\")\nwrite.csv2(heartSA, file = \"data/SAheart/NewSAheart.csv\")\n\nHowever, in case it is for further use within R, we recommend you use the RDS format, which works with any type of R object and is often very effective in terms of file size.\n\nsaveRDS(heartSA,\"data/SAheart/NewSAheart.rds\")\nNewSAheart&lt;-readRDS(\"data/SAheart/NewSAheart.rds\")",
    "crumbs": [
      "Part II - Getting Started with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reading and writing data to and from R</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html",
    "href": "031_sample_population.html",
    "title": "10  Sample and Population",
    "section": "",
    "text": "10.1 Definitions\nSurvey\nElement, record, individual\nPopulation\nTarget population\nSampling units\nFrame\nSample",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#definitions",
    "href": "031_sample_population.html#definitions",
    "title": "10  Sample and Population",
    "section": "",
    "text": "Any activity that collects information in an organised and methodical manner about characteristics of interest from some or all units of a population using well-defined concepts, methods and procedures and compiles such information into a useful summary form.\n\n\n\nAn object on which a measurement is taken.\n\n\n\nA collection of elements.\n\n\n\nThe population for which information is required.\n\n\n\nNon-overlapping collection of elements from the population that covers the entire population.\n\n\n\nThe device which delimits, identifies, and allows access to the elements of the target population. The frame is also called the survey frame or the sampling frame.\n\n\n\nA collection of sampling units drawn from a frame.",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "031_sample_population.html#usual-denotations",
    "href": "031_sample_population.html#usual-denotations",
    "title": "10  Sample and Population",
    "section": "10.2 Usual denotations",
    "text": "10.2 Usual denotations\nPopulation observations are usually denoted with capital letters: \\[X = \\{X_1, X_2,..., X_N\\}\\]\nTheir parameters are denoted with Greek letters, capital letters or a mix: \\[\\mu, \\mu_X, \\sigma, \\sigma_X, \\sigma_{X,Y}, E(X), Var(X), Cov(X,Y), \\rho_{X,Y}\\]\nSample observations are expressed with small letters: \\[x = (x_1, x_2,..., x_n)\\]\nTheir estimators associated are also expressed using small letters, i.e. \\[\\bar{x}, s, s_x, s_{x,y}, var(x), cov(x,y), r_{x,y}\\] …",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sample and Population</span>"
    ]
  },
  {
    "objectID": "032_univariate.html",
    "href": "032_univariate.html",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1 Measures of Center",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-center",
    "href": "032_univariate.html#measures-of-center",
    "title": "11  Univariate statistics",
    "section": "",
    "text": "11.1.1 Mean\nThe arithmetic mean of a vector x having n observations: x = (x1, x2,… , xi, …, xn) is given by the following formulae for the empirical and theoretical means:\n\\[\\bar{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\] \\[\\mu_X = E(X) = \\dfrac{1}{N} \\sum_{i=1}^N X_i\\]\nwith \\(n\\) the size of the sample and \\(N\\) the size of the population.\nLet’s take a variable with the ages of some individuals:\n\nAge &lt;- c (25, 27, 28, 23, 52, 27, 27, 26, 25, 30)\n\nThe empirical mean of this variable is given by:\n\\((1 / 10) * (25 + 27 + 28 + 23 + 52 + 27 + 27 + 26 + 25 + 30) = 290 / 10 = 29\\)\nYou can compute the mean of a vector as follows in R:\n\nsum(Age) / length(Age)\n#&gt; [1] 29\nmean(Age)\n#&gt; [1] 29\n\nBut in some cases, the mean is not a good indicator of the central value of a distribution. For the above variable, we can see that one individual is 52 years old. The mean is sensitive to extreme values or outliers.\n\n\n11.1.2 Median\nThe median corresponds to the value such that 50% of the individuals have a smaller or equal value and 50% of the individuals have a larger or equal value. It is also called the 50th percentile. We consider the variable Age and a second variable Age2 where the value 52 has been removed. They have respectively 10 and 9 elements.\n\nAge2 &lt;- c (25, 27, 28, 23, 27, 27, 26, 25, 30)\n\nFirst, sort the values of the vector considered from the smallest to the largest. For Age: \\({23, 25, 25, 26, 27, 27, 27, 28, 30, 52}\\)\nand for Age2:\n\\({23, 25, 25, 26, 27, 27, 27, 28, 30}\\)\nWe know already how to do this in R:\n\nsort(Age)\n#&gt;  [1] 23 25 25 26 27 27 27 28 30 52\nsort(Age2)\n#&gt; [1] 23 25 25 26 27 27 27 28 30\n\nSecond,\n\nFor an odd set of numbers (Age2), find the number in the middle of the vector, this is the empirical median. The number to take is also given by: \\((n + 1) / 2 = 10 / 2\\) , i.e. the 5th value, 27\n\n\nsort(Age2)[5]\n#&gt; [1] 27\n\n\nFor an even set of numbers (Age), find the two numbers in the middle and compute their average value, this is the empirical median, i.e. \\((27 + 27) / 2 = 27\\)\n\n\nsort(Age)[c(5,6)]\n#&gt; [1] 27 27\nsum(sort(Age)[c(5,6)])/2\n#&gt; [1] 27\n\nWe can see that the median is much less sensitive to extreme values. In both cases, the median is 27. Using R built-in functions, the median is :\n\nmedian(Age)\n#&gt; [1] 27\nmedian(Age2)\n#&gt; [1] 27\n\n\n\n11.1.3 Mode\nThe mode (not to be mistaken with R mode for vectors) corresponds to the value(s) which appears the most often. A vector can have 0, 1 or many modes. For our variable Age, the mode is 27 which appears 3 times.\n\ntable(Age)\n#&gt; Age\n#&gt; 23 25 26 27 28 30 52 \n#&gt;  1  2  1  3  1  1  1\nsort(table(Age), descending=TRUE)\n#&gt; Age\n#&gt; 23 26 28 30 52 25 27 \n#&gt;  1  1  1  1  1  2  3\n\nThe mode is an immediate output in R. But we can write what we have just done and extract the value after sorting, i.e.\n\nsort(table(Age),decreasing = TRUE)[1]\n#&gt; 27 \n#&gt;  3\n\nWhile this requires sorting (which can be long), an alternative would be to use the which.max() function:\n\nwhich.max(table(Age)) #returning the position of the max, i.e. 4th position here\n#&gt; 27 \n#&gt;  4\nAge[which.max(table(Age))] #then using that position into the original vector\n#&gt; [1] 23\n\nWe can also look at the mode for qualitative / categorical variables. If we take the example of the variable score, the mode is the value “C”.\n\nscore &lt;- as.factor ( c (\"C\",\"C\",\"A\",\"B\",\"A\",\"C\",\"B\",\"B\",\"A\",\"C\"))\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#measures-of-dispersion",
    "href": "032_univariate.html#measures-of-dispersion",
    "title": "11  Univariate statistics",
    "section": "11.2 Measures of Dispersion",
    "text": "11.2 Measures of Dispersion\n\n11.2.1 Range\nRange is the simplest measure of the spread of a distribution and corresponds to the difference between the maximum and the minimum values: \\[Max(x) - Min(x)\\]\nFor the variable Age the minimum being 23 and the maximum 52, the range is: \\(52 - 23 = 29\\).\nIn R the function range returns the two extrema, not the difference, see\n\nmax(Age) - min(Age)\n#&gt; [1] 29\nrange(Age)\n#&gt; [1] 23 52\n\n\n\n11.2.2 Quantiles.\nExtending the concept of a median, quantiles (percentiles, deciles, quartiles,…) divide the distribution into equal slices. The i-th percentile corresponds to the value at which i% of the distribution is below that value. The median is when \\(i=50%\\), i.e. the ditribution is split into 2 half parts so that the probability of drawing a number below the median is 50%.\nPercentiles divide the distribution into 100 slices (probability = \\({0.01, 0.02, 0.03, ..., 1}\\)) ; deciles into 10 (probability = \\({0.1, 0.2, 0.3, ..., 1}\\)) ; quartiles into 4 (probability = \\({0.25, 0.5, 0.75, 1}\\)). You obtain all of these using the same function quantile() and the corresponding probability of picking up a number below:\n\n# quartiles are the default\nquantile(Age)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 23.00 25.25 27.00 27.75 52.00\n#deciles\nquantile(Age, probs = seq(0, 1, 0.1))\n#&gt;   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#&gt; 23.0 24.8 25.0 25.7 26.6 27.0 27.0 27.3 28.4 32.2 52.0\n\n#Suppose a larger set of 100000 values \"normally\" distributed around the mean 0\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n#the 1st , 5th, 95th and 99th % and some others\nquantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99))\n#&gt;         1%         5%        16%        84%        95%        99% \n#&gt; -2.3333179 -1.6488276 -0.9924832  1.0009836  1.6512973  2.3372571\n\n\nhist(x, breaks = 100)\nabline(v=quantile(x, probs = c(0.01, 0.05, 0.16, 0.84, 0.95, 0.99)), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Inter-quartile range (IQR)\nIt is simply the difference between the 3rd and the 1st quartiles: \\[IQR = Q3(x) - Q1(x)\\].\nAgain for the variable Age, it equals: \\(27.75 - 25.25 = 2.5\\)\n\nquantile(Age, probs = 0.75) - quantile(Age, probs = 0.25)\n#&gt; 75% \n#&gt; 2.5\n\n#or\nIQR(Age)\n#&gt; [1] 2.5\n\n\n\n11.2.4 Variance and Standard Deviation\nAlthough quantiles and plots are very much in use to describe the spread of a distribution, statistical analysis relies most heavily on the notion of variance.\nFirst, think about the simplest way you can measure how a given observation is far from, (i.e. spread out of) a general expected value. A pretty effective way is to measure the difference between that observation and the mean of observations.\nThe Deviation to the mean for an individual i is \\[v_i=x_i-\\bar{x}\\]\nIt is then very tempting to say that the general dispersion of a variable is simply the sum of all those values. In order for the number not to grow with the number of observations, we then compute an average deviation by dividing by \\(n\\)\n\\[\\Sigma_i(x_i-\\bar{x})/n\\]\nBut is this a good idea?\nTake the Age example:\n\nv&lt;- Age-mean(Age) #set of deviations to the mean\nsum(v)/length(v)\n#&gt; [1] 0\n\nIt seems there is no “spreading” ? In fact, all the negative deviations compensate (here exactly) the positive deviations.\nWe can rather remove the signs and use the absolute value of each deviation, sum them up and divide by \\(n\\).\nThis is called MAD, the Mean Absolute Deviation and is quite easy to interpret indeed.\n\nabs_v&lt;- abs(Age-mean(Age)) #set of deviations to the mean\nmean(abs_v)\n#&gt; [1] 4.8\n\nYet, one could argue that large deviations to the mean are more important than the smaller ones to describe the pattern of deviations, especially since in a normal population there are more values closer to the mean than farther.\nRather than using absolute deviations, (most of) statisticians have therefore opted for squaring the deviations, which is still symmetrical and has the same characteristic of turning every negative value into a positive one.\nWe therefore usually consider the sum of squared deviations to the mean:\n\\[\\Sigma_i^n(x_i-\\bar{x})^2\\] which, we then divide by the number of observations to avoid the value to grow with the number of observations, thus allowing comparisons. It is then called the variance. More precisely, if we use a sample, we still need to use one of our observation in order to estimate the mean, hence we are left with \\(n-1\\) degrees of freedom.\nThe empirical variance is then given by:\n\\[var_x = s^2_x = \\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2\\]\nand the theoretical variance:\n\\[var_X = \\sigma^2_X = \\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2\\] \\[= E(X-E(X))^2 = E(X-\\mu)^2 \\]\nwhere \\(E(X)\\) is the expected mean of the population (or “Esperance”).\nThe variance is thus a single number that gives insight on how the variable is spread around the mean value. A small value (close to 0) indicates a small variability: values are not very different from the mean value. A high value indicated a strong variability.\nThe variance cannot be negative.\nIn R, we use the var() functio, which we here first reconstruct:\n\n(Age-mean(Age))^2 #squared deviations to the mean\n#&gt;  [1]  16   4   1  36 529   4   4   9  16   1\nsum((Age-mean(Age))^2) #sum of squared deviations to the mean\n#&gt; [1] 620\nsum((Age-mean(Age))^2)/(length(Age)-1) #...divided by n-1\n#&gt; [1] 68.88889\n\nvar(Age)\n#&gt; [1] 68.88889\nvar(Age2)\n#&gt; [1] 4.027778\n\nWe see that the default in R for var() is to divide by \\(n-1\\), i.e. the sample variance.\nFinally, we like the “dispersion” to be expressed in the same units as the original variable, i.e. years in this case. It is already the case for the Mean Absolute Deviation. We need to take the square root of the variance to obtain a “standard deviation”:\nThe standard deviations corresponding to the sample and population variance are then given by:\n\\[s_x = \\sqrt{\\dfrac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar{x})^2}\\]\n\\[\\sigma_X = \\sqrt{\\dfrac{1}{N} \\sum_{i=1}^N (X_i-E(X))^2}\\]\nAnd can be computed using:\n\nsqrt(var(Age))\n#&gt; [1] 8.299933\nsqrt(var(Age2))\n#&gt; [1] 2.006932\n#or simply\nsd(Age) #again remember it is the sample sd\n#&gt; [1] 8.299933\nsd(Age2)\n#&gt; [1] 2.006932",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#visualize-a-distribution",
    "href": "032_univariate.html#visualize-a-distribution",
    "title": "11  Univariate statistics",
    "section": "11.3 Visualize a distribution",
    "text": "11.3 Visualize a distribution\nVisualizing a distribution with graphics is important for both supporting the analysis and dissemination.\nWe have seen some graphics already above and we are going to produce improved graphics with ggplot later on. Without spending much time on design, the purpose here is show how graphics accompany the univariate statistics we introduced.\n\n11.3.1 Boxplots\nA boxplot visually provides a number of information about the distribution of a variable\n\nthe median value (thick black line),\nthe inter-quartile range (IQR) (the black box),\nthe minimum and maximum values or 1.5 times the IQR (horizontal lines),\nthe outlier(s) (dots out of the whiskers).\n\nValues are considered outliers when they fall outside the whiskers, that is outside a distance of 1.5 times the IQR. In absence of such outliers the horizontal lines show the extrema (min and max).\nWe have added the mean as a red point to clarify here the difference between the mean and median.\nExamine the difference again between Age and Age2\n\npar (mfrow = c(1, 2)) # to display multiple plots at once\nboxplot(Age, ylab = \"Age\", main = \"Boxplot of Age\")\npoints(mean(Age), col = 2, pch = 18)\n\nboxplot(Age2, ylab = \"Age2\", main = \"Boxplot of Age2\")\npoints(mean(Age2), col = 2, pch = 18)\n\n\n\n\n\n\n\n\n\n\n11.3.2 Stem and leaf\nProbably less in use nowadayd for visual purpose and reporting, a stem and leaf graph is a very effective way to look into the distribution of a variable while you are exploring, analyzing your data in the console.\nIt is not actually a plot but a presentation of the values into a “stem”, which is made of the values that are present across all cases and then a “leaf” where the remaining parts of each numbers is shown and concatenated, thus showing a kind of frequency together with the values:\nExamine the case for Age and Age2:\n\nstem(Age)\n#&gt; \n#&gt;   The decimal point is 1 digit(s) to the right of the |\n#&gt; \n#&gt;   2 | 35567778\n#&gt;   3 | 0\n#&gt;   4 | \n#&gt;   5 | 2\nstem(Age2)\n#&gt; \n#&gt;   The decimal point is at the |\n#&gt; \n#&gt;   22 | 0\n#&gt;   24 | 00\n#&gt;   26 | 0000\n#&gt;   28 | 0\n#&gt;   30 | 0\n\n\n\n11.3.3 Histogram\nHistograms are probably the first go to graphic in order to visualize a distribution\nLet’s reuse the x normal variable we created earlier and plot both its boxplot and the histogram\n\nset.seed(233)\nx&lt;-rnorm(n = 100000, mean=0, sd=1)\n\nsummary(x)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; -4.127442 -0.668690  0.004522  0.003651  0.682819  4.323841\nboxplot(x, main = \"Boxplot of a random variable following N(0,1) with n = 100,000\")\n\n\n\n\n\n\n\nhist(x)\n\n\n\n\n\n\n\n\nA histogram is more detailed than a boxplot because it shows every data but does not provide a central or dispersion measure. Key to using a histogram is to play with the number of bars, otherwise some information, gaps, or multimodalities may be not be seen. You adapt the number of bars using the option “breaks”\n\npar(mfrow=c(1,2))\nhist(x, breaks=5)\nhist(x, breaks=100)\n\n\n\n\n\n\n\n\nFor a categorical variable (factor), the function plot() gives the counts of each category (level). We have worked an example using Le Tour de France data earlier in the course. It is similar to a visualisation of the table() output and is equivalent to the function hist() for quantitative variables.\n\ntable(score)\n#&gt; score\n#&gt; A B C \n#&gt; 3 3 4\nplot(score, main = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nFor a numeric variable, a call to plot, shows values along the vertical axis and the index of the rows along the horizontal axis, which is rarely a useful information.\n\nplot(x)",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "032_univariate.html#the-shape-of-distributions-skewness-and-kurtosis",
    "href": "032_univariate.html#the-shape-of-distributions-skewness-and-kurtosis",
    "title": "11  Univariate statistics",
    "section": "11.4 The shape of distributions: Skewness and Kurtosis",
    "text": "11.4 The shape of distributions: Skewness and Kurtosis\n\n11.4.1 First and second moments:\nWe have seen earlier that the very first way to characterise a distribution is to use its mean and that the second way is to use the variance (or the square root of the variance, i.e. the standard deviation).\nThe mean and the variance are also named, respectively, the first and second moment of a distribution\nIndeed, for discrete data, the mean (or expectation) is calculated as:\n\\[\\mu = E[X] = \\sum_{i} x_i p(x_i)\\] where \\(x_i\\) are the values and \\(p(x_i)\\) are their probabilities.\nNote 1: for continuous variables, we should in fact use an integral rather that the sum symbol. Note 2, the “zeroth” moment of the distribution is in fact the sum of probabilities (x_i), i.e. the total mass (the concept is borrowed from physics), i.e. 1.\nThe variance is the second order moment, measuring the spread of the distribution around the mean. We have seen it is defined in difference to the mean with a square exponent:\n\\[\\sigma^2 = E[(X - \\mu)^2] = \\sum_{i} (x_i - \\mu)^2 p(x_i)\\] We can actually go on with higher exponents and use higher level moments to describe the shape of a distribution.\n\n\n11.4.2 Third moment: Skewness\nThe skewness is the third order moment of a distribution. The skewness measures the asymmetry of the distribution around the mean. It is is calculated as:\n\\[\\gamma_1 = E\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right] = \\sum_{i} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^3 p(x_i)\\]\nIn addition to the cubic exponent applied to the deviation to the mean, you notice we also divide by the standard deviation \\(\\sigma\\) to make the measure dimensionless, i.e. comparable across cases with different units. The skewness is not influenced by the scale of the data.\nIn practice, the skewness is computed as follows:\n\\[\\dfrac {\\sum_{i=1}^{n} (x_i - \\bar{x})^3} {n s^3}\\]\n\nset.seed(101)\nx&lt;-rnorm(1000, mean=50,sd=5)\nmean(x)\n#&gt; [1] 49.82569\nmedian(x)\n#&gt; [1] 49.72804\ne1071::skewness(x)\n#&gt; [1] -0.004246246\ne1071::skewness((x-100)/1000) #with this you see it is not influenced by any rescaling\n#&gt; [1] -0.004246246\nsum((x-mean(x))^3)/(sd(x)^3*length(x)) #manual computation\n#&gt; [1] -0.004246246\n\nWhen the skewness is \\(&gt;0\\), the tail of the distribution is heavier on the right side. This means there are more extreme values on the higher end. The mean is greater than the median.\nWhen the skewness is \\(&lt;0\\), the tail of the distribution is heavier on the left side. This means there are more extreme values on the lower end. The mean is lower than the median.\nA value around zero indicates a symmetric distribution.\n\nset.seed(101)\nright_skewed &lt;- x + rexp(1000, rate = 0.1) #we add an exp to the previous x for generating a right skewed distribution\nleft_skewed &lt;- rnorm(1000, mean = 50, sd = 5) - rexp(100, rate = 0.1) #left skewed\n\nhist(x, main = \"Symmetric\", col = \"blue\", breaks = 20)\n\n\n\n\n\n\n\nhist(right_skewed, main = \"Right-skewed\",col = \"green\", breaks = 20)\n\n\n\n\n\n\n\nhist(left_skewed, main = \"Left-skewed\", col = \"red\", breaks = 20)\n\n\n\n\n\n\n\n\ne1071::skewness(x)\n#&gt; [1] -0.004246246\ne1071::skewness(right_skewed)\n#&gt; [1] 1.350504\ne1071::skewness(left_skewed)\n#&gt; [1] -0.5836373\n\n\n\n11.4.3 Fourth moment: Kurtosis\nThe Kurtosis is the fourth order moment of a distribution. The Kurtosis measures the peakness of the distribution and is calculated as:\n\\[\\gamma_2 = E\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^4\\right] - 3 = \\sum_{i} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^4 p(x_i) - 3\\]\nYou notice the exponent to the level 4 applied to the deviations to the mean and that similarly to the skewness, we also divide by the standard deviation \\(\\sigma\\) to make the measure dimensionless, i.e. comparable across cases with different units. The Kurtosis is not influenced by the scale of the data.\nIn practice in R, the Kurtosis is similar to the skewness described above:\n\\[\\dfrac {\\sum_{i=1}^{n} (x_i - \\bar{x})^4} {n s^4}\\] However it is adjusted with some correction for small sample sizes and for comparison to a normal distribution for which the result would be 3. See the help for how it is computed by default\n\nset.seed(101)\nx&lt;-rnorm(1000, mean=50,sd=5)\ne1071::kurtosis(x)\n#&gt; [1] -0.119472\ne1071::kurtosis((x-100)/1000) #with this you see it is not dependent on scale \n#&gt; [1] -0.119472\n\nUsing this implementation, a Kurtosis close to 0 then indicates a distribution similar in shape to a normal (bell-shaped) distribution. A positive kurtosis indicates a more peaked distribution, also named leptokurtic.\nA negative kurtosis indicates a less peaked shape, named platykurtic.\nas an example, we can add some extreme values to the normal values we created earlier in order to higher peak look or trim the extremes of a normal distribution to have a flatter one:\n\nset.seed(1010)\nx&lt;-rnorm(1000, mean=50,sd=5)\nx_peaker&lt;-c(x, rnorm(300, mean = 50, sd = 20))\nx_flatter&lt;-x[abs(x) &lt; 60]\n\nhist(x, main = \"Normal\", col = \"blue\", breaks = 20)\n\n\n\n\n\n\n\nhist(x_peaker, main = \"Higher peak\",col = \"green\", breaks = 20)\n\n\n\n\n\n\n\nhist(x_flatter, main = \"Flatter\", col = \"red\", breaks = 20)\n\n\n\n\n\n\n\n\ne1071::kurtosis(x)\n#&gt; [1] 0.07423458\ne1071::kurtosis(x_peaker)\n#&gt; [1] 5.789643\ne1071::kurtosis(x_flatter)\n#&gt; [1] -0.1829135",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "033_discretisation.html",
    "href": "033_discretisation.html",
    "title": "12  Discretisation",
    "section": "",
    "text": "Geographers, maybe more than others because they like to produce maps, are often tempted to cut a numerical vector into a set classes.\nA typical use is the cartography of a continuous variable in a set of 5, 6 or 7 groups, knowing the human eye has difficulties to disentangle more colours. GIS and mapping software all have a menu where “discretisation” is made using a number of manually defined limits of preset algorithms, such as “Natural breaks”, “quantiles”, etc.\nIn R, the function cut()` is the base function to divide a range of numeric values x into intervals by a number of break values. It outputs a new factor where each value x is given a level according to which interval they fall in. The breaks are either\n\na scalar greater or equal to 2 for the range to be cut into equal length pieces.\na set of breaks to be used as the upper and lower limits of each discrete category\n\n\nset.seed(101)\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf4&lt;-cut(x, breaks=4)\nhead(xf4)\n#&gt; [1] (94.1,98.4] (98.4,103]  (94.1,98.4] (98.4,103]  (98.4,103]  (103,107]  \n#&gt; Levels: (89.7,94.1] (94.1,98.4] (98.4,103] (103,107]\ntable(xf4)\n#&gt; xf4\n#&gt; (89.7,94.1] (94.1,98.4]  (98.4,103]   (103,107] \n#&gt;           2           5           9           4\n\nNotice how the label clearly indicates the (default) closing on the right of each interval\n\nx&lt;-rnorm(20, mean = 100, sd=5)\nxf6&lt;-cut(x, breaks=c(min(x),-90,95,100,105,110, max(x)))\nhead(xf6)\n#&gt; [1] (95,100]  (100,105] (95,100]  (89.6,95] (100,105] (89.6,95]\n#&gt; Levels: (-90,89.6] (89.6,95] (95,100] (100,105] (105,106] (106,110]\ntable(xf6)\n#&gt; xf6\n#&gt; (-90,89.6]  (89.6,95]   (95,100]  (100,105]  (105,106]  (106,110] \n#&gt;          1          3          4         10          2          0\n\nSince we can choose any breaks, it is pretty easy to adapt and use any discretisation method one would find elsewhere, e.g. in mapping packages.\nThere is a wonderful package, classInt, that does so and where you can simply choose the discretisation methodology.\nLet’s explore!\nhttps://cran.r-project.org/web/packages/classInt/classInt.pdf\nWe refer to the help of the package and specifically the function classIntervals()to find out about the available methods\n\nx_quantile_5&lt;-classInt::classIntervals(x, n=5, style=\"quantile\") #Default style\nx_quantile_5\n#&gt; style: quantile\n#&gt;   one of 3,876 possible partitions of this variable into 5 classes\n#&gt; [89.63447,95.69211) [95.69211,100.2653) [100.2653,102.3357) [102.3357,103.8727) \n#&gt;                   4                   4                   4                   4 \n#&gt; [103.8727,105.9493] \n#&gt;                   4\n\nThe classIntervals() output has its own class and specific plotting method that works with a given colour palette\n\nclass(x_quantile_5)\n#&gt; [1] \"classIntervals\"\nmycolors&lt;-c(\"darkgreen\",\"lightgreen\",\"lightyellow\", \"orange\", \"orangered\")\nplot(x_quantile_5, pal=mycolors)\n\n\n\n\n\n\n\n\nGiven the normality of the distribution, and the use of quantiles, the central class logically needs a smaller range to host the same number of values.\nBelow another split based on standard deviations:\n\nmean(x)\n#&gt; [1] 99.97489\nsd(x)\n#&gt; [1] 4.865144\nx_sd_5&lt;-classInt::classIntervals(x, n=5, style=\"sd\")\nx_sd_5\n#&gt; style: sd\n#&gt;   one of 50,388 possible partitions of this variable into 8 classes\n#&gt;  [87.81203,90.2446)  [90.2446,92.67717) [92.67717,95.10974) [95.10974,97.54231) \n#&gt;                   1                   1                   2                   1 \n#&gt; [97.54231,99.97489) [99.97489,102.4075)   [102.4075,104.84)   [104.84,107.2726] \n#&gt;                   3                   5                   5                   2\nplot(x_sd_5, pal=mycolors)\n\n\n\n\n\n\n\n\nAnd with 7 classes using the “Jenks” method, similar to the one we find within Esri ArcGIS:\n\nx_jenks_7&lt;-classInt::classIntervals(x, n=7, style=\"jenks\")\nx_jenks_7\n#&gt; style: jenks\n#&gt;   one of 27,132 possible partitions of this variable into 7 classes\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2\nplot(x_jenks_7, pal=mycolors)\n\n\n\n\n\n\n\n\nNotice that ahead of plotting classInt expanded the number of colours, which we provided. In fact, 2 would be enough:\n\nplot(x_jenks_7, pal=c(\"yellow\",\"red\"))\n\n\n\n\n\n\n\n\nInterestingly, rather that specifying a number of classes, one could also use the same breaks as a standard boxplot:\n\nx_box&lt;-classInt::classIntervals(x,  style=\"box\")\nx_box\n#&gt; style: box\n#&gt;   one of 11,628 possible partitions of this variable into 6 classes\n#&gt; [89.63447,89.84276) [89.84276,98.08961) [98.08961,101.8191) [101.8191,103.5875) \n#&gt;                   1                   4                   5                   5 \n#&gt; [103.5875,111.8343)      [111.8343,Inf] \n#&gt;                   5                   0\nquantile(x,probs=c(0.25,0.5,0.75))\n#&gt;       25%       50%       75% \n#&gt;  98.08961 101.81905 103.58750\nc(quantile(x,probs=0.25)-1.5*IQR(x),\n  quantile(x,probs=0.75)+1.5*IQR(x))\n#&gt;       25%       75% \n#&gt;  89.84276 111.83435\nplot(x_box, pal=mycolors)\n\n\n\n\n\n\n\n\nOne then retrieves a vector of the categories in which each values fall using findCols(), which we can easily add to a dataframe as a new column, or even a vector of colours for use anywhere else using findColours().\nThis is shown with our Jenks example:\n\nclassInt::findCols(x_jenks_7)\n#&gt;  [1] 4 6 4 2 6 2 5 4 5 5 6 5 7 1 7 3 5 6 2 5\nclassInt::findColours(x_jenks_7, pal=c(\"yellow\",\"red\"))\n#&gt;  [1] \"#FF7F00\" \"#FF2A00\" \"#FF7F00\" \"#FFD400\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt;  [8] \"#FF7F00\" \"#FF5500\" \"#FF5500\" \"#FF2A00\" \"#FF5500\" \"#FF0000\" \"#FFFF00\"\n#&gt; [15] \"#FF0000\" \"#FFAA00\" \"#FF5500\" \"#FF2A00\" \"#FFD400\" \"#FF5500\"\n#&gt; attr(,\"palette\")\n#&gt; [1] \"#FFFF00\" \"#FFD400\" \"#FFAA00\" \"#FF7F00\" \"#FF5500\" \"#FF2A00\" \"#FF0000\"\n#&gt; attr(,\"table\")\n#&gt; [89.63447,89.63447] (89.63447,92.94805] (92.94805,96.37813]  (96.37813,99.4034] \n#&gt;                   1                   3                   1                   3 \n#&gt;  (99.4034,102.4907] (102.4907,104.6017] (104.6017,105.9493] \n#&gt;                   6                   4                   2",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discretisation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html",
    "href": "034_cross_tabulation.html",
    "title": "13  Cross-tabulation",
    "section": "",
    "text": "13.1 Case of 2 or several factors (contingency tables)\nSuppose a first data.frame is made with a single factor:\nD1&lt;-data.frame(\nScore1=factor(c(\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\",\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\"))\n)\nThe function table() returns counts, i.e. frequencies:\ntable(D1)\n#&gt; Score1\n#&gt; A B C D \n#&gt; 4 4 8 2\nLet’s add a second factor to this data-frame, we see the function table now returns a cross-tabulation, which we alsso call a contingency table (and to which we will later add significance tests)\nD2&lt;-D1\nD2[,\"Gender\"]&lt;-factor(c(\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\",\"M\",\"F\",\"M\",\"F\",\"M\",\"F\",\"F\",\"M\"))\ntable(D2)\n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 2 2\n#&gt;      B 2 2\n#&gt;      C 5 3\n#&gt;      D 1 1\nWhat happens if there are 3 and more factors?\nD3&lt;-D2\nD3[,\"Score2\"]&lt;-factor(c(\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\",\"A\",\"A\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\",\"D\", \"A\",\"B\"))\nD3[,\"Country\"]=factor(c(\"LU\",\"DE\",\"DE\",\"DE\",\"DE\",\"FR\",\"DE\",\"LU\",\"DE\",\"BE\",\"DE\",\"FR\",\"BE\",\"FR\",\"LU\",\"LU\",\"FR\",\"DE\"))\n\ntable(D3)\n#&gt; , , Score2 = A, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 1 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 1\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = BE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 1 0\n#&gt; \n#&gt; , , Score2 = B, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 1 1\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 1\n#&gt; \n#&gt; , , Score2 = C, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 1 1\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = DE\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 1 0\n#&gt;      C 2 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = FR\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = A, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = B, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 1\n#&gt;      B 0 0\n#&gt;      C 0 0\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = C, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 0 1\n#&gt;      D 0 0\n#&gt; \n#&gt; , , Score2 = D, Country = LU\n#&gt; \n#&gt;       Gender\n#&gt; Score1 F M\n#&gt;      A 0 0\n#&gt;      B 0 0\n#&gt;      C 1 0\n#&gt;      D 0 0\nThe same cross-tabulation is undertaken (counts) but now for each level of the third one.",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html#case-of-2-or-several-factors-contingency-tables",
    "href": "034_cross_tabulation.html#case-of-2-or-several-factors-contingency-tables",
    "title": "13  Cross-tabulation",
    "section": "",
    "text": "13.1.1 Margins\nA table object is supposed to store frequencies. In many cases one will need to also compute vertical and horizontal totals. This is doen by done by applying the addmargins() function to a table object. By default both margins are added\n\naddmargins(table(D2))\n#&gt;       Gender\n#&gt; Score1  F  M Sum\n#&gt;    A    2  2   4\n#&gt;    B    2  2   4\n#&gt;    C    5  3   8\n#&gt;    D    1  1   2\n#&gt;    Sum 10  8  18\naddmargins(table(D2), margin = 1)\n#&gt;       Gender\n#&gt; Score1  F  M\n#&gt;    A    2  2\n#&gt;    B    2  2\n#&gt;    C    5  3\n#&gt;    D    1  1\n#&gt;    Sum 10  8\naddmargins(table(D2), margin = 2)\n#&gt;       Gender\n#&gt; Score1 F M Sum\n#&gt;      A 2 2   4\n#&gt;      B 2 2   4\n#&gt;      C 5 3   8\n#&gt;      D 1 1   2\n\nSimilar functions are rowSums and rowCols. However, although they are more general as applicable to any data.frame, not just tables, they don’t assemble the table with margins, they are simply vectors\n\nrowSums(table(D2))\n#&gt; A B C D \n#&gt; 4 4 8 2\ncolSums(table(D2))\n#&gt;  F  M \n#&gt; 10  8\nclass (rowSums)\n#&gt; [1] \"function\"\nclass (addmargins(table(D2)))\n#&gt; [1] \"table\"  \"matrix\" \"array\"\n\n\n\n13.1.2 Proportions\nIn many instances as well, one needs to compute proportions rather than counts:\nThe prop.table() function does it by deafutl across all the two dimensions of the table:\n\nprop.table(table(D2))\n#&gt;       Gender\n#&gt; Score1          F          M\n#&gt;      A 0.11111111 0.11111111\n#&gt;      B 0.11111111 0.11111111\n#&gt;      C 0.27777778 0.16666667\n#&gt;      D 0.05555556 0.05555556\naddmargins(prop.table(table(D2)))\n#&gt;       Gender\n#&gt; Score1          F          M        Sum\n#&gt;    A   0.11111111 0.11111111 0.22222222\n#&gt;    B   0.11111111 0.11111111 0.22222222\n#&gt;    C   0.27777778 0.16666667 0.44444444\n#&gt;    D   0.05555556 0.05555556 0.11111111\n#&gt;    Sum 0.55555556 0.44444444 1.00000000\n\nWhile you may need the proportions of columns or rows only:\n\nprop.table(table(D2),margin = 1)\n#&gt;       Gender\n#&gt; Score1     F     M\n#&gt;      A 0.500 0.500\n#&gt;      B 0.500 0.500\n#&gt;      C 0.625 0.375\n#&gt;      D 0.500 0.500\nprop.table(table(D2),margin = 2)\n#&gt;       Gender\n#&gt; Score1     F     M\n#&gt;      A 0.200 0.250\n#&gt;      B 0.200 0.250\n#&gt;      C 0.500 0.375\n#&gt;      D 0.100 0.125\n\nSince they are table output you can also add margins to the proportions and make sure how it sums to 1:\n\naddmargins(prop.table(table(D2)))\n#&gt;       Gender\n#&gt; Score1          F          M        Sum\n#&gt;    A   0.11111111 0.11111111 0.22222222\n#&gt;    B   0.11111111 0.11111111 0.22222222\n#&gt;    C   0.27777778 0.16666667 0.44444444\n#&gt;    D   0.05555556 0.05555556 0.11111111\n#&gt;    Sum 0.55555556 0.44444444 1.00000000\naddmargins(prop.table(table(D2),margin = 1))\n#&gt;       Gender\n#&gt; Score1     F     M   Sum\n#&gt;    A   0.500 0.500 1.000\n#&gt;    B   0.500 0.500 1.000\n#&gt;    C   0.625 0.375 1.000\n#&gt;    D   0.500 0.500 1.000\n#&gt;    Sum 2.125 1.875 4.000\naddmargins(prop.table(table(D2),margin = 2))\n#&gt;       Gender\n#&gt; Score1     F     M   Sum\n#&gt;    A   0.200 0.250 0.450\n#&gt;    B   0.200 0.250 0.450\n#&gt;    C   0.500 0.375 0.875\n#&gt;    D   0.100 0.125 0.225\n#&gt;    Sum 1.000 1.000 2.000",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "034_cross_tabulation.html#case-of-a-numeric-vector",
    "href": "034_cross_tabulation.html#case-of-a-numeric-vector",
    "title": "13  Cross-tabulation",
    "section": "13.2 Case of a numeric vector",
    "text": "13.2 Case of a numeric vector\nLet’s add a numeric vector to our data-frame:\n\nD3[,\"Q\"]&lt;-c(rnorm(18, mean=12, sd=2))\n\nTable is useless in this case\n\ntable(D3[,c(\"Q\")])\n#&gt; \n#&gt; 7.38631799988652 9.60103391463592 10.2690086198913   11.47883540917 \n#&gt;                1                1                1                1 \n#&gt; 11.5797905267653 11.5894093133887 12.0109582243589 12.0280547196443 \n#&gt;                1                1                1                1 \n#&gt; 12.0544942201325 12.3679072370425 12.6904877358195 12.7427499237557 \n#&gt;                1                1                1                1 \n#&gt; 12.8002292317856 12.8467180223358 13.4174130435357 14.1252215112809 \n#&gt;                1                1                1                1 \n#&gt; 14.6245232774751 15.6864490854856 \n#&gt;                1                1\ntable(D3[,c(\"Country\", \"Q\")])\n#&gt;        Q\n#&gt; Country 7.38631799988652 9.60103391463592 10.2690086198913 11.47883540917\n#&gt;      BE                0                0                1              1\n#&gt;      DE                1                0                0              0\n#&gt;      FR                0                0                0              0\n#&gt;      LU                0                1                0              0\n#&gt;        Q\n#&gt; Country 11.5797905267653 11.5894093133887 12.0109582243589 12.0280547196443\n#&gt;      BE                0                0                0                0\n#&gt;      DE                0                1                0                1\n#&gt;      FR                1                0                1                0\n#&gt;      LU                0                0                0                0\n#&gt;        Q\n#&gt; Country 12.0544942201325 12.3679072370425 12.6904877358195 12.7427499237557\n#&gt;      BE                0                0                0                0\n#&gt;      DE                1                0                1                0\n#&gt;      FR                0                0                0                0\n#&gt;      LU                0                1                0                1\n#&gt;        Q\n#&gt; Country 12.8002292317856 12.8467180223358 13.4174130435357 14.1252215112809\n#&gt;      BE                0                0                0                0\n#&gt;      DE                1                1                1                0\n#&gt;      FR                0                0                0                1\n#&gt;      LU                0                0                0                0\n#&gt;        Q\n#&gt; Country 14.6245232774751 15.6864490854856\n#&gt;      BE                0                0\n#&gt;      DE                0                0\n#&gt;      FR                1                0\n#&gt;      LU                0                1\n\nAggregation of a numeric vector over factor levels require a certain statistcis to be computed, for example a center or dispersion indicator. We use the function aggregate for that purpose.\n\nmeanCountry&lt;-aggregate(D3[\"Q\"], by=D3[\"Country\"], FUN=\"mean\")\nsdCountry&lt;-aggregate(D3[\"Q\"], by=D3[\"Country\"], FUN=\"sd\")\ncbind(meanCountry, sdCountry)\n#&gt;   Country        Q Country         Q\n#&gt; 1      BE 10.87392      BE 0.8554767\n#&gt; 2      DE 11.85164      DE 1.8945580\n#&gt; 3      FR 13.08512      FR 1.5134303\n#&gt; 4      LU 12.59954      LU 2.4895923\n\naggregate() essentially splits the data into subsets, and computes the requested summary statistics (FUN) for each.\nor even across several factors using a formula and the data argument rather than by\n\nmeanCountry2&lt;-aggregate(Q ~ Country, data=D3, FUN=\"mean\")\nmeanCountry2\n#&gt;   Country        Q\n#&gt; 1      BE 10.87392\n#&gt; 2      DE 11.85164\n#&gt; 3      FR 13.08512\n#&gt; 4      LU 12.59954\nmeanCountryGender&lt;-aggregate(Q ~ Country + Gender, data=D3, FUN=\"mean\")\nmeanCountryGender\n#&gt;   Country Gender        Q\n#&gt; 1      BE      F 11.47884\n#&gt; 2      DE      F 12.14804\n#&gt; 3      FR      F 13.08512\n#&gt; 4      LU      F 11.17189\n#&gt; 5      BE      M 10.26901\n#&gt; 6      DE      M 11.67380\n#&gt; 7      LU      M 14.02718",
    "crumbs": [
      "Part III - Univariate analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cross-tabulation</span>"
    ]
  },
  {
    "objectID": "041_distributions_unif_normal.html",
    "href": "041_distributions_unif_normal.html",
    "title": "14  Uniform and normal distributions",
    "section": "",
    "text": "14.1 The uniform distribution\nThe uniform distribution is one of the simplest distribution of a continuous (ratio or interval) variable. Although it is rare in practice that each value along a continuum has the same chance of occurring than all others, it is a base distribution to know of and the source of generating random numbers.\nEach value has the same probability of occurrence. It is a simple case through which we show how we can compute density, probabilities and cumulative distributions functions in R.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "041_distributions_unif_normal.html#the-uniform-distribution",
    "href": "041_distributions_unif_normal.html#the-uniform-distribution",
    "title": "14  Uniform and normal distributions",
    "section": "",
    "text": "14.1.1 Definition and key characteristics\nA random variable \\(X \\in [a;b]\\) following an uniform distribution, can be expressed as: \\(X \\sim U[a;b]\\), with expectation \\(E(X) = \\dfrac{a+b}{2}\\) and variance \\(Var(X) = \\dfrac{(b-a)^2}{12}\\).\nMath note: The denominator, 12, of the variance may seem a little surprising, but results from an integration. Indeed, the variance is the expected value of the squared deviations to the mean. You thus integrate the squared difference between each point \\(X\\) and the mean, which itself is \\((a+b)/2\\), over the interval \\([a, b]\\), that is integrating \\(((X-(a+b)/2)^2)/(b-a)\\), leading to the above defined \\(Var(X)\\).\nTo generate empirically a uniform function, we feed the runif() function with a number of observations and a range, i.e. a minimum value (default \\(a=0\\)) and a maximum value (default \\(b=1\\)).\n\nset.seed(101)\nu&lt;-runif(1000, min=10, max=30)\nhist(u)\n\n\n\n\n\n\n\n\nThe histogram shows that each value (interval of values) is similarly frequent.\n\n\n14.1.2 Density\nInstead of a histogram, or on top of it, we often plot densities, which is a smoother representation than the bars. The computation uses a local density (kernel density i.e. using a bandwidth instead of the strict silos of the histogram). Beyond the visual, the notion of density is important because, once it is scaled so that the entire area under the curve sums to 1, it is interpretable as a probability.\nIn R in order to compute the density for an empirical distribution, we use the density() function and use its returned values for plotting. Note that we don’t have frequencies anymore along the y-axis, but values below 1, i.e. probabilities. This is also why plotting the density on top of a histogram requires further fine tuning (which we leave out for now).\n\ndensity(u)\n#&gt; \n#&gt; Call:\n#&gt;  density.default(x = u)\n#&gt; \n#&gt; Data: u (1000 obs.); Bandwidth 'bw' = 1.316\n#&gt; \n#&gt;        x               y            \n#&gt;  Min.   : 6.06   Min.   :7.237e-05  \n#&gt;  1st Qu.:13.03   1st Qu.:1.876e-02  \n#&gt;  Median :20.00   Median :4.731e-02  \n#&gt;  Mean   :20.00   Mean   :3.581e-02  \n#&gt;  3rd Qu.:26.96   3rd Qu.:5.010e-02  \n#&gt;  Max.   :33.93   Max.   :5.278e-02\nplot(density(u))\n\n\n\n\n\n\n\n\nI order to obtaine the corresponding continuous - not numerically simulated - density, using the same definition (i.e. min=10 and max=30) and a similar range for display (5 to 35), we can use dunif() within the curve() function:\n\ncurve(dunif(x,min=10,max=30),5,35)\n\n\n\n\n\n\n\n\nwhere we see more clearly that the probability to obtain a particular value with a uniform distribution is constant over the defined range and is zero otherwise.\nThe previous plot based on a numerical empirical generation runif() is of course less regular, and there was some smoothing at the borders of the graph due to the density being computed within a kernel (bandwidth). Yet, the kernel is necessary in practice for having some increment. In theory, the density can be defined over a point, i.e. for an infinitesimal delta of x (\\(\\delta x\\)), but in practice you need a discrete interval (\\(dx\\)), hence the different ‘look’ of our two density plots: the continuous theoretical one using d...() and the numerical empirical one using density().\nMathematically, the density of a uniform distribution is given by (see help(dunif)) :\n\\[\n\\begin{align}\nf_X(x) &= \\dfrac{1}{b-a} &\\text{ for } a \\le x \\le b \\\\\nf_X(x) &= 0 &\\text{ otherwise}\n\\end{align}\n\\] where \\(b\\) is the maximum and \\(a\\) the minimum whereby the distribution is defined.\nIn our example we now see why the constant was at 0.05, i.e. (\\(1/(30-10)\\))\nAnd can check it for any point x fed into dunif()\n\ndunif(c(2, 10, 15, 20, 30, 55),min=10, max=30)\n#&gt; [1] 0.00 0.05 0.05 0.05 0.05 0.00\n\n\n\n14.1.3 Cumulated probabilities\nFor any continuous distribution, it is interesting to accumulate the probabilities along the x values so that this cumulative sum indicates the probability of randomly drawing a number that would fall below any given value. This is called the cumulative density function, aka cdf.\nSimilar to the density, there is both a numerical empirical way and a continuous theoretical way to get the cumulative density function in R, using respectively the ecdf() function or the p...() function corresponding to the distribution of interest.\nWith our empirically generated (sample) uniform distribution \\(u\\), we compute the empirical cumulative distribution function (ecdf) as follows:\n\necdf(u)\n#&gt; Empirical CDF \n#&gt; Call: ecdf(u)\n#&gt;  x[1:1000] = 10.007, 10.015, 10.048,  ..., 29.977, 29.986\nplot(ecdf(u))\n\n\n\n\n\n\n\n\nWe can see that it is (almost) a straight line. Its slope is of course the density we have seen above. density() is the derivative of the ecdf() and the ecdf() the integral (cumulative sum) of density(). For every increment of x, we increase the probability of drawing a number below x by 0.05, up until the max (30) is reached.\nInstead of simulating numbers or using an empirical sample, we can use the theoretical punif() function in this case to obtain the theoretical cumulative density function corresponding to our parameters.\n\ncurve(punif(x,min=10, max=30),5,35)\n\n\n\n\n\n\n\n\nWe see it is the theoretical continuous version of ecdf() applied to our vector \\(u\\).\nMathematically, the cumulative distribution function of the uniform is defined by: \\[\\begin{align}\nF_X(x) &= 0 &\\text{ for }& x &lt; a \\\\\nF_X(x) &= \\dfrac{x-a}{b-a} &\\text{ for }& a \\le x \\le b \\\\\nF_X(x) &= 1 &\\text{ for }& x &gt; b\n\\end{align}\n\\]\nInstead of using the p...() function (similar for d...()) with a general x variable, we can supply a quantile (or a set of quantiles), in order to obtain the probability of drawing a number below this quantile.\nWith the uniform distribution and punif() it is very straightforward because the cumulative probability simply is the quantile as shown below\n\npunif(q=c(0.01,0.25,0.5,0.75,0.99))\n#&gt; [1] 0.01 0.25 0.50 0.75 0.99\npunif(min=50, max=200, q=c(100)) #the probability of drawing a number below 100 knowing the min and max are 50 and 200 and the distribution uniform is 1/3 (the range being 150 and 100 being 50 beyond the max)\n#&gt; [1] 0.3333333\n\nThe last of the 4 R functions for distributions is the reverse of p...(), i.e. q...(), which provides the corresponding quantile for any given probability \\(p\\). It is again quite straightforward for a uniform distribution defined over the range 0 to 1 since the quantile is then the probability, and proportionally within the defined range for other uniform distributions.\n\ncurve(qunif(x,min=1000,max=10000))\n\n\n\n\n\n\n\nqunif(p=0.77,min=1000,max=10000)\n#&gt; [1] 7930",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "041_distributions_unif_normal.html#the-normal-distribution",
    "href": "041_distributions_unif_normal.html#the-normal-distribution",
    "title": "14  Uniform and normal distributions",
    "section": "14.2 The Normal distribution",
    "text": "14.2 The Normal distribution\nThe Normal or Gaussian distribution is the workhorse of statistics and the most used distribution. We have seen its general bell shape earlier.\n\n14.2.1 Definition and key characteristics:\nA normal distribution is entirely defined by its mean and standard deviation, which we thus need to provide for generating examples.\nA normal distribution is thus denoted by \\[X \\sim N(\\mu, \\sigma^2) \\ \\  \\  \\forall X \\in R\\]\nwhere \\(\\mu\\) and \\(\\sigma^2\\) are the real and unknown mean and variance of the studied population. It is defined over the entire set of real numbers R.\nLet’s generate a normal distribution and observe its histogram.\n\nset.seed(101)\nN&lt;-rnorm(100000,mean=5,sd=1)\nhist(N, breaks = 100)\n\n\n\n\n\n\n\nmean(N)\n#&gt; [1] 5.002881\nmedian(N)\n#&gt; [1] 5.00018\nsd(N)\n#&gt; [1] 0.9947701\n\n\nSymmetry\nWith a numeric example the characteristics are not exact, but the mean and standard deviations are close to those requested. We also see it is a symmetric distribution and the mean thus equals the median (approximately here). In other words half of the values are below the mean and the other half above.\nSymmetry also means that the skewness (3rd moment) is close to zero. Note that its 4th moment (Kurtosis) is theoretically 3, but R takes out this value in its computation so the kurtosis of the normal distribution is 0, which is used as a reference. Again it is approximately given this is only a numerical example of it.\n\nmean(N)\n#&gt; [1] 5.002881\nmedian(N)\n#&gt; [1] 5.00018\ne1071::skewness(N)\n#&gt; [1] -0.001284229\ne1071::kurtosis(N)\n#&gt; [1] 0.004644777\n\nWe can look at the impact of generating a smaller sample, to show how means, medians, skewness or kurtosis and the histogram vary quite a bit from the expected when the size of the sample decreases:\n\nN10000&lt;-rnorm(10000,mean=5,sd=1)\nN1000&lt;-rnorm(1000,mean=5,sd=1)\nN100&lt;-rnorm(100,mean=5,sd=1)\nN10&lt;-rnorm(10,mean=5,sd=1)\nhist(N10000, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N1000, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N100, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\nhist(N10, breaks = 100, border=NA, col=rgb(1, 0.5, 1, 0.5))\n\n\n\n\n\n\n\n\n\n\nQuantiles and standard deviations\nA key feature of the normal distribution is that approximately 68% of its values fall within one standard deviation from the mean (both side as it is symmetrical), 95% within two standard deviations, and 99.7% within three standard deviations. This characteristic is heavily used to make probability tests.\n\n\n\n14.2.2 Density\nWe know compute the empirical density for our sample, showing the previous histogram with large sample size was indeed bell-shaped (but not so much the small sample examples)\n\ndensity(N)\n#&gt; \n#&gt; Call:\n#&gt;  density.default(x = N)\n#&gt; \n#&gt; Data: N (100000 obs.);   Bandwidth 'bw' = 0.08953\n#&gt; \n#&gt;        x                y            \n#&gt;  Min.   :0.5144   Min.   :0.0000007  \n#&gt;  1st Qu.:2.8273   1st Qu.:0.0011045  \n#&gt;  Median :5.1403   Median :0.0258991  \n#&gt;  Mean   :5.1403   Mean   :0.1078749  \n#&gt;  3rd Qu.:7.4533   3rd Qu.:0.2048571  \n#&gt;  Max.   :9.7663   Max.   :0.3981291\nplot(density(N)) #The integral of the density being 1, overlaying on top of the histogram doesn't work because it depends on the heights of the bar which itself depends on the number of bars. See Crawley, p215 for such a manipulation or later with ggplot\n\n\n\n\n\n\n\n\n\nplot(density(N10000), main=\"Varying sample size\", col=rgb(1, 0.5, 1, 0.5), lwd=5,xlab=\"x\")\nlines(density(N1000), col=rgb(1, 0.5, 1, 0.5),lwd=3)\nlines(density(N100), col=rgb(1, 0.5, 1, 0.5), lwd=2)\nlines(density(N10), col=rgb(1, 0.5, 1, 0.5), lwd=1)\n\n\n\n\n\n\n\n\nTo obtain the corresponding theoretical function of the density, using the same mean and standard deviation, and along the same range of x values (0 to 10) for the graph, we can use dnorm() instead of the empirical function density(). Observe also that the density of a normal distribution at its peak, i.e. at the mean or median, is around 0.39.\n\ncurve(dnorm(x,mean=5,sd=1),0,10)\n\n\n\n\n\n\n\ndnorm(5, mean=5,sd=1)\n#&gt; [1] 0.3989423\n\nMathematically, the density of a norma distribution is given by the following, which is used within dnorm() (see help(dnorm)) :\n\\[f(x) = \\dfrac {1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp{(- \\dfrac{1}{2} \\dfrac{(x-\\mu)^2}{\\sigma^2})}\\]\nIn the following, we play with the two defining parameters, mean and standard deviation, to show how each impact the shape of the distribution while keeping the second parameter constant:\n\ncurve(dnorm(x,mean=0,sd=1),-5,5, lty=1)\ncurve(dnorm(x,mean=-2,sd=1),-5,5, lty=2, add=TRUE)\ncurve(dnorm(x,mean=1,sd=1),-5,5, lty=3, add=TRUE)\n\n\n\n\n\n\n\n\ncurve(dnorm(x,mean=0,sd=1),-5,5, lty=1, col=\"blue\")\ncurve(dnorm(x,mean=0,sd=0.7),-5,5, lty=2, col=\"blue\", add=TRUE)\ncurve(dnorm(x,mean=0,sd=2),-5,5, lty=3, col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\n\nThe general shape is not influenced by the mean as the distribution is simply translated along x. When the standard deviation increases, as expected, the distribution is flatter but still around the same mean.\nIf we generate two new (large) samples with a different standard deviation, we see the flattening. However, the shape is still a bell shape and the kurtosis will not pick the difference!. The peakness referred to by the kurtosis is one that is relative to the corresponding normal distribution (thus knowing its variance).\n\nset.seed(101)\nZ&lt;-rnorm(1000000,mean=0,sd=1)\nFlat&lt;-rnorm(1000000,mean=0,sd=3)\nplot(density(Z))\nlines(density(Flat))\n\n\n\n\n\n\n\nkurtosis(Z)\n#&gt; Error in kurtosis(Z): could not find function \"kurtosis\"\nkurtosis(Flat)\n#&gt; Error in kurtosis(Flat): could not find function \"kurtosis\"\n\nWe know explore graphically and with the function dnorm() the other property of the normal distribution by which we know that in a normal distribution, the range of values situated - + - 1 standard deviation from the mean represents 68% of the data - + - 2 standard deviations from the mean represents 95% of the data - + - 3 standard deviations from the mean represents 99.7% of the data\n\ncurve(dnorm(x, 0, 1), col = 'green', lwd = 8, xlim = c(-3, 3), main = 'Density function of X ~ N(0,1)\\n Intervals', ylab = 'f(x)')\ncurve(dnorm(x, 0, 1), add = TRUE, col = 'gold', lwd = 5, xlim = c(-2, 2))\ncurve(dnorm(x, 0, 1), add = TRUE, col = \"red\", lwd = 2, xlim = c(-1, 1))\nabline(v = 0, col = \"grey\", lty = 1)\nabline(v = c(-1, 1), col = \"red\", lty = 3)\nabline(v = c(-2, 2), col = \"gold\", lty = 3)\nabline(v = c(-3, 3), col = \"green\", lty = 3)\nlegend(\"topleft\", lwd = c(2,5,8),\n       legend = c(\"68% in [ mu +/- sigma ]\", \n                  \"95% in [ mu +/- 2*sigma ]\", \n                  \"99.7% in [ mu +/- 3*sigma ]\"),\n       col = c(\"red\", 'gold', 'green'))\n\n\n\n\n\n\n\n\n\n\ncurve(dnorm(x, 0, 1), col = 'green', lwd = 8, xlim = c(-2.58, 2.58), main = 'Density function of X ~ N(0,1)\\n Intervals', ylab = 'f(x)')\ncurve(dnorm(x, 0, 1), add = TRUE, col = 'gold', lwd = 5, xlim = c(-1.96, 1.96))\ncurve(dnorm(x, 0, 1), add = TRUE, col = \"red\", lwd = 2, xlim = c(-1.645, 1.645))\nabline(v = 0, col = \"grey\", lty = 1)\nabline(v = c(-1.645, 1.645), col = \"red\", lty = 3)\nabline(v = c(-1.96, 1.96), col = 'gold', lty = 3)\nabline(v = c(-2.58, 2.58), col = 'green', lty = 3)\nlegend(\"topleft\", lty = 1, lwd = c(2,5,8),\n       legend = c(\"90% in [ +/- 1.645 ]\", \n                  \"95% in [ +/- 1.96 ]\", \n                  \"99% in [ +/- 2.58 ]\"),\n       col = c(\"red\", 'gold', 'green'))\n\n\n\n\n\n\n\n\n\n\n14.2.3 Cumulated probabilities\nSimilar to the case of the uniform distribution, we can also look at the cumulative probability. Theoretically we use pnorm(), which it the equivalent of the uniform punif(). Empirically we can use the ecdf() function again since it makes no assumption on the distribution (for it is used to explore empirical material).\n\ncurve(pnorm(x),-4,4) #theoretical (unit normal)\n\n\n\n\n\n\n\n\nN&lt;-rnorm(100)\nplot(ecdf(N)) #sample\n\n\n\n\n\n\n\n\nWe see that the bell shape of the density (from dnorm()) is the derivative of an S-shaped function. The probability of drawing a number below a certain value is not constantly increasing as in the case of the uniform distribution. Rather, the probability grows slowly for lower values, then very quickly around the mean where the mass of data is found, then slows downs and plateau again for higher values.\nMathematically, the cumulative distribution function of the standard normal distribution is the integral of the probability distribution \\(f(x)\\) defined earlier. It is often denoted by \\(\\Phi(x)\\) but has no closed solution and thus statistical software compute values numerically.\n\nplot(x, pnorm(x, 0, 1), type='l', col=2, ylab = 'F(x)', \n     main = 'Cumulative Distribution Function of N(mu ; 1)')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlines(x, pnorm(x, 2, 1), col='darkorange')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlines(x, pnorm(x, 4, 1), col='gold')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlegend(\"topleft\", lty = 1,\n       legend = c(\"mu = 0\", \"mu = 2\", \"mu = 4\"),\n       col = c(2, 'darkorange', 'gold'))\n#&gt; Error in (function (s, units = \"user\", cex = NULL, font = NULL, vfont = NULL, : plot.new has not been called yet\n\n\nplot(x, pnorm(x, 0, 1), type='l', col=2, ylab = 'F(x)', \n     main = 'Cumulative Distribution Function of N(0 ; sigma^2)')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlines(x, pnorm(x, 0, 2), col='purple')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlines(x, pnorm(x, 0, 4), col='blue')\n#&gt; Error in eval(expr, envir, enclos): object 'x' not found\nlegend(\"topleft\", lty = 1,\n       legend = c(\"sigma = 1\", \"sigma = 2\", \"sigma = 4\"),\n       col = c(2, 'purple', 'blue'))\n#&gt; Error in (function (s, units = \"user\", cex = NULL, font = NULL, vfont = NULL, : plot.new has not been called yet\n\nWe can also use pnorm()to verify the key property of the normal distribution, that 68 % of the values fall in between -1 and +1 standard deviation from the mean, and respectively 95% and 98.7% for 2 and 3 standard deviations.\n\n1-(pnorm(-1)+(1-pnorm(1))) #1 minus the sum of the probability of being below -1 and being above 1)\n#&gt; [1] 0.6826895\n\n1-(pnorm(-2)+(1-pnorm(2))) \n#&gt; [1] 0.9544997\n\n1-(pnorm(-3)+(1-pnorm(3))) \n#&gt; [1] 0.9973002\n\nOf course pnorm()can be interrogated for any normal distribution, i.e. with other means and standard deviations. For example, suppose we know the mean of GDP /capita for all countries in Europe is 40 000 euro on average with a standard deviation of 20 000 and we have indication that the distribution is normal. What is the probability of finding a country below the GDP of Latvia, i.e. 22 000 euro per capita or Luxembourg 100 000 euro per capita?\n\npnorm(22000,mean=40000,sd = 20000)\n#&gt; [1] 0.1840601\npnorm(100000,mean=40000,sd = 20000)\n#&gt; [1] 0.9986501\n\nWe see that about 18.4% of the countries will have a GDP lower than Latvia and 99.9% will be lower than Luxembourg.\nThe other way around, we can ask what would be the GDP/capita for a country to be in the top 1% or top 10% using qnorm()",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Uniform and normal distributions</span>"
    ]
  },
  {
    "objectID": "042_CLT_Standardisation.html",
    "href": "042_CLT_Standardisation.html",
    "title": "15  Central Limit Theorem and standardisation",
    "section": "",
    "text": "15.1 Central Limit Theorem\nThe reason for the success of the normal distribution is not only due to its key features (symmetry and the linkage between quantiles and standard deviations) but is also explained by the Central Limit Theorem:\nLet’s draw 5 times a 100 numbers from a uniform distribution within the interval 0 to 10. For each of the five cases, the average should be close to 5.\nmean(runif(100)*10)\n#&gt; [1] 4.73003\nmean(runif(100)*10)\n#&gt; [1] 5.18611\nmean(runif(100)*10)\n#&gt; [1] 4.891777\nmean(runif(100)*10)\n#&gt; [1] 5.668358\nmean(runif(100)*10)\n#&gt; [1] 5.339969\nWe can write this in a loop and produce a histogram of the means:\nn&lt;-5\nmeans&lt;-numeric(n)\nfor (i in 1:n){\n  means[i]&lt;-mean(runif(100)*10)\n}\nmeans\n#&gt; [1] 4.318216 4.774432 4.908065 5.336650 4.885099\nhist(means)\nIt is not quite impressive at this stage, but if we repeat, say 10000 times the experiment rather than 5, the histogram tends towards the shape of a normal distribution!\nn&lt;-10000\nmeans&lt;-numeric(n)\nfor (i in 1:n){\n  means[i]&lt;-mean(runif(100)*10)\n}\nhead(means)\n#&gt; [1] 4.914835 4.703657 4.803858 4.979406 4.549791 5.137734\nhist(means, breaks=100)\nAnd the median, mean and standard deviation of the “means” are\nmedian(means)\n#&gt; [1] 4.998348\nmean(means)\n#&gt; [1] 5.000063\nsd(means)\n#&gt; [1] 0.283767",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "042_CLT_Standardisation.html#central-limit-theorem",
    "href": "042_CLT_Standardisation.html#central-limit-theorem",
    "title": "15  Central Limit Theorem and standardisation",
    "section": "",
    "text": "If you take repeated samples from a population with finite variance and calculate their averages, then the averages will be normally distributed (Crawley (2012), p213)",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "042_CLT_Standardisation.html#standardization",
    "href": "042_CLT_Standardisation.html#standardization",
    "title": "15  Central Limit Theorem and standardisation",
    "section": "15.2 Standardization",
    "text": "15.2 Standardization\nIn order to visually appreciate how well the obtained distribution compares to a corresponding normal distribution, we can generate a normal distribution with the same parameters (mean and variance). Alternatively, we can also standardize the outcome in order to fit a “Unit” normal distribution, i.e. one where the mean is 0 and the standard deviation is 1.\nThis process is called Standardization. It involves centering an empirical variable, i.e. computing deviations to the mean, so that the mean is set to 0, and reducing by dividing the centered by the standard deviation. Hence making the new standard deviation equal to 1. One often name a standradized variable Z. The built in function for standardization is scale(). It centers and reduces by default, but you can toggle one or the other on or off.\nImportant: Since standardization involves a division by a number (standard deviation) with the same units as the original variable, a standardized variable has no unit ! This is a property you may like for comparing with other variables that have different original units.\n\nCentered&lt;-means-mean(means) #centering\nZ&lt;-Centered/sd(means) #reducing /scaling\n#or simply \nZ&lt;-scale(means, center = TRUE, scale = TRUE)\n\n#Compare\ncbind(mean(means), mean(Z))\n#&gt;          [,1]         [,2]\n#&gt; [1,] 5.000063 1.256143e-15\ncbind(sd(means), sd(Z))\n#&gt;          [,1] [,2]\n#&gt; [1,] 0.283767    1\n\n\n15.2.1 Standardized density from many repetitions\nWe can now compare our empirical distribution density (blue) to the theoretical density of a unit normal distribution (black). The overlay effectively proves the Central Limit Theorem, showing that even from a very simple distribution like the uniform distribution, the mean over repeated samples is distributed normally.\n\ncurve(dnorm,-4,4) #Density of theoretical normal distribution\nlines(density(Z), col=\"blue\") #Density of our empirical distribution after n repetitions\n\n\n\n\n\n\n\n\n\n\n15.2.2 QQplot\nAnother classical way to know whether an empirical distribution corresponds well to a given theoretical one is to use a quantile-quantile plot, aka qqplot, where empirical quantiles are plotted along the y-axis and theoretical ones (e.g. normal) along the x-axis.\nWe expect a straight line at least in a good range of values around the mean. Towards the extreme there are less points and the variability is thus greater. Given we look at the shape of the curve, not the values, we don’t need to standardize the data ahead this time (but of course then the values on the two axes will differ). We verify visually that the qqplot of our mean of means is close to a straight line.\n\nqqnorm(y=means)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Central Limit Theorem and standardisation</span>"
    ]
  },
  {
    "objectID": "043_distributions_other_continuous.html",
    "href": "043_distributions_other_continuous.html",
    "title": "16  Usual Continuous Distributions",
    "section": "",
    "text": "16.1 Exponential\nA random variable \\(X \\in [0;+\\infty[\\) following an exponential distribution with parameter \\(\\lambda &gt; 0\\) can be expressed as: \\(X \\sim \\epsilon(\\lambda)\\) with expectation \\(E(X) = 1/ \\lambda\\) and variance \\(Var(X) = 1/ \\lambda^2\\).\nIts density function can be written as:\n\\[f(x) = λ {e}^{- λ x} \\text{ for } x \\geq 0\\] \\[f(x) = 0 \\text{ for } x &lt; 0\\]\nIts cumulative distribution function is defined by:\n\\[F_X(x) = 1 - \\exp{(- \\lambda x)} \\text{ for } x \\geq 0\\] \\[F_X(x) = 0 \\text{ for } x &lt; 0\\]\nx &lt;- seq(0,8,0.01)\ndExp.5 &lt;- dexp(x, .5)\ndExp1 &lt;- dexp(x, 1)\ndExp2 &lt;- dexp(x, 2)\ndExp3 &lt;- dexp(x, 3)\nplot(x, dExp3, type = 'l', col = 'green',\n     main = 'Density function of X~Exp(lambda)', ylab = 'f(x)')\nlines(x, dExp2, col = 'gold')\nlines(x, dExp1, col = 'darkorange')\nlines(x, dExp.5, col = 2)\nlegend(\"topright\", lty = 1,\n       legend = c(\"lambda = 0.5\", \"lambda = 1\", \n                  \"lambda = 2\", \"lambda = 3\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\nplot(x, pexp(x, 3), type = 'l', col = 'green',\n     main = 'Cumulative Distribution Function of X~Exp(lambda)', ylab = 'F(x)')\nlines(x, pexp(x, 2), col='gold')\nlines(x, pexp(x, 1), col='darkorange')\nlines(x, pexp(x, .5), col=2)\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"lambda = 0.5\", \"lambda = 1\", \"lambda = 2\", \"lambda = 3\"),\n       col = c(2, 'darkorange', 'gold', 'green'))",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Usual Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "043_distributions_other_continuous.html#chi-square",
    "href": "043_distributions_other_continuous.html#chi-square",
    "title": "16  Usual Continuous Distributions",
    "section": "16.2 Chi-square",
    "text": "16.2 Chi-square\nA random variable \\(X\\) following a chi-square (or khi-square) distribution can be expressed as: \\(X \\sim \\chi^2(k)\\) with parameter \\(k&gt; 0\\), expectation \\(E(X) = k\\), variance \\(Var(X) = 2k\\).\n\\(k\\) represents the degree of freedom.\nThe density function is\n\\[f_X(x) = \\dfrac {1} {2^{k/2} \\gamma(k/2)} x^{(k/2)-1} \\exp{(\\dfrac{-x}{2})} \\text{ for } x \\geq 0\\]\n\\[f_X(x) = 0 \\text{ for } x &lt; 0\\]\n\nx &lt;- seq(0,8,0.01)\ndChiSq1 &lt;- dchisq(x, df=1)\ndChiSq2 &lt;- dchisq(x, df=2)\ndChiSq3 &lt;- dchisq(x, df=3)\ndChiSq6 &lt;- dchisq(x, df=6)\n\nWe see (and know from the CLT) that when the degrees of freedom increases, the function gets closer to a normal distribution. However it is not symmetrical and the right hand side stays longer than in the normal distribution for quite some time.\n\nplot(x, dChiSq1, type = 'l', col = 2, ylim = c(0, 1.25),\n     main = 'Density function of X~Chi^2(p)', ylab = 'f(x)')\nlines(x, dChiSq2, col = 'darkorange')\nlines(x, dChiSq3, col = 'gold')\nlines(x, dChiSq6, col = 'green')\nlegend(\"topright\", lty = 1,\n       legend = c(\"k = 1\", \"k = 2\", \"k = 3\", \"k = 6\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nThe cumulative density is as follows:\n\nplot(x, pchisq(x, df=1), type = 'l', col = 2,\n     main = 'Cumulative Distribution Function of X~Chi2(p)', ylab = 'F(x)')\nlines(x, pchisq(x, df=2), col='darkorange')\nlines(x, pchisq(x, df=3), col='gold')\nlines(x, pchisq(x, df=6), col='green')\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"k = 1\", \"k = 2\", \"k = 3\", \"k = 6\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nThe chi-square distribution is typically used to test the independence of two variables. It is typically applied to a categorical or ordinal metric along two categories, e.g. the level of education of Luxembourg natives vs Luxembourg migrants or the counts of votes for Kamala Harris among young vs elders population or among urban vs rural population. It is thus often applied to contingency tables after the cross-tabulation of two factors in order to test if observed frequencies differ from expected frequencies.\nThe df considered are usually low since it corresponds to the number of categories-1 (e.g. urban+rural, i.e. 2 -1=1) times the types of output -1, (e.g. vote for Harris vs Trump, i.e. 2-1=1). Thus far from a normal distribution.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Usual Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "043_distributions_other_continuous.html#student-t-distribution",
    "href": "043_distributions_other_continuous.html#student-t-distribution",
    "title": "16  Usual Continuous Distributions",
    "section": "16.3 Student t-distribution",
    "text": "16.3 Student t-distribution\nA random variable \\(X\\) following a student distribution, can be expressed as: \\(X \\sim t(n)\\) with parameter \\(n\\) degrees of freedom.\nIts density function is\n\\[f(x) = \\dfrac{1}{\\sqrt{n\\pi}} \\dfrac {\\gamma{(\\dfrac{n+1}{2})}} {\\gamma{(\\dfrac{n}{2})}} \\dfrac {1} {(1+\\dfrac{x^2}{n})^{(n+1)/2}} \\text{ for all } x\\in R\\] We have: \\[E(X) = 0 \\text{ for } n \\ge 2\\] \\[Var(X) = \\dfrac{n}{n-2} \\text{ for } n \\ge 3\\]\nIf we have a random variable \\(U \\sim N (0, 1)\\) and a random variable \\(X \\sim \\chi^2(n)\\) which are independent, then the random variable \\(T_n = \\dfrac{U}{\\sqrt{X/n}}\\) follows a student distribution \\(t(n)\\)\n\nx &lt;- seq(-8, 8, 0.01)\ndStudent1 &lt;- dt(x, 1)\ndStudent2 &lt;- dt(x, 2)\ndStudent4 &lt;- dt(x, 4)\n\n\nplot(x, dnorm(x), type = 'l', col = 'darkgray', xlim = c(-4, 4), lty = 3,\n     main = 'Density function of X~t(n)', ylab = 'f(x)')\nlines(x, dStudent4, col = 'gold')\nlines(x, dStudent2, col = 'darkorange')\nlines(x, dStudent1, col = 2)\nlegend(\"topright\", lty = c(rep(1, 3), 3),\n       legend = c(\"t(n = 1)\", \"t(n = 2)\", \"t(n = 4)\", \"N(0,1)\"),\n       col = c(2, 'darkorange', 'gold', 'darkgray'))\n\n\n\n\n\n\n\n\n\nplot(x, pnorm(x), type = 'l', col = 'darkgray', lty = 3,\n     main = 'Cumulative Distribution Function of X~t(n)', ylab = 'F(x)')\nlines(x, pt(x, 2), col='gold')\nlines(x, pt(x, 3), col='darkorange')\nlines(x, pt(x, 6), col=2)\nlegend(\"bottomright\", lty = c(rep(1, 3), 3),\n       legend = c(\"t(n = 1)\", \"t(n = 2)\", \"t(n = 4)\", \"N(0,1)\"),\n       col = c(2, 'darkorange', 'gold', 'darkgray'))\n\n\n\n\n\n\n\n\nCompared to the normal distribution, we can see the t-distribution depends solely on \\(n\\). It is used for continuous variables in the non-rare cases where the sample size is small and the variance of the population is unknown.\nt-Student’s tests are typically used to test whether two samples have different means or if a sample mean differ from a given value.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Usual Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "043_distributions_other_continuous.html#fisher-snedecor",
    "href": "043_distributions_other_continuous.html#fisher-snedecor",
    "title": "16  Usual Continuous Distributions",
    "section": "16.4 Fisher-Snedecor",
    "text": "16.4 Fisher-Snedecor\nA random variable \\(X\\) following a Fisher-Snedecor distribution can be expressed as \\(X \\sim F(n, p)\\) with parameters \\(n\\) and \\(p\\) degrees of freedom.\nIts density function is\n\\[f_X(x) = \\dfrac {\\gamma(\\dfrac{n+p}{2})} {\\gamma(\\dfrac{n}{2}) \\gamma(\\dfrac{p}{2})} (\\dfrac{n}{p})^{(n/2)} \\dfrac{x^{\\dfrac{n-2}{2}}}{(1 + \\dfrac{n}{p}x)^{\\dfrac{n-2}{2}}} \\text{ for } x \\geq 0\\] \\[f_X(x) = 0 \\text{ for } x &lt; 0\\]\nWe have:\n\\[E(X) = \\dfrac{p}{p-2} \\text{ for } p \\ge 3\\] \\[Var(X) = \\dfrac{2p^2(n+p-2)}{n(p-2)^2(p-4)} \\text{ for } p \\ge 5\\]\nIf we have two random variables \\(X\\) and \\(Y\\) independent such that \\(X \\sim \\chi^2(n)\\) and \\(Y \\sim \\chi^2(p)\\), then the random variable \\(\\dfrac{X/n}{Y/p}\\) follows a Fisher distribution \\(F(n, p)\\)\n\nx &lt;- seq(0, 3, 0.01)\ndF1 &lt;- df(x, 1, 1)\ndF10 &lt;- df(x, 10, 10)\ndF20 &lt;- df(x, 20, 20)\ndF50 &lt;- df(x, 50, 50)\n\n\nplot(x, dF1, type = 'l', col = 2, \n     main = 'Density function of X ~ F(n, p)', ylab = 'f(x)')\nlines(x, dF10, col = 'darkorange')\nlines(x, dF20, col = 'gold')\nlines(x, dF50, col = 'green')\nlegend(\"topright\", lty = 1,\n       legend = c(\"F(n = p = 1)\", \"F(n = p = 10)\", \"F(n = p = 20)\", \"F(n = p = 50)\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\n\nplot(x, pf(x, 50, 50), type = 'l', col = 'green',\n     main = 'Cumulative Distribution Function of X ~ F(n, p)', ylab = 'F(x)')\nlines(x, pf(x, 20, 20), col='gold')\nlines(x, pf(x, 10, 10), col='darkorange')\nlines(x, pf(x, 1, 1), col=2)\nlegend(\"bottomright\", lty = 1,\n       legend = c(\"F(n = p = 1)\", \"F(n = p = 10)\", \"F(n = p = 20)\", \"F(n = p = 50)\"),\n       col = c(2, 'darkorange', 'gold', 'green'))\n\n\n\n\n\n\n\n\nYou will mostly encounter F-tests in association with the goodness of fit of a regression analysis. It is used to test whether a linear model better fits the data than a ‘intercept only’ model, i.e. one that contains no independent variables, i.e. whether your model provides any useful information for prediction.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Usual Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "044_distributions_discrete.html",
    "href": "044_distributions_discrete.html",
    "title": "17  Discrete functions",
    "section": "",
    "text": "17.1 Discrete uniform function\n(to be done)",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Discrete functions</span>"
    ]
  },
  {
    "objectID": "044_distributions_discrete.html#discrete-binomial-function",
    "href": "044_distributions_discrete.html#discrete-binomial-function",
    "title": "17  Discrete functions",
    "section": "17.2 Discrete binomial function",
    "text": "17.2 Discrete binomial function\nThrowing a coin holds two possible values and generates a so called Bernoulli random variable. We can represent the outcome by a 0 or 1. Let’s suppose 1 (whichever side you choose) is what we consider a success. This is the usual convention.\nA binomial distribution is the sum of independent and identically distributed Bernoulli random variables.\nWhen there are \\(n\\) Bernoulli trials (all independent), then the sum of those trials, each with the same probability of success \\(p\\)) is binomially distributed, with parameters \\(n\\) and \\(p\\). It is defined with the two parameters: \\(B(n, p)\\) with parameter \\(p \\in [0; 1]\\), the probability that the event of interest (success) happens\nA Bernoulli random variable \\(X = \\{0 ;1\\}\\) can thus be expressed as a binomial distribution with a single toss \\(n=1\\), \\(X \\sim B(1, p)\\)\n    X        1         0\n-------- --------- ---------\n$P(X=i)$    $p$    $q = 1-p$\nThe probability that \\(X\\) takes any other value is null (unless the coin stays on its edge…)\nIn a binomial distribution, the expectation is \\(E(X) = np\\) and the variance is \\(Var(X) = npq\\).\nThe probability that one obtains \\(k\\) successes over \\(n\\) repetitions (experiences), is then given by \\(P(X=k) = (n|k) p^k q^{n-k}\\).\nFor an intuition we can simulate 3 trials and produce a tree of probabilities. Where we see that the probability of 3 successes is one of 8 possible outcomes,\n     1st      2nd     3rd         Total\n     result   result  result\n     \n                     --1--        3 successes\n              --1---|\n             |       --0--        2 successes\n     --1-----|\n    |        |       --1--        2 successes\n    |         --0---|\n    |                --0--        1 success\n----|\n    |                --1--        2 successes\n    |         --1---|\n    |        |       --0--        1 success\n     --0-----|\n             |       --1--        1 success\n              --0---|\n                     --0--        0 success\nwhich in R is:\n\ndbinom(3, size=3, prob=0.5)\n#&gt; [1] 0.125\n\nOther example: suppose we run a trial 10 times, knowing the probability of success is 0.5, what is the probability that the total number of successes will be exactly 3? or exactly 5 ? or 10?\n\ndbinom(3, size=10, prob=0.5)\n#&gt; [1] 0.1171875\ndbinom(5, size=10, prob=0.5)\n#&gt; [1] 0.2460938\ndbinom(10, size=10, prob=0.5)\n#&gt; [1] 0.0009765625\n\nLet’s generalize a little for values from 1 to 100 and different probabilities:\n\nx &lt;- 1:100\ndBer1 &lt;- dbinom(x, size=100, prob=0.1)\ndBer2 &lt;- dbinom(x, size=100, prob=0.25)\ndBer3 &lt;- dbinom(x, size=100, prob=0.5)\ndBer4 &lt;- dbinom(x, size=100, prob=0.75)\ndBer5 &lt;- dbinom(x, size=100, prob=0.9)\n\nWith 100 trials, you can see the density ‘curve’ is symmetrical but shifted towards higher total success when the probability of success gets higher, and is flatter when closer to 0.5, (and symmetrical behaviour for cases for \\(p\\) and \\(q=1-p\\))\n\nplot(x, dBer1,main = 'Distribution of B(100, p)', ylab = 'f(x)', type=\"h\", col = 'darkgreen')\npoints(x, dBer2, col = 'lightgreen', type=\"h\")\npoints(x, dBer3, col = 'gold', type=\"h\")\npoints(x, dBer4, col = 'orange', type=\"h\")\npoints(x, dBer5, col = 'darkred', type=\"h\")\nlegend(\"top\",pch=1,\n       legend = c(\"p = 0.10\", \"p = 0.25\", \"p = 0.50\",\"p = 0.75\", \"p = 0.90\"),\n       col = c(\"darkgreen\",\"lightgreen\", 'gold', 'orange',\"darkred\"))\n\n\n\n\n\n\n\n\nBack to using \\(p=0.5\\), we now look at the effect of sample size to see how from right-skewed it shifts to a normal distribution:\n\ndn1 &lt;- dbinom(x, size=5, prob=0.5)\ndn2 &lt;- dbinom(x, size=10, prob=0.5)\ndn3 &lt;- dbinom(x, size=50, prob=0.5)\ndn4 &lt;- dbinom(x, size=100, prob=0.5)\n\n\nplot(x, dn1, col=\"lightblue\", type=\"h\",\n     main = 'Distribution of B(n, 0.5)', ylab = 'f(x)')\npoints(x, dn2, col = 'blue', type=\"h\")\npoints(x, dn3, col = 'blue4', type=\"h\")\npoints(x, dn4, col = 'purple',  type=\"h\")\nlegend(\"top\", pch = 1,\n       legend = c(\"n = 5\", \"n = 10\", \"n = 50\", \"n=100\"),\n       col = c(\"lightblue\", 'blue',\"blue4\", 'purple'))\n\n\n\n\n\n\n\n\nWhile we have looked at the distribution of the probabilities for a total outcome after repetitions to be of a given “exact” value using the density function dbinom(), we can use the cumulative form pbinom() to know where the total outcome is smaller or equal to a given value. For example, after 100 experiments with probability 0.5, what is the probability I obtain less than 50 successes? Or \\(n/2\\) successes depending on \\(n\\) ?\n\npbinom(50, size=100, prob=0.5)\n#&gt; [1] 0.5397946\npbinom(5, size=10, prob=0.5)\n#&gt; [1] 0.6230469\npbinom(5000000, size=10000000, prob=0.5)\n#&gt; [1] 0.5001262\n\nThe cumulative density function can be plotted as follows for different probabilities and a given size:\n\nplot(pbinom(1:100,size=100,prob=0.10), main = 'CDF of B(100, p)', ylab = 'f(x)', type=\"s\", col = 'darkgreen')\nlines(pbinom(1:100,size=100,prob=0.25), type=\"s\", col = 'lightgreen')\nlines(pbinom(1:100,size=100,prob=0.5), type=\"s\", col = 'gold')\nlines(pbinom(1:100,size=100,prob=0.75), type=\"s\", col = 'orange')\nlines(pbinom(1:100,size=100,prob=0.9), type=\"s\", col = 'darkred')\nlegend(\"bottomright\",pch=1,\n       legend = c(\"p = 0.10\", \"p = 0.25\", \"p = 0.50\",\"p = 0.75\", \"p = 0.90\"),\n       col = c(\"darkgreen\",\"lightgreen\", 'gold', 'orange',\"darkred\"))\n\n\n\n\n\n\n\n\nThe binomial distribution is used in many cases where there are two potential outcomes that are mutually exclusive. For example when we try to explain why some plots of land are developed or not or why people use the car rather than an active mode of transport.",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Discrete functions</span>"
    ]
  },
  {
    "objectID": "044_distributions_discrete.html#discrete-poisson-function",
    "href": "044_distributions_discrete.html#discrete-poisson-function",
    "title": "17  Discrete functions",
    "section": "17.3 Discrete Poisson function",
    "text": "17.3 Discrete Poisson function\nThe Poisson distribution is for rare discrete occurrence events. It is used when counting the occurrence of a certain event that appears randomly but at a known rate or density. The main statistical property of the Poisson distribution is that its variance equals its mean\nThere are many uses in geography, transport or planning, such as the counting of cars passing a rural road segment over a certain time, or the distribution of points (trees, bees, houses…) over a homogeneous set of spatial polygons (grid cells)\nSuppose there are 100 houses or trees over 100 grid cells. The overall density (\\(\\lambda\\)) is 1. What is the probability that a cell does not receive any single house? Or in other words what will be the proportion of cells without a single house or tree?\n\nppois(lambda = 1,q=0)\n#&gt; [1] 0.3678794\n\nHow does this change when the overall density is even higher or lower? i.e. with 150 houses/trees or only 25?\n\nppois(lambda = 1.5,q=0)\n#&gt; [1] 0.2231302\nppois(lambda = 0.5,q=0)\n#&gt; [1] 0.6065307\nppois(lambda = 0.25,q=0)\n#&gt; [1] 0.7788008\n\nLet’s generate such cases and get the frequency of counts to “see” those occurrences:\n\npois025&lt;-rpois(100,0.25)\npois025\n#&gt;   [1] 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 2 1 0 0 0 0 0 0 0 0 0 1 0 0\n#&gt;  [38] 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 2 0 1 0 0 0 0 1 1\n#&gt;  [75] 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\ntable(pois025)\n#&gt; pois025\n#&gt;  0  1  2 \n#&gt; 71 27  2\npois150&lt;-rpois(100,1.5)\npois150\n#&gt;   [1] 0 3 2 0 2 6 5 1 0 2 1 2 1 2 1 1 2 0 3 4 1 1 0 2 0 2 1 2 4 0 0 2 0 2 1 4 3\n#&gt;  [38] 3 0 2 3 1 0 0 2 2 0 2 0 2 3 2 1 2 1 3 2 2 2 1 1 0 1 0 1 4 2 1 0 0 3 1 1 1\n#&gt;  [75] 3 1 2 4 4 0 1 1 2 3 2 0 2 1 0 1 2 1 1 1 3 3 0 1 0 1\ntable(pois150)\n#&gt; pois150\n#&gt;  0  1  2  3  4  5  6 \n#&gt; 23 30 27 12  6  1  1\n\nWe examine how the probability of different counts (not just 0 or more) changes when lambda changes:\n\nlambdan&lt;-data.frame(n=rep(1:4,4),lambda=rep(seq(1,0.25,by=-0.25),each=4))\nlambdan$d&lt;-dpois(lambdan$n,lambdan$lambda)\nlambdan\n#&gt;    n lambda            d\n#&gt; 1  1   1.00 0.3678794412\n#&gt; 2  2   1.00 0.1839397206\n#&gt; 3  3   1.00 0.0613132402\n#&gt; 4  4   1.00 0.0153283100\n#&gt; 5  1   0.75 0.3542749146\n#&gt; 6  2   0.75 0.1328530930\n#&gt; 7  3   0.75 0.0332132732\n#&gt; 8  4   0.75 0.0062274887\n#&gt; 9  1   0.50 0.3032653299\n#&gt; 10 2   0.50 0.0758163325\n#&gt; 11 3   0.50 0.0126360554\n#&gt; 12 4   0.50 0.0015795069\n#&gt; 13 1   0.25 0.1947001958\n#&gt; 14 2   0.25 0.0243375245\n#&gt; 15 3   0.25 0.0020281270\n#&gt; 16 4   0.25 0.0001267579\n\nInteresingly, the distribution depends on the segments of observations or, in space, the rsolution of the grid, i.e. the modifiable areal unit problem (MAUP)\nConsider the following:\nLet’s define a grid of 100 (10x10) cells over a mixed forest. Suppose a poisson process with mean and variance = 1 gives the number of coniferous trees within that forest. We can expect around 37 % of cells to have at least a coniferous, right? (see above).\nNow suppose the mean and variance increase to 2, we are still in a poisson process because the number of events is still quite rare even there are less empty cells.\n\nppois(lambda=1, q=0)\n#&gt; [1] 0.3678794\nppois(lambda=2, q=0)\n#&gt; [1] 0.1353353\n\nBut if we now groups cells to make them larger, say divide the space into 25 cells (5 x 5) rather than 100. Then you see that lambda is multiplied by 4 and the probability of a zero count:\n\nppois(lambda=2*4, q=0)\n#&gt; [1] 0.0003354626\n\nAgain, following the the Central Limit Theorem, the higher will be the mean (λ) and thus the spatial aggregation, the closer the distribution of coniferous will be to a normal distribution.\n\nr2_100&lt;-rpois(lambda=2, n=100)\nr2_100\n#&gt;   [1] 2 2 1 1 3 2 0 3 2 0 2 5 1 3 3 0 1 4 7 2 1 3 4 0 3 2 3 1 3 4 1 1 3 0 1 1 2\n#&gt;  [38] 2 0 2 1 1 1 1 2 7 3 4 3 0 2 1 2 0 0 6 1 1 0 2 2 1 1 2 3 4 3 3 1 3 0 5 0 3\n#&gt;  [75] 2 3 1 3 3 4 3 1 3 2 2 0 2 4 2 2 1 2 6 3 1 2 5 6 3 2\npar(mfrow = c(1, 2))\nimage(matrix(r2_100, 10),asp=1)\nimage(matrix(r2_100, 10)&gt;1, asp=1)\n\n\n\n\n\n\n\n\nRe-aggregated:\n\nlarge&lt;-matrix(rep(1:5,each=20)*10+rep(rep(1:5, each=2),10),10)\nlarge\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]   11   11   21   21   31   31   41   41   51    51\n#&gt;  [2,]   11   11   21   21   31   31   41   41   51    51\n#&gt;  [3,]   12   12   22   22   32   32   42   42   52    52\n#&gt;  [4,]   12   12   22   22   32   32   42   42   52    52\n#&gt;  [5,]   13   13   23   23   33   33   43   43   53    53\n#&gt;  [6,]   13   13   23   23   33   33   43   43   53    53\n#&gt;  [7,]   14   14   24   24   34   34   44   44   54    54\n#&gt;  [8,]   14   14   24   24   34   34   44   44   54    54\n#&gt;  [9,]   15   15   25   25   35   35   45   45   55    55\n#&gt; [10,]   15   15   25   25   35   35   45   45   55    55\n\nsumbylarge&lt;-aggregate(r2_100, by=list(matrix(large)), FUN=sum)\n\nmatrix(sumbylarge$x,5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]   11    6    5    8    7\n#&gt; [2,]    6    7    4    6   14\n#&gt; [3,]    8    7   15   12    5\n#&gt; [4,]    8    8    9   10   17\n#&gt; [5,]   11    9    5   11    9\n\npar(mfrow = c(1, 2))\nimage(matrix(sumbylarge$x,5))\nimage(matrix(sumbylarge$x,5)&gt;1)",
    "crumbs": [
      "Part IV - Distributions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Discrete functions</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html",
    "href": "050_graphics_ggplot.html",
    "title": "18  ggplot graphics",
    "section": "",
    "text": "18.1 Library\nHere, we simply load the various packages that will allow us to manipulate data tables and create figures:\nIn ggplot2, a geom (short for “geometric object”) is a layer that defines how data points are visually represented in a plot. Each type of plot, such as points, lines, bars, etc., is created using a specific geom function. Here are some common examples:\nThese functions are added to a basic ggplot object using the + operator.\nExample of code :\nggplot() +\ngeom_…………(data, aes(x = x, y = y, fill = color)) +\nlabs(title = “title”, x = “abs”, y = “ord”) +\ntheme_bw()\nlibrary(ggplot2)\nlibrary(agridat) # oats data\nlibrary(questionr) # insee data\nlibrary(dplyr) \n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nlibrary(RColorBrewer)\nlibrary(readr) \nlibrary(viridis)\n#&gt; Loading required package: viridisLite\nlibrary(tidyr)\n\ndisplay.brewer.all()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#library",
    "href": "050_graphics_ggplot.html#library",
    "title": "18  ggplot graphics",
    "section": "",
    "text": "tidyverse is a package that contains several packages designed for data manipulation in R (stringr, dplyr, ggplot2)\nggplot2 which allows you to create complex graphics and observing data with visuals\n\n\n\ngeom_point(): for scatter plots.\ngeom_bar(): for bar charts.\ngeom_histogram(): for histograms.\ngeom_boxplot(): for box plots.",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#data",
    "href": "050_graphics_ggplot.html#data",
    "title": "18  ggplot graphics",
    "section": "18.2 Data",
    "text": "18.2 Data\nWe will present different code examples with 2 datasets to show that the methods are applicable to all datasets. Therefore, it is important to save our codes properly so that we can copy them for other data. Here are the two datasets :\nThe table below comes from the library ‘agridat’ shows the grain yield with 3 types of species. The ‘yield’ column represents the yield in quarter-pounds (lbs), and the ‘grain’ column represents the yield in pounds (lbs), (1lbs = 453 g). You can display the first lines of the table using the head() function.\n\nOat yield (grain, straw)\nNitrogen dose\nGenotype (variety)\nBlocks defined for experimentation\n\nThe file ‘rp2018’ contains, for all the municipalities in France in 2018, the following variables.\n\npop_tot (population)\netud (student)\ncadres (senior executive)\nlocataire (tenant)\n\n\n\ndata(yates.oats)\nhead(yates.oats)\n#&gt;   row col yield nitro        gen block grain straw\n#&gt; 1  16   3    80     0 GoldenRain    B1 20.00 28.00\n#&gt; 2  12   4    60     0 GoldenRain    B2 15.00 25.00\n#&gt; 3   3   3    89     0 GoldenRain    B3 22.25 40.50\n#&gt; 4  14   1   117     0 GoldenRain    B4 29.25 28.75\n#&gt; 5   8   2    64     0 GoldenRain    B5 16.00 32.00\n#&gt; 6   5   2    70     0 GoldenRain    B6 17.50 27.25\n\ndata(rp2018)\nhead(rp2018)\n#&gt; # A tibble: 6 × 62\n#&gt;   code_insee commune     code_region region code_departement departement pop_tot\n#&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 01004      Ambérieu-e… 84          Auver… 01               Ain          14204 \n#&gt; 2 01007      Ambronay    84          Auver… 01               Ain           2763 \n#&gt; 3 01014      Arbent      84          Auver… 01               Ain           3356 \n#&gt; 4 01024      Attignat    84          Auver… 01               Ain           3196 \n#&gt; 5 01025      Bâgé-Domma… 84          Auver… 01               Ain           4078.\n#&gt; 6 01027      Balan       84          Auver… 01               Ain           2513 \n#&gt; # ℹ 55 more variables: pop_cl &lt;fct&gt;, pop_0_14 &lt;dbl&gt;, pop_15_29 &lt;dbl&gt;,\n#&gt; #   pop_18_24 &lt;dbl&gt;, pop_75p &lt;dbl&gt;, pop_femmes &lt;dbl&gt;, pop_act_15p &lt;dbl&gt;,\n#&gt; #   pop_chom &lt;dbl&gt;, pop_agric &lt;dbl&gt;, pop_indep &lt;dbl&gt;, pop_cadres &lt;dbl&gt;,\n#&gt; #   pop_interm &lt;dbl&gt;, pop_empl &lt;dbl&gt;, pop_ouvr &lt;dbl&gt;, pop_scol_18_24 &lt;dbl&gt;,\n#&gt; #   pop_non_scol_15p &lt;dbl&gt;, pop_dipl_aucun &lt;dbl&gt;, pop_dipl_bepc &lt;dbl&gt;,\n#&gt; #   pop_dipl_capbep &lt;dbl&gt;, pop_dipl_bac &lt;dbl&gt;, pop_dipl_sup2 &lt;dbl&gt;,\n#&gt; #   pop_dipl_sup34 &lt;dbl&gt;, pop_dipl_sup &lt;dbl&gt;, log_rp &lt;dbl&gt;, …",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#basic-plot---ggplot",
    "href": "050_graphics_ggplot.html#basic-plot---ggplot",
    "title": "18  ggplot graphics",
    "section": "18.3 Basic plot - ggplot",
    "text": "18.3 Basic plot - ggplot\nHere, we indicate to R that we want to plot a scatterplot, so we need to specify a “x” and “y” for our graph. Here the example of the basic R plot, and the ggplot plot.\n\n\nplot(yates.oats$grain, yates.oats$straw) # plot(x, y)\n\n\n\n\n\n\n\n\nggplot() + \n  geom_point(data = yates.oats, aes(x = grain, y = straw))",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#histogram",
    "href": "050_graphics_ggplot.html#histogram",
    "title": "18  ggplot graphics",
    "section": "18.4 Histogram",
    "text": "18.4 Histogram\nUsually, when you’re interested in a variable, you look at its distribution. First, a classic basic histogram: you specify the data table in the ggplot() function, and the components of the table that will be used to create the plot in the aes() function. For example, we need to specify which column of the table will be represented on the x-axis, on the y-axis, or which column will allow us to color certain elements or modify the shape of the points, for instance. We then add successive layers to the graph using the + sign to achieve the desired rendering.\nHere, we indicate to R that we want to plot a histogram, so we don’t need to specify a “y” for our graph:\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThe problem is that the figure is quite unattractive, isn’t it? So, we will modify it with options: fill for the bar fill, color for the border, and alpha for transparency. The theme allows us to change the overall appearance of the figure. Here you can see all theme : https://ggplot2.tidyverse.org/reference/ggtheme.html\nBelow, geom_histogram() is used to plot a histogram.\n\nWe can also modify the number of classes represented by the histogram with the bins option.\nTo modify the size of the classes represented by the histogram, we use the binwidth option.\n\nTry both codes below:\n\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_histogram(fill = \"lightblue\",\n                 color = \"black\",\n                 bins = 10, # bar numbers (number of classes)\n                 alpha = 0.5) + \n  labs(title = \"Grain yield\", \n       x = \"Grain (lbs)\", \n       y = \"Count\") + \n  theme_bw() \n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud)) +\n  geom_histogram(fill = \"red\",\n                 color = \"black\",\n                 binwidth = 5, # reduce or increase the width of the bar (number of classes)\n                 alpha = 0.5) + \n  labs(title = \"Student proportion in France per municipalities\", \n       x = \"Student (%)\", \n       y = \"Count\") + \n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#density",
    "href": "050_graphics_ggplot.html#density",
    "title": "18  ggplot graphics",
    "section": "18.5 Density",
    "text": "18.5 Density\ngeom_density() is used to plot the density curve.\n\n\nggplot(yates.oats, aes(x = grain)) +\n  geom_density(fill = \"lightblue\",\n                 color = \"black\",\n                 alpha = 0.5) +\n  labs(title = \"Grain yield\", \n       x = \"lbs\", \n       y = \"Count\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud)) +\n  geom_density(fill = \"red\",\n                 color = \"black\",\n                 alpha = 0.5) + \n  labs(title = \"Student proportion in France per municipalities\", \n       x = \"Student (%)\", \n       y = \"Count\") + \n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#qqplot",
    "href": "050_graphics_ggplot.html#qqplot",
    "title": "18  ggplot graphics",
    "section": "18.6 QQplot",
    "text": "18.6 QQplot\nA QQ plot indicates a normal distribution when the points closely follow a straight line, suggesting that the quantiles of the data match the quantiles of a normal distribution. Here is a good QQ-plot reference : https://www.tjmahr.com/quantile-quantile-plots-from-scratch/\n\n\nggplot(yates.oats, aes(sample = grain)) +\n  geom_qq(color = \"blue\") +\n  labs(x = \"yield (lbs)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(sample = etud)) +\n  geom_qq(color = \"red\") +\n  labs(x = \"Student (%)\") +\n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#boxplot",
    "href": "050_graphics_ggplot.html#boxplot",
    "title": "18  ggplot graphics",
    "section": "18.7 Boxplot",
    "text": "18.7 Boxplot\nLet’s look at the yields in the form of boxplots for each variety. To do this, we use the geom_boxplot() function. Please note that the color of boxplots and barplots is managed not with color but with fill (points and lines are managed with ‘color’). Other palettes are available at the following address: https://bookdown.org/rdpeng/exdata/plotting-and-color-in-r.html\n\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") + \n  scale_fill_brewer(palette = \"Set1\") + # color\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = rp2018, aes(x = code_region, y = etud, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Proportion of cadre per region\", x = \"Region\", y = \"Student(%)\", fill = \"Region\") + \n  scale_fill_brewer(palette = \"Set1\") + # color\n  theme_bw() \n#&gt; Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9\n#&gt; Returning the palette you asked for with that many colors\n\n\n\n\n\n\n\n\nThe colorRampPalette() function in manner similar to colorRamp((), however the function that it returns gives you a fixed number of colors that interpolate the palette. Again we have a function pal() that was returned by colorRampPalette(), this time interpolating a palette containing the colors red and yellow. But now, the pal() function takes an integer argument specifing the number of interpolated colors to return.\nThe reference for the packages legocolors : https://cran.r-project.org/web/packages/legocolors/readme/README.html\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") + \n  scale_fill_manual(values = c(\"black\", \"red\", \"yellow\"),\n                    limits = c(\"GoldenRain\", \"Marvellous\", \"Victory\")) + # color\n  theme_bw()\n\n\n\n\n\n\n\n\npal1 &lt;- colorRampPalette(c(\"lightblue\", \"purple\", \"red\", \"green\"))\npal2 &lt;- colorRampPalette(brewer.pal(9, \"Set1\")) # 9 to use all colors\n\nlibrary(legocolors)\n#&gt; Warning: package 'legocolors' was built under R version 4.4.1\npal3 &lt;- colorRampPalette(legoCols$hex[2:13])\n\n# length(rp2018$code_region)\n# length(unique(rp2018$code_region))\n\nggplot(data = rp2018, aes(x = code_region, y = etud, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Proportion of cadre per region\", x = \"Region\", y = \"Student(%)\", fill = \"Region\") + \n  scale_fill_manual(values = pal3(17)) + # you can change the pal() here (see above)\n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#boxplot-with-point",
    "href": "050_graphics_ggplot.html#boxplot-with-point",
    "title": "18  ggplot graphics",
    "section": "18.8 Boxplot with point",
    "text": "18.8 Boxplot with point\nTo display points on a boxplot, you can use the geom_jitter() function, which displays the points by offsetting them from each other to avoid overlap. When displaying the points, you should add the 'outlier.shape = NA' option in the geom_boxplot()function to prevent displaying the same points twice.\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_boxplot(outlier.shape = NA) + #exclude point outlier\n  geom_jitter(width = 0.1, color = \"black\") + # jitter allows to arrange the place of the point comparaing to the geom_point\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#multi-boxplot",
    "href": "050_graphics_ggplot.html#multi-boxplot",
    "title": "18  ggplot graphics",
    "section": "18.9 Multi-boxplot",
    "text": "18.9 Multi-boxplot\nWe could also put the blocks in x, and color according to the varieties. Or conversely, the varieties on the x-axis and the blocks in color.\n\n\nggplot(yates.oats, aes(x = block, y = grain, fill = gen)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Blocks\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = block)) +\n  geom_boxplot() +\n  labs(title = \"Grain yield\", x = \"Blocks\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor the ‘rp2018’ data, we can create categories based on regions or departments; however, this still results in a large number. In the example below, I still made the boxplot graph but removed the legend. This is an example of data observation to see a marked trend at first glance, but it is rarely used for reports.\n\nggplot(rp2018, aes(x = code_region, y = etud, fill = code_departement)) +\n  geom_boxplot(outlier.shape = NA) + \n  labs(title = \"Department in the region\", x = \"Region\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# 44 Grand-Est, 76 Occitanie, 53 Bretagne, 28 Normandie\n\nrp2018_reg44 &lt;- rp2018[rp2018$code_region == \"44\",]\n\ng44 &lt;- ggplot(rp2018_reg44, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Grand Est\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg76 &lt;- rp2018[rp2018$code_region == \"76\",]\n\ng76 &lt;- ggplot(rp2018_reg76, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Occitanie\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg53 &lt;- rp2018[rp2018$code_region == \"53\",]\n\ng53 &lt;- ggplot(rp2018_reg53, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Bretagne\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nrp2018_reg28 &lt;- rp2018[rp2018$code_region == \"28\",]\n\ng28 &lt;- ggplot(rp2018_reg28, aes(x = code_departement, y = etud, fill = code_departement)) +\n  geom_boxplot() + \n  labs(title = \"Normandie\", x = \"Department\", y = \"Student(%)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\nlibrary(gridExtra)\n#&gt; \n#&gt; Attaching package: 'gridExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\ngrid.arrange(g44, g76, g53, g28,\n          nrow = 2, ncol = 2)",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#violinplot",
    "href": "050_graphics_ggplot.html#violinplot",
    "title": "18  ggplot graphics",
    "section": "18.10 Violinplot",
    "text": "18.10 Violinplot\nOne last one, the violin plot, which shows us the distribution curve. We use the geom_violin() function for this. On it, we could display the mean and the standard deviation, for example. For this, we use the stat_summary() function, in which we specify the function used (fun.data), the number of standard deviations represented by the error bars (fun.args), the geometry of the representation (geom), and the color, of course.\n\n\nggplot(yates.oats, aes(x = gen, y = grain, fill = gen)) +\n  geom_violin() +\n  geom_jitter(width = .1) + \n    stat_summary(fun.data = \"mean_sdl\", # mean representation + standard deviation\n               fun.args = list(mult = 1), # number of standard deviation\n               geom = \"pointrange\", # geometry \n               color = \"grey\") +\n  labs(title = \"Grain yield\", x = \"Genotype\", y = \"Yield (lbs)\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_bw()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#scatterplot",
    "href": "050_graphics_ggplot.html#scatterplot",
    "title": "18  ggplot graphics",
    "section": "18.11 Scatterplot",
    "text": "18.11 Scatterplot\nWe can also create a scatterplot (a graph with points) between straw yield and grain yield, or etud and cadres for example. The geom_point() option allows us to display points (with continuous data, for instance).\nThe scale_color_gradient() here allows us to directly detect the numerical values of the variables and produce a continuous color palette for data visualization. This allows us to integrate another variable on the points of the graph, and we can also apply it to the size to compare the x-axis and y-axis of the data to find the effect of other variables. For example, we can see that the higher the proportion of students, the more apartment rentals we observe. The scale_color_gradientn()here allows to\n\nggplot(yates.oats, aes(x = straw, y = grain, color = yield)) + # the color is set with 'color' \n  geom_point() + \n  scale_color_gradient(\"yield\", low = \"white\", high = \"blue\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nggplot(yates.oats, aes(x = straw, y = grain, color = yield)) +  \n  geom_point() + \n  scale_color_gradientn(colors = c(\"white\", \"lightblue\", \"blue\", \"black\"), \n                        limits = c(min(yates.oats$yield), max(yates.oats$yield)),\n                        values = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud, y = cadres, color = locataire, size = pop_tot)) + \n  geom_point() + \n  scale_color_gradient(\"locataire\", low = \"white\", high = \"blue\") +\n  theme_classic()",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#scatterplot-and-regression-line",
    "href": "050_graphics_ggplot.html#scatterplot-and-regression-line",
    "title": "18  ggplot graphics",
    "section": "18.12 Scatterplot and regression line",
    "text": "18.12 Scatterplot and regression line\nWe can also try to put the color of the points according to the variable (to color points, use color), and we add regression lines with the geom_smooth() for each genotype, also for the students and the cadres. The scale_color_brewer() function defines the color palette to color the points.\n\nggplot(yates.oats, aes(x = straw, y = grain, color = gen)) +\n    geom_point(shape = 1) + # ajust the size and choose the shape\n    geom_smooth(method = \"lm\", se = F) + #regression line\n    scale_fill_viridis() + # ajust a linear color with numerics values\n    theme_bw() +\n    labs(title = \"Grain vs Straw\", x = \"Straw yield (lbs)\", y = \"Grain yield (lbs)\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nggplot(rp2018, aes(x = etud, y = cadres, color = locataire)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) +\n  scale_color_gradient(\"locataire\", low = \"white\", high = \"blue\") +\n  theme_classic()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; Warning: The following aesthetics were dropped during statistical transformation:\n#&gt; colour.\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#barplot",
    "href": "050_graphics_ggplot.html#barplot",
    "title": "18  ggplot graphics",
    "section": "18.13 Barplot",
    "text": "18.13 Barplot\nHere’s a quick example to understand the proportion of forest area in each canton of Luxembourg:\nFor instance, you can create a bar chart where the x-axis represents the cantons and the y-axis represents the proportion of forest area. Using a function like geom_bar() to create a barplot.\nTo specify that we want the height of the barplot to represent the value indicated in our data frame, we need to add stat = \"identity in the geom_bar() function. Normally, geom_bar() counts the number of cases at each x position (as an histogram), but with stat = \"identity\", it uses the values in the data directly. The ‘width’ option allows you to adjust the width of the barplot.\n\nforest_area &lt;- read.csv2(\"data/statec/forest_area_canton.csv\", sep = \",\") \n\nggplot(forest_area, aes(x = GEO..Géographie, y = OBS_VALUE, fill = GEO..Géographie )) +\n  geom_bar(stat = \"identity\",\n           width = 0.5) + # width of the bar\n  theme_bw() +\n  labs(title = \"Area of the afforestation rate\", x = \"Cantons\", y = \"Afforestation rate\") +\n  scale_fill_manual(values = pal2(17)) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n\nggplot(forest_area, aes(x = GEO..Géographie, y = OBS_VALUE, fill = GEO..Géographie )) +\n  geom_bar(stat = \"identity\",\n           width = 0.5) + # width of the bar\n  theme_bw() +\n  labs(title = \"Area of the afforestation rate\", x = \"Cantons\", y = \"Afforestation rate\") +\n  scale_fill_manual(values = pal2(17)) +\n  theme(legend.position = \"none\") +\n  coord_flip() # allow to keep the real name of the canton",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#to-help-you",
    "href": "050_graphics_ggplot.html#to-help-you",
    "title": "18  ggplot graphics",
    "section": "18.14 To help you",
    "text": "18.14 To help you\ndata-to-viz\nr-graph-gallery\ncookbook-r\n\nData of the classes :\nlibrary(agridat) -&gt; yates.oats\nlibrary(questionr) -&gt; rp2018\nurl air bnb data -&gt; githubusercontent.com\nForest area by canton and commune -&gt; lustat.statec.lu",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "050_graphics_ggplot.html#your-turn",
    "href": "050_graphics_ggplot.html#your-turn",
    "title": "18  ggplot graphics",
    "section": "18.15 Your turn!",
    "text": "18.15 Your turn!\n\nGo take the data on land use proportions for some cities in the ouest of Europe : land_use_EU_ouest.rds\nCreate a few plots with a ‘ggplot2’ style.\n\nThe data comes from the urban atlas 2018 shows the proportion and the area of different classes of land use per city.\n\np_…. is the proportion in %\narea_….. is the surface in m²\npop_tot of the city\nperimeter_tot and area_tot of the city",
    "crumbs": [
      "Part V - Graphics with ggplot",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ggplot graphics</span>"
    ]
  },
  {
    "objectID": "061_inference.html",
    "href": "061_inference.html",
    "title": "19  Statistical Inference",
    "section": "",
    "text": "19.1 Point Estimates",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "061_inference.html#point-estimates",
    "href": "061_inference.html#point-estimates",
    "title": "19  Statistical Inference",
    "section": "",
    "text": "19.1.1 Point Estimate\nIn order to estimate the population mean \\(\\bar{X}\\) (or variance, median or any other metric) based on a sample, we use to take the sample mean: \\[\\bar{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\]\n\\(\\bar{x}\\) is called a point estimate or punctual estimation of the population mean.\n\n\n19.1.2 Problem\nIf we take another sample from that same population and compute the new sample mean, we are likely to obtain a different point estimate. This difference is called sampling variation.\nIn other words, an estimate is close to the real value, without being exactly equal to the real (population) parameter value. Also, the sampling variation is supposed to decrease with the size of the sample. A point estimate tends to the population parameter value as the sample size increases and gets closer to the entire population size.\n\n\n19.1.3 Illustration\nWe suppose that the dataset \\(census\\) contains our target population. Our aim is to estimate the population mean for the variable \\(age\\) and the proportion of the different genders in the case of variable \\(sex\\).\nWe look at the running mean of the variable \\(age\\) and at the running proportion of men for the variable \\(sex\\).\nWe start by extracting from the total population a sample made of one observation: \\(n = 1\\). At each step, we extract randomly a new sample with one additional observation. We use a sampling with replacement, i.e. the previous sample is not simply augmented: we take a new sample (made of completely new individuals) each time. We stop when we reach the total population, i.e. when \\(n = 500\\) in our case.\nThe key function for sampling in R is sample(), which randomly selects a number (arg size) of records for a given vector or data frame (or, if none are specified, outputs a vector of \\(1\\) to \\(size\\) integers in a random order).\nNote also that we introduce here a first “for loop” to iterate from 1 to \\(N\\) (our total population)\n\nN &lt;- nrow(census) #500\nmu_age &lt;- mean(census$age)\nmu_men &lt;- mean(census$sex == 'Male')\n\nrMeanAge = NULL\nrPropMale = NULL\n\nset.seed(201292)\n\nfor (i in 1:N) {\n  sample &lt;- census[sample(N, i), ]\n  rMeanAge[i] &lt;- mean(sample$age)\n  rPropMale[i] &lt;- mean(sample$sex == \"Male\")\n}\nrm(sample) #we don't keep the last sample either\n\nplot(rMeanAge, type = 'l', col = 2, \n     xlab = 'Sample size', ylab = 'age', \n     main = 'Running mean')\nabline(h = mu_age, col = 'darkgray')\n\n\n\n\n\n\n\ntext(500, age + 1.5, age, col = 'darkgray')\n#&gt; Error in eval(expr, envir, enclos): object 'age' not found\n\nplot(rPropMale, type = 'l', col = 4, \n     xlab = 'Sample size', ylab = 'men', \n     main = 'Running proportion')\nabline(h = mu_men, col = 'darkgray')\n\n\n\n\n\n\n\ntext(500, men + .05, men, col = 'darkgray')\n#&gt; Error in eval(expr, envir, enclos): object 'men' not found\n\n\n\n19.1.4 Standard Error\nTo quantify the uncertainty of a point estimate, we use its standard error. The standard error of an estimate is the standard deviation of its sampling distribution. For the sample mean, it is expressed as follows:\n\\[SE_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\] \\(s\\) is used here because we usually don’t know the standard deviation of the population (\\(\\sigma_X\\)).\nThe denominator, \\(\\sqrt{n}\\) reflects how the variability of the sample mean decreases as the sample size increases. This is because the standard error measures the spread of the sample means around the population mean, not just the spread of individual data points within a single sample.\n\nn &lt;- 50\nsample_age &lt;- sample(census$age, n)\nSEage &lt;- sd(sample_age) / sqrt(n)\nSEage\n#&gt; [1] 3.440362\n\nsample_sex &lt;- sample(census$sex, n)\nSEmen &lt;- sd(sample_sex == 'Male') / sqrt(n)\nSEmen\n#&gt; [1] 0.07091242\n\n\n\n19.1.5 Sampling Distribution\nLet’s now build the sampling distribution of the sampling mean. We generate \\(K = 1000\\) samples of size \\(n = 50\\).\nFrom a sampling distribution we can compute the same basic metrics as from any other distribution, with some differences. In our case, the distribution is symmetric and centred around the real value \\(\\mu = 35.298\\) (see mu_mean above).\n\nK &lt;- 1000\n\nmeanAge = NULL\npropMen = NULL\n\nset.seed(201292)\n\nfor (i in 1:K) {\n  sample &lt;- census[sample(N, n), ]\n  meanAge[i] &lt;- mean(sample$age)\n  propMen[i] &lt;- mean(sample$sex == 'Male')\n}\nrm(sample)\n\nhist(meanAge, breaks = 25,\n     main = 'Sampling distribution of the mean age')\n\n\n\n\n\n\n\nabline(v = age, col = 2)\n#&gt; Error in eval(expr, envir, enclos): object 'age' not found\ntext(38.5, 100, age, col = 2)\n#&gt; Error in eval(expr, envir, enclos): object 'age' not found\n\nhist(propMen, breaks = 20,\n     main = 'Sampling distribution of the proportion of men')\n\n\n\n\n\n\n\nabline(v = men, col = 2)\n#&gt; Error in eval(expr, envir, enclos): object 'men' not found\ntext(men + .1, 100, men, col = 2)\n#&gt; Error in eval(expr, envir, enclos): object 'men' not found\n\nExample:\nWe have a sample of 100 trees with measures of their diameter at breast height. We are interested in the mean value and standard error. The point estimate equals 164 with a empirical variance of 333221.\nWhat is the standard error of the sample mean?\n\nn &lt;- 100\nvariance &lt;- 333221\n# compute sample standard deviation\ns &lt;- sqrt(variance)\ns\n#&gt; [1] 577.253\n# compute standard error of the sample mean\nSE_mean &lt;- s / sqrt(n)\nSE_mean\n#&gt; [1] 57.7253\n\nWhat is the standard error if the sample was made of 1000 trees?\n\nSE_mean &lt;- s / sqrt(1000)\nSE_mean\n#&gt; [1] 18.25434\n\n\n\n19.1.6 Confidence Interval Estimation\nDue to the error in point estimates, it is often more relevant to look at intervals instead. The range of values of estimates for a given parameter is called a confidence interval (CI). To build a confidence interval we need three elements:\n\na point estimate \\(\\hat{\\theta}\\) for the parameter of interest \\(\\theta\\)\nthe standard error associated \\(SE_{\\hat{\\theta}}\\)\na confidence level \\(1 - \\alpha\\)\n\nWe know the first two elements. A confidence interval use the information of uncertainty of the point estimate in order to define the range of values such that we are certain at a level \\(1 - \\alpha\\) to capture the real (population) parameter value. We call \\(1 - \\alpha\\) the confidence level. For \\(\\alpha = 5\\% = (0.05)\\), we say that “we are confident at level 95% that the interval will capture the population parameter”\nIn other words, if we generate 100 samples, and compute their confidence interval, in 95 cases, the interval will contain the parameter value.\nWhen a point estimate \\(\\hat{\\theta}\\) follows a normal distribution, its confidence interval is defined by: \\[[\\hat{\\theta} \\pm z_\\alpha SE]\\]\nWith \\(z_\\alpha SE\\) the margin of error.\nUnder a normal distribution:\n\n\\(z_{0.1} = 1.645\\): 90% in the interval\n\\(z_{0.05} = 1.96\\): 95% in the interval\n\\(z_{0.001} = 2.58\\): 99% in the interval\n\nIn the case of the age variable of the census data, the intervals for each of our \\(K\\) samples are\n\nCI95 &lt;- data.frame(low = meanAge - 1.96 * SEage, \n                   point = meanAge,\n                   high = meanAge + 1.96 * SEage)\n\nCI95[1:10, ]\n#&gt;         low point     high\n#&gt; 1  23.21689 29.96 36.70311\n#&gt; 2  34.27689 41.02 47.76311\n#&gt; 3  28.59689 35.34 42.08311\n#&gt; 4  29.53689 36.28 43.02311\n#&gt; 5  28.71689 35.46 42.20311\n#&gt; 6  27.31689 34.06 40.80311\n#&gt; 7  32.31689 39.06 45.80311\n#&gt; 8  24.65689 31.40 38.14311\n#&gt; 9  29.83689 36.58 43.32311\n#&gt; 10 31.05689 37.80 44.54311\n\n\nplot(CI95[1:10, 'low'], col = 'darkgray', lwd = .75, \n     type = 'l', ylim = c(23,45), \n     xlab = 'sample', ylab = 'mean age',\n     main = 'Confidence Intervals at level 95%')\nlines(CI95[1:10, 'point'], col = 4, type = 'b')\nlines(CI95[1:10, 'high'], col = 'darkgray', lwd = .75)\n\n\n\n\nThe population mean in red = 35.298 ; the first 10 point estimates in dark blue ; their corresponding confidence intervals at level 95% in gray\n\n\n\nabline(h = age, col = 2, lwd = 2)\n#&gt; Error in eval(expr, envir, enclos): object 'age' not found\n\nWhile for most cases, the interval is within the 95% confidence around \\(\\mu = 35.298\\), the mean from samples 2 and 7 are a little too high.",
    "crumbs": [
      "Part VI - Statistical inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html",
    "href": "901_copy_paste.html",
    "title": "20  Copy-pasting",
    "section": "",
    "text": "20.1 clipr way\nA simple way to access to input some small data using the clipboard and that should work across all platforms is to use the read_clip() function from the clipr package.\nSuppose you have a series of numbers copied from a series like this one: 11 12 13 14 15 16 or this one: 11, 12, 13, 14, 15, 16\nget to the console and type clipr::read_clip()\nIn this case you will notice the whole set is a single character string entry, which then necessitates a split. See\na&lt;-clipr::read_clip()\na\n#&gt; [1] \"11, 12, 13, 14, 15, 16\"\nstrsplit(a,\", \")\n#&gt;[[1]]\n#&gt;[1] \"11\" \"12\" \"13\" \"14\" \"15\" \"16\"\nHowever, when it comes from a spreadsheet (e.g. open office)\nyou’ll get separate character entries directly:\nclipr::read_clip()\n#&gt; [1] \"0.7226332924\" \"0.5949296139\" \"0.0513909524\"\n#&gt; [4] \"0.2215940265\" \"0.8725634748\" \"0.0032392712\"\n#&gt; [7] \"0.774327883\"  \"0.1773198219\" \"0.1791877889\"\n#&gt; [10] \"0.004243708\"",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "901_copy_paste.html#read.table-way",
    "href": "901_copy_paste.html#read.table-way",
    "title": "20  Copy-pasting",
    "section": "20.2 read.table way",
    "text": "20.2 read.table way\nYou can also use the read/write table approach after saying the clipboard in the source or output. The inconvenience is that the MacOSX and MS Window approach have a slightly different code:\nCopy from spreadsheet, then paste in R using\n\nb &lt;- read.table(pipe(\"pbpaste\"),header = TRUE) #on macOSX \nb &lt;- read.table(\"clipboard\",header = TRUE) #on MS Windows\n\nand similarly to write to a spreadsheet:\n\nb3&lt;-b^3\nwrite.table(b3, pipe(\"pbcopy\"),row.names = FALSE,sep = \"\\t\") #MACOSX\nwrite.table(A3, \"clipboard\",row.names = FALSE,sep = \"\\t\") #MS Windows\n\nThen paste in spreadsheet.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Copy-pasting</span>"
    ]
  },
  {
    "objectID": "902_making_data.html",
    "href": "902_making_data.html",
    "title": "21  Making data",
    "section": "",
    "text": "21.1 Scraping a wikipedia table\nExample of the Tour de France winners table from wikipedia, used in the Vectors chapter of the course: Section 6.3.\n#R script to scrape the tour de France winners table from wikipedia page:\n#https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\n#Geoffrey Caruso  Sept 18th 2024\n#\n# Script is adapted from\n# https://help.displayr.com/hc/en-us/articles/360003582875-How-to-Import-a-Wikipedia-Table-using-R\n# \n# Since there are different tables in Tour de France page and the one of interest is a sortable one,\n# I have adapted following suggestions in\n# \n# https://stackoverflow.com/questions/72380279/how-to-scrape-with-table-class-name-with-r\n# \n# I still had to look into the source of the table to find out what table class this is\n# In this case it was a \"wikitable plainrowheaders sortable'\n\nLeTour_url&lt;-\"https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners\"\nLeTour_Page&lt;-rvest::read_html(LeTour_url)\nLeTour_Table&lt;-rvest::html_node(LeTour_Page, xpath=\"//table[@class='wikitable plainrowheaders sortable']\")\nLeTour_Tibble = rvest::html_table(LeTour_Table, fill = TRUE)\nLeTour_df&lt;-as.data.frame(LeTour_Tibble)\n\nsaveRDS(LeTour_df,\"data/TourDeFrance/LeTour_df.rds\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "href": "902_making_data.html#scotland-rain-and-elevation-by-ferguson",
    "title": "21  Making data",
    "section": "21.2 Scotland rain and elevation (by Ferguson)",
    "text": "21.2 Scotland rain and elevation (by Ferguson)\n\n#Ferguson Rob,\n#linear regression in geography, CATMOG 15. https://github.com/qmrg/CATMOG/blob/Main/15-linear-regression-in-geography.pdf\n\n#Data from Table 1 and 2, p8:\n#Average precipitation and elevation across southern Scotland\n#\n#Source caption: British Rainfall (HMSO),\n#selected raingauges between national grid lines 600 and 601 km N.\n#Sites are in West-East order\n#Elevation in m above OD\n#Rainfall in mm/yr\n#DistanceE in km from W coast\n#\nRainScotland&lt;-data.frame(\n  SiteNo=1:20,\n  Elevation=c(240,430,420,470,300,150,520,460,300,410,\n              140,540,280,240,200,210,160,270,320,230),\n  Rainfall=c(1720,2320,2050,1870,1690,1250,2130,2090, 1730,2040,\n             1460,1860,1670,1580,1490,1420,900,1250,1170,1170),\n  DistanceE=c(37,43,48,49,52,59,73,75,76,77,\n              86,97,100,103,104,114,138,152,153,154)\n    )\nwrite.csv(RainScotland, \"data/Ferguson/RainScotland.csv\")",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Making data</span>"
    ]
  },
  {
    "objectID": "903_summarizing.html",
    "href": "903_summarizing.html",
    "title": "22  Summary tables",
    "section": "",
    "text": "22.1 Descriptive statistics with summarytools\nTake the RainScotland dataset as a first example and use the dfSummary() function that provides a rather comprehensive set of statistics for entire dataframes:\nRainScotland&lt;-read.csv(\"data/Ferguson/RainScotland.csv\")\nsummarytools::dfSummary(RainScotland)\n#&gt; Data Frame Summary  \n#&gt; RainScotland  \n#&gt; Dimensions: 20 x 5  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------\n#&gt; No   Variable    Stats / Values              Freqs (% of Valid)   Graph               Valid      Missing  \n#&gt; ---- ----------- --------------------------- -------------------- ------------------- ---------- ---------\n#&gt; 1    X           Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 2    SiteNo      Mean (sd) : 10.5 (5.9)      20 distinct values   : : : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:            (Integer sequence)   : : : :             (100.0%)   (0.0%)   \n#&gt;                  1 &lt; 10.5 &lt; 20                                    : : : :                                 \n#&gt;                  IQR (CV) : 9.5 (0.6)                             : : : :                                 \n#&gt;                                                                   : : : :                                 \n#&gt; \n#&gt; 3    Elevation   Mean (sd) : 314.5 (125.5)   18 distinct values       : :             20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                     : :     :       (100.0%)   (0.0%)   \n#&gt;                  140 &lt; 290 &lt; 540                                  . . : :     : . .                       \n#&gt;                  IQR (CV) : 197.5 (0.4)                           : : : :     : : :                       \n#&gt;                                                                   : : : : :   : : :                       \n#&gt; \n#&gt; 4    Rainfall    Mean (sd) : 1643 (380.6)    18 distinct values         : :   :       20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                       : :   :       (100.0%)   (0.0%)   \n#&gt;                  900 &lt; 1680 &lt; 2320                                  . . : : . :                           \n#&gt;                  IQR (CV) : 535 (0.2)                               : : : : : :                           \n#&gt;                                                                   : : : : : : : :                         \n#&gt; \n#&gt; 5    DistanceE   Mean (sd) : 89.5 (37.7)     20 distinct values     :                 20         0        \n#&gt;      [integer]   min &lt; med &lt; max:                                   : :               (100.0%)   (0.0%)   \n#&gt;                  37 &lt; 81.5 &lt; 154                                    : : : :   :                           \n#&gt;                  IQR (CV) : 49.2 (0.4)                              : : : :   :                           \n#&gt;                                                                   : : : : : : :                           \n#&gt; ----------------------------------------------------------------------------------------------------------\nAn nice feature is that it can generate its own view (beware: view not View(), as an html file for display in any browser:\nsummarytools::view(summarytools::dfSummary(RainScotland), file = \"output/RainScotland.html\")\nIf we would a more complex dataset (including factors) such as the wikipedia table of the Tour de France winners, we would have:\nTDF&lt;-readRDS(\"data/TourDeFrance/LeTour_df.rds\")\nsummarytools::dfSummary(TDF)\n#&gt; Data Frame Summary  \n#&gt; TDF  \n#&gt; Dimensions: 122 x 8  \n#&gt; Duplicates: 0  \n#&gt; \n#&gt; ----------------------------------------------------------------------------------------------------------------\n#&gt; No   Variable       Stats / Values                 Freqs (% of Valid)    Graph              Valid      Missing  \n#&gt; ---- -------------- ------------------------------ --------------------- ------------------ ---------- ---------\n#&gt; 1    Year           Mean (sd) : 1963.5 (35.4)      122 distinct values   . : : : : :        122        0        \n#&gt;      [integer]      min &lt; med &lt; max:               (Integer sequence)    : : : : : :        (100.0%)   (0.0%)   \n#&gt;                     1903 &lt; 1963.5 &lt; 2024                                 : : : : : :                            \n#&gt;                     IQR (CV) : 60.5 (0)                                  : : : : : :                            \n#&gt;                                                                          : : : : : : :                          \n#&gt; \n#&gt; 2    Country        1. France                      36 (29.5%)            IIIII              122        0        \n#&gt;      [character]    2. —                           18 (14.8%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Belgium                     18 (14.8%)            II                                     \n#&gt;                     4. Spain                       12 ( 9.8%)            I                                      \n#&gt;                     5. Italy                       10 ( 8.2%)            I                                      \n#&gt;                     6. Great Britain                6 ( 4.9%)                                                   \n#&gt;                     7. Luxembourg                   5 ( 4.1%)                                                   \n#&gt;                     8. Denmark                      3 ( 2.5%)                                                   \n#&gt;                     9. Slovenia                     3 ( 2.5%)                                                   \n#&gt;                     10. United States               3 ( 2.5%)                                                   \n#&gt;                     [ 6 others ]                    8 ( 6.6%)            I                                      \n#&gt; \n#&gt; 3    Cyclist        1. ~Not contested due to Wor    7 ( 5.7%)            I                  122        0        \n#&gt;      [character]    2. No winner[c]                 7 ( 5.7%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. Jacques Anquetil             5 ( 4.1%)                                                   \n#&gt;                     4. Miguel Indurain              5 ( 4.1%)                                                   \n#&gt;                     5. ~Not contested due to Wor    4 ( 3.3%)                                                   \n#&gt;                     6. Bernard Hinault              4 ( 3.3%)                                                   \n#&gt;                     7. Chris Froome                 3 ( 2.5%)                                                   \n#&gt;                     8. Greg LeMond                  3 ( 2.5%)                                                   \n#&gt;                     9. Louison Bobet                3 ( 2.5%)                                                   \n#&gt;                     10. Philippe Thys               3 ( 2.5%)                                                   \n#&gt;                     [ 67 others ]                  78 (63.9%)            IIIIIIIIIIII                           \n#&gt; \n#&gt; 4    Sponsor/Team   1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. France                      13 (10.7%)            II                 (100.0%)   (0.0%)   \n#&gt;                     3. Alcyon–Dunlop                7 ( 5.7%)            I                                      \n#&gt;                     4. Peugeot–Wolber               7 ( 5.7%)            I                                      \n#&gt;                     5. Team Sky                     6 ( 4.9%)                                                   \n#&gt;                     6. Banesto                      5 ( 4.1%)                                                   \n#&gt;                     7. Italy                        5 ( 4.1%)                                                   \n#&gt;                     8. Automoto–Hutchinson          3 ( 2.5%)                                                   \n#&gt;                     9. Belgium                      3 ( 2.5%)                                                   \n#&gt;                     10. La Sportive                 3 ( 2.5%)                                                   \n#&gt;                     [ 40 others ]                  52 (42.6%)            IIIIIIII                               \n#&gt; \n#&gt; 5    Distance       1. —                           11 ( 9.0%)            I                  122        0        \n#&gt;      [character]    2. 2,428 km (1,509 mi)          2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 3,765 km (2,339 mi)          2 ( 1.6%)                                                   \n#&gt;                     4. 4,498 km (2,795 mi)          2 ( 1.6%)                                                   \n#&gt;                     5. 2,994 km (1,860 mi)          1 ( 0.8%)                                                   \n#&gt;                     6. 3,278 km (2,037 mi)          1 ( 0.8%)                                                   \n#&gt;                     7. 3,285 km (2,041 mi)          1 ( 0.8%)                                                   \n#&gt;                     8. 3,286 km (2,042 mi)          1 ( 0.8%)                                                   \n#&gt;                     9. 3,328 km (2,068 mi)          1 ( 0.8%)                                                   \n#&gt;                     10. 3,349 km (2,081 mi)         1 ( 0.8%)                                                   \n#&gt;                     [ 99 others ]                  99 (81.1%)            IIIIIIIIIIIIIIII                       \n#&gt; \n#&gt; 6    Time/Points    1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 100h 30′ 35″                 1 ( 0.8%)                               (100.0%)   (0.0%)   \n#&gt;                     3. 100h 49′ 30″                 1 ( 0.8%)                                                   \n#&gt;                     4. 101h 01′ 20″                 1 ( 0.8%)                                                   \n#&gt;                     5. 103h 06′ 50″                 1 ( 0.8%)                                                   \n#&gt;                     6. 103h 38′ 38″                 1 ( 0.8%)                                                   \n#&gt;                     7. 105h 07′ 52″                 1 ( 0.8%)                                                   \n#&gt;                     8. 108h 17′ 18″                 1 ( 0.8%)                                                   \n#&gt;                     9. 108h 18′ 00″                 1 ( 0.8%)                                                   \n#&gt;                     10. 109h 19′ 14″                1 ( 0.8%)                                                   \n#&gt;                     [ 95 others ]                  95 (77.9%)            IIIIIIIIIIIIIII                        \n#&gt; \n#&gt; 7    Margin         1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. + 3′ 10″                     2 ( 1.6%)                               (100.0%)   (0.0%)   \n#&gt;                     3. + 3′ 21″                     2 ( 1.6%)                                                   \n#&gt;                     4. + 4′ 01″                     2 ( 1.6%)                                                   \n#&gt;                     5. + 4′ 35″                     2 ( 1.6%)                                                   \n#&gt;                     6. + 4′ 59″                     2 ( 1.6%)                                                   \n#&gt;                     7. + 1′ 07″                     1 ( 0.8%)                                                   \n#&gt;                     8. + 1′ 11″                     1 ( 0.8%)                                                   \n#&gt;                     9. + 1′ 12″                     1 ( 0.8%)                                                   \n#&gt;                     10. + 1′ 22″                    1 ( 0.8%)                                                   \n#&gt;                     [ 90 others ]                  90 (73.8%)            IIIIIIIIIIIIII                         \n#&gt; \n#&gt; 8    Stage wins     1. —                           18 (14.8%)            II                 122        0        \n#&gt;      [character]    2. 0                            8 ( 6.6%)            I                  (100.0%)   (0.0%)   \n#&gt;                     3. 1                           20 (16.4%)            III                                    \n#&gt;                     4. 2                           27 (22.1%)            IIII                                   \n#&gt;                     5. 3                           19 (15.6%)            III                                    \n#&gt;                     6. 4                           12 ( 9.8%)            I                                      \n#&gt;                     7. 5                            8 ( 6.6%)            I                                      \n#&gt;                     8. 6                            6 ( 4.9%)                                                   \n#&gt;                     9. 7                            2 ( 1.6%)                                                   \n#&gt;                     10. 8                           2 ( 1.6%)                                                   \n#&gt; ----------------------------------------------------------------------------------------------------------------\nsummarytools::view(summarytools::dfSummary(TDF), file = \"output/LeTour.html\")\nA less visual but even more complete set (including range and indicators of the shape of the distributions) can be obtained with the descr()function.\nsummarytools::descr(RainScotland)\n#&gt; Descriptive Statistics  \n#&gt; RainScotland  \n#&gt; N: 20  \n#&gt; \n#&gt;                     DistanceE   Elevation   Rainfall   SiteNo        X\n#&gt; ----------------- ----------- ----------- ---------- -------- --------\n#&gt;              Mean       89.50      314.50    1643.00    10.50    10.50\n#&gt;           Std.Dev       37.74      125.51     380.62     5.92     5.92\n#&gt;               Min       37.00      140.00     900.00     1.00     1.00\n#&gt;                Q1       55.50      220.00    1335.00     5.50     5.50\n#&gt;            Median       81.50      290.00    1680.00    10.50    10.50\n#&gt;                Q3      109.00      425.00    1955.00    15.50    15.50\n#&gt;               Max      154.00      540.00    2320.00    20.00    20.00\n#&gt;               MAD       38.55      155.67     459.61     7.41     7.41\n#&gt;               IQR       49.25      197.50     535.00     9.50     9.50\n#&gt;                CV        0.42        0.40       0.23     0.56     0.56\n#&gt;          Skewness        0.40        0.33      -0.09     0.00     0.00\n#&gt;       SE.Skewness        0.51        0.51       0.51     0.51     0.51\n#&gt;          Kurtosis       -1.13       -1.30      -1.05    -1.38    -1.38\n#&gt;           N.Valid       20.00       20.00      20.00    20.00    20.00\n#&gt;         Pct.Valid      100.00      100.00     100.00   100.00   100.00\nIn the case we apply it to the Tour de France, only the year is available as a numeric, which is not quite interesting because there is only one Tour per year, thus providing basically no useful information.\nsummarytools::descr(TDF)\n#&gt; Non-numerical variable(s) ignored: Country, Cyclist, Sponsor/Team, Distance, Time/Points, Margin, Stage wins\n#&gt; Descriptive Statistics  \n#&gt; TDF$Year  \n#&gt; N: 122  \n#&gt; \n#&gt;                        Year\n#&gt; ----------------- ---------\n#&gt;              Mean   1963.50\n#&gt;           Std.Dev     35.36\n#&gt;               Min   1903.00\n#&gt;                Q1   1933.00\n#&gt;            Median   1963.50\n#&gt;                Q3   1994.00\n#&gt;               Max   2024.00\n#&gt;               MAD     45.22\n#&gt;               IQR     60.50\n#&gt;                CV      0.02\n#&gt;          Skewness      0.00\n#&gt;       SE.Skewness      0.22\n#&gt;          Kurtosis     -1.23\n#&gt;           N.Valid    122.00\n#&gt;         Pct.Valid    100.00\nThe specific function freq() is rather to be used here. Given most variables are categorical, we are interested in counts and percentages:\nsummarytools::freq(TDF)\n#&gt; Variable(s) ignored: Year\n#&gt; Frequencies  \n#&gt; TDF$Country  \n#&gt; Type: Character  \n#&gt; \n#&gt;                       Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------- ------ --------- -------------- --------- --------------\n#&gt;                   —     18     14.75          14.75     14.75          14.75\n#&gt;           Australia      1      0.82          15.57      0.82          15.57\n#&gt;             Belgium     18     14.75          30.33     14.75          30.33\n#&gt;            Colombia      1      0.82          31.15      0.82          31.15\n#&gt;             Denmark      3      2.46          33.61      2.46          33.61\n#&gt;              France     36     29.51          63.11     29.51          63.11\n#&gt;             Germany      1      0.82          63.93      0.82          63.93\n#&gt;       Great Britain      6      4.92          68.85      4.92          68.85\n#&gt;             Ireland      1      0.82          69.67      0.82          69.67\n#&gt;               Italy     10      8.20          77.87      8.20          77.87\n#&gt;          Luxembourg      5      4.10          81.97      4.10          81.97\n#&gt;         Netherlands      2      1.64          83.61      1.64          83.61\n#&gt;            Slovenia      3      2.46          86.07      2.46          86.07\n#&gt;               Spain     12      9.84          95.90      9.84          95.90\n#&gt;         Switzerland      2      1.64          97.54      1.64          97.54\n#&gt;       United States      3      2.46         100.00      2.46         100.00\n#&gt;                &lt;NA&gt;      0                               0.00         100.00\n#&gt;               Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Cyclist  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                            Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ---------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;        ~Not contested due to World War I      4      3.28           3.28      3.28           3.28\n#&gt;       ~Not contested due to World War II      7      5.74           9.02      5.74           9.02\n#&gt;                         Alberto Contador      1      0.82           9.84      0.82           9.84\n#&gt;                        Alberto Contador#      1      0.82          10.66      0.82          10.66\n#&gt;                             André Leducq      2      1.64          12.30      1.64          12.30\n#&gt;                         Andy Schleck#[e]      1      0.82          13.11      0.82          13.11\n#&gt;                            Antonin Magne      2      1.64          14.75      1.64          14.75\n#&gt;                          Bernard Hinault      4      3.28          18.03      3.28          18.03\n#&gt;                         Bernard Hinault†      1      0.82          18.85      0.82          18.85\n#&gt;                         Bernard Thévenet      2      1.64          20.49      1.64          20.49\n#&gt;                           Bjarne Riis[b]      1      0.82          21.31      0.82          21.31\n#&gt;                          Bradley Wiggins      1      0.82          22.13      0.82          22.13\n#&gt;                              Cadel Evans      1      0.82          22.95      0.82          22.95\n#&gt;                            Carlos Sastre      1      0.82          23.77      0.82          23.77\n#&gt;                              Charly Gaul      1      0.82          24.59      0.82          24.59\n#&gt;                             Chris Froome      3      2.46          27.05      2.46          27.05\n#&gt;                             Chris Froome      1      0.82          27.87      0.82          27.87\n#&gt;                              Eddy Merckx      1      0.82          28.69      0.82          28.69\n#&gt;                              Eddy Merckx      1      0.82          29.51      0.82          29.51\n#&gt;                             Eddy Merckx†      2      1.64          31.15      1.64          31.15\n#&gt;                             Eddy Merckx‡      1      0.82          31.97      0.82          31.97\n#&gt;                             Egan Bernal#      1      0.82          32.79      0.82          32.79\n#&gt;                             Fausto Coppi      2      1.64          34.43      1.64          34.43\n#&gt;                      Federico Bahamontes      1      0.82          35.25      0.82          35.25\n#&gt;                           Felice Gimondi      1      0.82          36.07      0.82          36.07\n#&gt;                         Ferdinand Kübler      1      0.82          36.89      0.82          36.89\n#&gt;                            Firmin Lambot      2      1.64          38.52      1.64          38.52\n#&gt;                           François Faber      1      0.82          39.34      0.82          39.34\n#&gt;                          Gastone Nencini      1      0.82          40.16      0.82          40.16\n#&gt;                         Georges Speicher      1      0.82          40.98      0.82          40.98\n#&gt;                           Geraint Thomas      1      0.82          41.80      0.82          41.80\n#&gt;                             Gino Bartali      2      1.64          43.44      1.64          43.44\n#&gt;                              Greg LeMond      3      2.46          45.90      2.46          45.90\n#&gt;                         Gustave Garrigou      1      0.82          46.72      0.82          46.72\n#&gt;                          Henri Cornet[a]      1      0.82          47.54      0.82          47.54\n#&gt;                          Henri Pélissier      1      0.82          48.36      0.82          48.36\n#&gt;                              Hugo Koblet      1      0.82          49.18      0.82          49.18\n#&gt;                         Jacques Anquetil      5      4.10          53.28      4.10          53.28\n#&gt;                              Jan Janssen      1      0.82          54.10      0.82          54.10\n#&gt;                             Jan Ullrich#      1      0.82          54.92      0.82          54.92\n#&gt;                               Jean Robic      1      0.82          55.74      0.82          55.74\n#&gt;                         Jonas Vingegaard      1      0.82          56.56      0.82          56.56\n#&gt;                         Jonas Vingegaard      1      0.82          57.38      0.82          57.38\n#&gt;                           Joop Zoetemelk      1      0.82          58.20      0.82          58.20\n#&gt;                           Laurent Fignon      1      0.82          59.02      0.82          59.02\n#&gt;                          Laurent Fignon#      1      0.82          59.84      0.82          59.84\n#&gt;                              Léon Scieur      1      0.82          60.66      0.82          60.66\n#&gt;                        Louis Trousselier      1      0.82          61.48      0.82          61.48\n#&gt;                            Louison Bobet      3      2.46          63.93      2.46          63.93\n#&gt;                             Lucien Aimar      1      0.82          64.75      0.82          64.75\n#&gt;                            Lucien Buysse      1      0.82          65.57      0.82          65.57\n#&gt;                      Lucien Petit-Breton      2      1.64          67.21      1.64          67.21\n#&gt;                          Lucien Van Impe      1      0.82          68.03      0.82          68.03\n#&gt;                               Luis Ocaña      1      0.82          68.85      0.82          68.85\n#&gt;                            Marco Pantani      1      0.82          69.67      0.82          69.67\n#&gt;                         Maurice De Waele      1      0.82          70.49      0.82          70.49\n#&gt;                            Maurice Garin      1      0.82          71.31      0.82          71.31\n#&gt;                          Miguel Indurain      5      4.10          75.41      4.10          75.41\n#&gt;                           Nicolas Frantz      2      1.64          77.05      1.64          77.05\n#&gt;                             No winner[c]      7      5.74          82.79      5.74          82.79\n#&gt;                            Octave Lapize      1      0.82          83.61      0.82          83.61\n#&gt;                            Odile Defraye      1      0.82          84.43      0.82          84.43\n#&gt;                         Óscar Pereiro[d]      1      0.82          85.25      0.82          85.25\n#&gt;                       Ottavio Bottecchia      2      1.64          86.89      1.64          86.89\n#&gt;                            Pedro Delgado      1      0.82          87.70      0.82          87.70\n#&gt;                            Philippe Thys      3      2.46          90.16      2.46          90.16\n#&gt;                             René Pottier      1      0.82          90.98      0.82          90.98\n#&gt;                            Roger Lapébie      1      0.82          91.80      0.82          91.80\n#&gt;                            Roger Pingeon      1      0.82          92.62      0.82          92.62\n#&gt;                          Roger Walkowiak      1      0.82          93.44      0.82          93.44\n#&gt;                              Romain Maes      1      0.82          94.26      0.82          94.26\n#&gt;                            Stephen Roche      1      0.82          95.08      0.82          95.08\n#&gt;                             Sylvère Maes      1      0.82          95.90      0.82          95.90\n#&gt;                             Sylvère Maes      1      0.82          96.72      0.82          96.72\n#&gt;                            Tadej Pogačar      1      0.82          97.54      0.82          97.54\n#&gt;                           Tadej Pogačar§      2      1.64          99.18      1.64          99.18\n#&gt;                          Vincenzo Nibali      1      0.82         100.00      0.82         100.00\n#&gt;                                     &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                    Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Sponsor/Team  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                           Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; --------------------------------------- ------ --------- -------------- --------- --------------\n#&gt;                                       —     18     14.75          14.75     14.75          14.75\n#&gt;             AD Renting–W-Cup–Bottecchia      1      0.82          15.57      0.82          15.57\n#&gt;                           Alcyon–Dunlop      7      5.74          21.31      5.74          21.31\n#&gt;                                  Astana      2      1.64          22.95      1.64          22.95\n#&gt;                                Automoto      1      0.82          23.77      0.82          23.77\n#&gt;                     Automoto–Hutchinson      3      2.46          26.23      2.46          26.23\n#&gt;                                 Banesto      5      4.10          30.33      4.10          30.33\n#&gt;                                 Belgium      3      2.46          32.79      2.46          32.79\n#&gt;                                     Bic      1      0.82          33.61      0.82          33.61\n#&gt;                         BMC Racing Team      1      0.82          34.43      0.82          34.43\n#&gt;          Caisse d'Epargne–Illes Balears      1      0.82          35.25      0.82          35.25\n#&gt;                  Carrera Jeans–Vagabond      1      0.82          36.07      0.82          36.07\n#&gt;                                   Conte      1      0.82          36.89      0.82          36.89\n#&gt;                       Discovery Channel      1      0.82          37.70      0.82          37.70\n#&gt;                                   Faema      1      0.82          38.52      0.82          38.52\n#&gt;                           Faemino–Faema      1      0.82          39.34      0.82          39.34\n#&gt;                  Ford France–Hutchinson      1      0.82          40.16      0.82          40.16\n#&gt;                                  France     13     10.66          50.82     10.66          50.82\n#&gt;                       Gitane–Campagnolo      1      0.82          51.64      0.82          51.64\n#&gt;                                   Italy      5      4.10          55.74      4.10          55.74\n#&gt;                            La Française      1      0.82          56.56      0.82          56.56\n#&gt;                             La Sportive      3      2.46          59.02      2.46          59.02\n#&gt;                           La Vie Claire      2      1.64          60.66      1.64          60.66\n#&gt;                              Luxembourg      1      0.82          61.48      0.82          61.48\n#&gt;                   Mercatone Uno–Bianchi      1      0.82          62.30      0.82          62.30\n#&gt;                                 Molteni      3      2.46          64.75      2.46          64.75\n#&gt;                Pelforth–Sauvage–Lejeune      1      0.82          65.57      0.82          65.57\n#&gt;                     Peugeot–BP–Michelin      2      1.64          67.21      1.64          67.21\n#&gt;                   Peugeot–Esso–Michelin      1      0.82          68.03      0.82          68.03\n#&gt;                          Peugeot–Wolber      7      5.74          73.77      5.74          73.77\n#&gt;                             Renault–Elf      2      1.64          75.41      1.64          75.41\n#&gt;                      Renault–Elf–Gitane      2      1.64          77.05      1.64          77.05\n#&gt;                          Renault–Gitane      1      0.82          77.87      0.82          77.87\n#&gt;               Renault–Gitane–Campagnolo      1      0.82          78.69      0.82          78.69\n#&gt;                                Reynolds      1      0.82          79.51      0.82          79.51\n#&gt;             Saint-Raphaël–Gitane–Dunlop      1      0.82          80.33      0.82          80.33\n#&gt;       Saint-Raphaël–Gitane–R. Geminiani      1      0.82          81.15      0.82          81.15\n#&gt;        Saint-Raphaël–Helyett–Hutchinson      1      0.82          81.97      0.82          81.97\n#&gt;                               Salvarani      1      0.82          82.79      0.82          82.79\n#&gt;                                   Spain      1      0.82          83.61      0.82          83.61\n#&gt;                             Switzerland      2      1.64          85.25      1.64          85.25\n#&gt;                                Team CSC      1      0.82          86.07      0.82          86.07\n#&gt;                              Team Ineos      1      0.82          86.89      0.82          86.89\n#&gt;                        Team Jumbo–Visma      2      1.64          88.52      1.64          88.52\n#&gt;                          Team Saxo Bank      1      0.82          89.34      0.82          89.34\n#&gt;                                Team Sky      6      4.92          94.26      4.92          94.26\n#&gt;                            Team Telekom      2      1.64          95.90      1.64          95.90\n#&gt;                        TI–Raleigh–Creda      1      0.82          96.72      0.82          96.72\n#&gt;                       UAE Team Emirates      3      2.46          99.18      2.46          99.18\n#&gt;                               Z–Tomasso      1      0.82         100.00      0.82         100.00\n#&gt;                                    &lt;NA&gt;      0                               0.00         100.00\n#&gt;                                   Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Distance  \n#&gt; Type: Character  \n#&gt; \n#&gt;                                 Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------------------------- ------ --------- -------------- --------- --------------\n#&gt;                             —     11      9.02           9.02      9.02           9.02\n#&gt;           2,428 km (1,509 mi)      2      1.64          10.66      1.64          10.66\n#&gt;           2,994 km (1,860 mi)      1      0.82          11.48      0.82          11.48\n#&gt;           3,278 km (2,037 mi)      1      0.82          12.30      0.82          12.30\n#&gt;           3,285 km (2,041 mi)      1      0.82          13.11      0.82          13.11\n#&gt;           3,286 km (2,042 mi)      1      0.82          13.93      0.82          13.93\n#&gt;           3,328 km (2,068 mi)      1      0.82          14.75      0.82          14.75\n#&gt;           3,349 km (2,081 mi)      1      0.82          15.57      0.82          15.57\n#&gt;           3,359 km (2,087 mi)      1      0.82          16.39      0.82          16.39\n#&gt;       3,360.3 km (2,088.0 mi)      1      0.82          17.21      0.82          17.21\n#&gt;           3,366 km (2,092 mi)      1      0.82          18.03      0.82          18.03\n#&gt;           3,391 km (2,107 mi)      1      0.82          18.85      0.82          18.85\n#&gt;           3,404 km (2,115 mi)      1      0.82          19.67      0.82          19.67\n#&gt;           3,406 km (2,116 mi)      1      0.82          20.49      0.82          20.49\n#&gt;       3,414.4 km (2,121.6 mi)      1      0.82          21.31      0.82          21.31\n#&gt;           3,427 km (2,129 mi)      1      0.82          22.13      0.82          22.13\n#&gt;           3,430 km (2,130 mi)      1      0.82          22.95      0.82          22.95\n#&gt;           3,458 km (2,149 mi)      1      0.82          23.77      0.82          23.77\n#&gt;           3,459 km (2,149 mi)      1      0.82          24.59      0.82          24.59\n#&gt;           3,484 km (2,165 mi)      1      0.82          25.41      0.82          25.41\n#&gt;           3,496 km (2,172 mi)      1      0.82          26.23      0.82          26.23\n#&gt;           3,498 km (2,174 mi)      1      0.82          27.05      0.82          27.05\n#&gt;           3,504 km (2,177 mi)      1      0.82          27.87      0.82          27.87\n#&gt;           3,507 km (2,179 mi)      1      0.82          28.69      0.82          28.69\n#&gt;           3,529 km (2,193 mi)      1      0.82          29.51      0.82          29.51\n#&gt;           3,540 km (2,200 mi)      1      0.82          30.33      0.82          30.33\n#&gt;           3,559 km (2,211 mi)      1      0.82          31.15      0.82          31.15\n#&gt;           3,570 km (2,220 mi)      1      0.82          31.97      0.82          31.97\n#&gt;           3,608 km (2,242 mi)      1      0.82          32.79      0.82          32.79\n#&gt;           3,635 km (2,259 mi)      1      0.82          33.61      0.82          33.61\n#&gt;           3,642 km (2,263 mi)      1      0.82          34.43      0.82          34.43\n#&gt;           3,657 km (2,272 mi)      1      0.82          35.25      0.82          35.25\n#&gt;       3,660.5 km (2,274.5 mi)      1      0.82          36.07      0.82          36.07\n#&gt;           3,662 km (2,275 mi)      1      0.82          36.89      0.82          36.89\n#&gt;           3,687 km (2,291 mi)      1      0.82          37.70      0.82          37.70\n#&gt;           3,714 km (2,308 mi)      1      0.82          38.52      0.82          38.52\n#&gt;           3,753 km (2,332 mi)      1      0.82          39.34      0.82          39.34\n#&gt;           3,765 km (2,339 mi)      2      1.64          40.98      1.64          40.98\n#&gt;           3,809 km (2,367 mi)      1      0.82          41.80      0.82          41.80\n#&gt;           3,842 km (2,387 mi)      1      0.82          42.62      0.82          42.62\n#&gt;           3,846 km (2,390 mi)      1      0.82          43.44      0.82          43.44\n#&gt;           3,875 km (2,408 mi)      1      0.82          44.26      0.82          44.26\n#&gt;           3,908 km (2,428 mi)      1      0.82          45.08      0.82          45.08\n#&gt;           3,914 km (2,432 mi)      1      0.82          45.90      0.82          45.90\n#&gt;           3,950 km (2,450 mi)      1      0.82          46.72      0.82          46.72\n#&gt;           3,978 km (2,472 mi)      1      0.82          47.54      0.82          47.54\n#&gt;           3,983 km (2,475 mi)      1      0.82          48.36      0.82          48.36\n#&gt;           4,000 km (2,500 mi)      1      0.82          49.18      0.82          49.18\n#&gt;           4,017 km (2,496 mi)      1      0.82          50.00      0.82          50.00\n#&gt;           4,021 km (2,499 mi)      1      0.82          50.82      0.82          50.82\n#&gt;           4,090 km (2,540 mi)      1      0.82          51.64      0.82          51.64\n#&gt;           4,094 km (2,544 mi)      1      0.82          52.46      0.82          52.46\n#&gt;           4,096 km (2,545 mi)      1      0.82          53.28      0.82          53.28\n#&gt;           4,098 km (2,546 mi)      1      0.82          54.10      0.82          54.10\n#&gt;           4,109 km (2,553 mi)      1      0.82          54.92      0.82          54.92\n#&gt;           4,117 km (2,558 mi)      1      0.82          55.74      0.82          55.74\n#&gt;           4,138 km (2,571 mi)      1      0.82          56.56      0.82          56.56\n#&gt;           4,173 km (2,593 mi)      1      0.82          57.38      0.82          57.38\n#&gt;           4,188 km (2,602 mi)      1      0.82          58.20      0.82          58.20\n#&gt;           4,224 km (2,625 mi)      1      0.82          59.02      0.82          59.02\n#&gt;           4,231 km (2,629 mi)      1      0.82          59.84      0.82          59.84\n#&gt;           4,254 km (2,643 mi)      1      0.82          60.66      0.82          60.66\n#&gt;           4,274 km (2,656 mi)      1      0.82          61.48      0.82          61.48\n#&gt;           4,319 km (2,684 mi)      1      0.82          62.30      0.82          62.30\n#&gt;           4,329 km (2,690 mi)      1      0.82          63.11      0.82          63.11\n#&gt;           4,338 km (2,696 mi)      1      0.82          63.93      0.82          63.93\n#&gt;           4,358 km (2,708 mi)      1      0.82          64.75      0.82          64.75\n#&gt;           4,395 km (2,731 mi)      1      0.82          65.57      0.82          65.57\n#&gt;           4,397 km (2,732 mi)      1      0.82          66.39      0.82          66.39\n#&gt;           4,415 km (2,743 mi)      1      0.82          67.21      0.82          67.21\n#&gt;           4,442 km (2,760 mi)      1      0.82          68.03      0.82          68.03\n#&gt;           4,470 km (2,780 mi)      1      0.82          68.85      0.82          68.85\n#&gt;           4,476 km (2,781 mi)      1      0.82          69.67      0.82          69.67\n#&gt;           4,479 km (2,783 mi)      1      0.82          70.49      0.82          70.49\n#&gt;           4,488 km (2,789 mi)      1      0.82          71.31      0.82          71.31\n#&gt;           4,492 km (2,791 mi)      1      0.82          72.13      0.82          72.13\n#&gt;           4,495 km (2,793 mi)      1      0.82          72.95      0.82          72.95\n#&gt;           4,497 km (2,794 mi)      1      0.82          73.77      0.82          73.77\n#&gt;           4,498 km (2,795 mi)      2      1.64          75.41      1.64          75.41\n#&gt;           4,504 km (2,799 mi)      1      0.82          76.23      0.82          76.23\n#&gt;           4,637 km (2,881 mi)      1      0.82          77.05      0.82          77.05\n#&gt;           4,642 km (2,884 mi)      1      0.82          77.87      0.82          77.87\n#&gt;           4,656 km (2,893 mi)      1      0.82          78.69      0.82          78.69\n#&gt;           4,669 km (2,901 mi)      1      0.82          79.51      0.82          79.51\n#&gt;           4,690 km (2,910 mi)      1      0.82          80.33      0.82          80.33\n#&gt;           4,694 km (2,917 mi)      1      0.82          81.15      0.82          81.15\n#&gt;           4,734 km (2,942 mi)      1      0.82          81.97      0.82          81.97\n#&gt;           4,773 km (2,966 mi)      1      0.82          82.79      0.82          82.79\n#&gt;           4,779 km (2,970 mi)      1      0.82          83.61      0.82          83.61\n#&gt;           4,808 km (2,988 mi)      1      0.82          84.43      0.82          84.43\n#&gt;           4,822 km (2,996 mi)      1      0.82          85.25      0.82          85.25\n#&gt;           4,898 km (3,043 mi)      1      0.82          86.07      0.82          86.07\n#&gt;           4,922 km (3,058 mi)      1      0.82          86.89      0.82          86.89\n#&gt;           5,091 km (3,163 mi)      1      0.82          87.70      0.82          87.70\n#&gt;           5,286 km (3,285 mi)      1      0.82          88.52      0.82          88.52\n#&gt;           5,287 km (3,285 mi)      1      0.82          89.34      0.82          89.34\n#&gt;           5,289 km (3,286 mi)      1      0.82          90.16      0.82          90.16\n#&gt;           5,343 km (3,320 mi)      1      0.82          90.98      0.82          90.98\n#&gt;           5,375 km (3,340 mi)      1      0.82          91.80      0.82          91.80\n#&gt;           5,380 km (3,340 mi)      1      0.82          92.62      0.82          92.62\n#&gt;           5,386 km (3,347 mi)      1      0.82          93.44      0.82          93.44\n#&gt;           5,398 km (3,354 mi)      1      0.82          94.26      0.82          94.26\n#&gt;           5,425 km (3,371 mi)      1      0.82          95.08      0.82          95.08\n#&gt;           5,440 km (3,380 mi)      1      0.82          95.90      0.82          95.90\n#&gt;           5,476 km (3,403 mi)      1      0.82          96.72      0.82          96.72\n#&gt;           5,485 km (3,408 mi)      1      0.82          97.54      0.82          97.54\n#&gt;           5,503 km (3,419 mi)      1      0.82          98.36      0.82          98.36\n#&gt;           5,560 km (3,450 mi)      1      0.82          99.18      0.82          99.18\n#&gt;           5,745 km (3,570 mi)      1      0.82         100.00      0.82         100.00\n#&gt;                          &lt;NA&gt;      0                               0.00         100.00\n#&gt;                         Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Time/Points  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;       100h 30′ 35″      1      0.82          15.57      0.82          15.57\n#&gt;       100h 49′ 30″      1      0.82          16.39      0.82          16.39\n#&gt;       101h 01′ 20″      1      0.82          17.21      0.82          17.21\n#&gt;       103h 06′ 50″      1      0.82          18.03      0.82          18.03\n#&gt;       103h 38′ 38″      1      0.82          18.85      0.82          18.85\n#&gt;       105h 07′ 52″      1      0.82          19.67      0.82          19.67\n#&gt;       108h 17′ 18″      1      0.82          20.49      0.82          20.49\n#&gt;       108h 18′ 00″      1      0.82          21.31      0.82          21.31\n#&gt;       109h 19′ 14″      1      0.82          22.13      0.82          22.13\n#&gt;       110h 35′ 19″      1      0.82          22.95      0.82          22.95\n#&gt;       112h 03′ 40″      1      0.82          23.77      0.82          23.77\n#&gt;       112h 08′ 42″      1      0.82          24.59      0.82          24.59\n#&gt;       113h 24′ 23″      1      0.82          25.41      0.82          25.41\n#&gt;       113h 30′ 05″      1      0.82          26.23      0.82          26.23\n#&gt;       114h 31′ 54″      1      0.82          27.05      0.82          27.05\n#&gt;       114h 35′ 31″      1      0.82          27.87      0.82          27.87\n#&gt;       115h 27′ 42″      1      0.82          28.69      0.82          28.69\n#&gt;       115h 38′ 30″      1      0.82          29.51      0.82          29.51\n#&gt;       116h 16′ 02″      1      0.82          30.33      0.82          30.33\n#&gt;       116h 16′ 58″      1      0.82          31.15      0.82          31.15\n#&gt;       116h 22′ 23″      1      0.82          31.97      0.82          31.97\n#&gt;       116h 42′ 06″      1      0.82          32.79      0.82          32.79\n#&gt;       116h 59′ 05″      1      0.82          33.61      0.82          33.61\n#&gt;       117h 34′ 21″      1      0.82          34.43      0.82          34.43\n#&gt;       119h 31′ 49″      1      0.82          35.25      0.82          35.25\n#&gt;       122h 01′ 33″      1      0.82          36.07      0.82          36.07\n#&gt;       122h 25′ 34″      1      0.82          36.89      0.82          36.89\n#&gt;       123h 46′ 45″      1      0.82          37.70      0.82          37.70\n#&gt;       124h 01′ 16″      1      0.82          38.52      0.82          38.52\n#&gt;       127h 09′ 44″      1      0.82          39.34      0.82          39.34\n#&gt;       129h 23′ 25″      1      0.82          40.16      0.82          40.16\n#&gt;       130h 29′ 26″      1      0.82          40.98      0.82          40.98\n#&gt;       132h 03′ 17″      1      0.82          41.80      0.82          41.80\n#&gt;       133h 49′ 42″      1      0.82          42.62      0.82          42.62\n#&gt;       135h 44′ 42″      1      0.82          43.44      0.82          43.44\n#&gt;       136h 53′ 50″      1      0.82          44.26      0.82          44.26\n#&gt;       138h 58′ 31″      1      0.82          45.08      0.82          45.08\n#&gt;       140h 06′ 05″      1      0.82          45.90      0.82          45.90\n#&gt;       141h 23′ 00″      1      0.82          46.72      0.82          46.72\n#&gt;       142h 20′ 14″      1      0.82          47.54      0.82          47.54\n#&gt;       142h 47′ 32″      1      0.82          48.36      0.82          48.36\n#&gt;       145h 36′ 56″      1      0.82          49.18      0.82          49.18\n#&gt;       147h 10′ 36″      1      0.82          50.00      0.82          50.00\n#&gt;       147h 13′ 58″      1      0.82          50.82      0.82          50.82\n#&gt;       147h 51′ 37″      1      0.82          51.64      0.82          51.64\n#&gt;       148h 11′ 25″      1      0.82          52.46      0.82          52.46\n#&gt;       148h 29′ 12″      1      0.82          53.28      0.82          53.28\n#&gt;       149h 40′ 49″      1      0.82          54.10      0.82          54.10\n#&gt;       151h 57′ 20″      1      0.82          54.92      0.82          54.92\n#&gt;       154h 11′ 49″      1      0.82          55.74      0.82          55.74\n#&gt;       172h 12′ 16″      1      0.82          56.56      0.82          56.56\n#&gt;       177h 10′ 03″      1      0.82          57.38      0.82          57.38\n#&gt;       186h 39′ 15″      1      0.82          58.20      0.82          58.20\n#&gt;       192h 48′ 58″      1      0.82          59.02      0.82          59.02\n#&gt;       197h 54′ 00″      1      0.82          59.84      0.82          59.84\n#&gt;       198h 16′ 42″      1      0.82          60.66      0.82          60.66\n#&gt;       200h 28′ 48″      1      0.82          61.48      0.82          61.48\n#&gt;       219h 10′ 18″      1      0.82          62.30      0.82          62.30\n#&gt;       221h 50′ 26″      1      0.82          63.11      0.82          63.11\n#&gt;       222h 08′ 06″      1      0.82          63.93      0.82          63.93\n#&gt;       222h 15′ 30″      1      0.82          64.75      0.82          64.75\n#&gt;       226h 18′ 21″      1      0.82          65.57      0.82          65.57\n#&gt;       228h 36′ 13″      1      0.82          66.39      0.82          66.39\n#&gt;       231h 07′ 15″      1      0.82          67.21      0.82          67.21\n#&gt;       238h 44′ 25″      1      0.82          68.03      0.82          68.03\n#&gt;                 31      1      0.82          68.85      0.82          68.85\n#&gt;                 35      1      0.82          69.67      0.82          69.67\n#&gt;                 36      1      0.82          70.49      0.82          70.49\n#&gt;                 37      1      0.82          71.31      0.82          71.31\n#&gt;                 43      1      0.82          72.13      0.82          72.13\n#&gt;                 47      1      0.82          72.95      0.82          72.95\n#&gt;                 49      1      0.82          73.77      0.82          73.77\n#&gt;                 63      1      0.82          74.59      0.82          74.59\n#&gt;        79h 32′ 29″      1      0.82          75.41      0.82          75.41\n#&gt;        82h 05′ 42″      1      0.82          76.23      0.82          76.23\n#&gt;        82h 56′ 36″      1      0.82          77.05      0.82          77.05\n#&gt;        82h 57′ 00″      1      0.82          77.87      0.82          77.87\n#&gt;        83h 17′ 13″      1      0.82          78.69      0.82          78.69\n#&gt;        83h 38′ 56″      1      0.82          79.51      0.82          79.51\n#&gt;        83h 56′ 20″      1      0.82          80.33      0.82          80.33\n#&gt;        84h 27′ 53″      1      0.82          81.15      0.82          81.15\n#&gt;        84h 46′ 14″      1      0.82          81.97      0.82          81.97\n#&gt;        85h 48′ 35″      1      0.82          82.79      0.82          82.79\n#&gt;        86h 12′ 22″      1      0.82          83.61      0.82          83.61\n#&gt;        86h 20′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;        87h 20′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;        87h 34′ 47″      1      0.82          86.07      0.82          86.07\n#&gt;        87h 38′ 35″      1      0.82          86.89      0.82          86.89\n#&gt;        87h 52′ 52″      1      0.82          87.70      0.82          87.70\n#&gt;        89h 04′ 48″      1      0.82          88.52      0.82          88.52\n#&gt;        89h 40′ 27″      1      0.82          89.34      0.82          89.34\n#&gt;        89h 59′ 06″      1      0.82          90.16      0.82          90.16\n#&gt;        90h 43′ 20″      1      0.82          90.98      0.82          90.98\n#&gt;        91h 00′ 26″      1      0.82          91.80      0.82          91.80\n#&gt;        91h 59′ 27″      1      0.82          92.62      0.82          92.62\n#&gt;        92h 08′ 46″      1      0.82          93.44      0.82          93.44\n#&gt;        92h 44′ 59″      1      0.82          94.26      0.82          94.26\n#&gt;        92h 49′ 46″      1      0.82          95.08      0.82          95.08\n#&gt;        94h 33′ 14″      1      0.82          95.90      0.82          95.90\n#&gt;        95h 57′ 09″      1      0.82          96.72      0.82          96.72\n#&gt;        95h 57′ 16″      1      0.82          97.54      0.82          97.54\n#&gt;        96h 05′ 55″      1      0.82          98.36      0.82          98.36\n#&gt;        96h 19′ 38″      1      0.82          99.18      0.82          99.18\n#&gt;        96h 45′ 14″      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Margin  \n#&gt; Type: Character  \n#&gt; \n#&gt;                      Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ------------------ ------ --------- -------------- --------- --------------\n#&gt;                  —     18     14.75          14.75     14.75          14.75\n#&gt;           + 1′ 07″      1      0.82          15.57      0.82          15.57\n#&gt;           + 1′ 11″      1      0.82          16.39      0.82          16.39\n#&gt;           + 1′ 12″      1      0.82          17.21      0.82          17.21\n#&gt;           + 1′ 22″      1      0.82          18.03      0.82          18.03\n#&gt;           + 1′ 25″      1      0.82          18.85      0.82          18.85\n#&gt;           + 1′ 34″      1      0.82          19.67      0.82          19.67\n#&gt;           + 1′ 41″      1      0.82          20.49      0.82          20.49\n#&gt;           + 1′ 42″      1      0.82          21.31      0.82          21.31\n#&gt;           + 1′ 50″      1      0.82          22.13      0.82          22.13\n#&gt;           + 1′ 51″      1      0.82          22.95      0.82          22.95\n#&gt;          + 10′ 32″      1      0.82          23.77      0.82          23.77\n#&gt;          + 10′ 41″      1      0.82          24.59      0.82          24.59\n#&gt;          + 10′ 55″      1      0.82          25.41      0.82          25.41\n#&gt;          + 12′ 14″      1      0.82          26.23      0.82          26.23\n#&gt;          + 12′ 41″      1      0.82          27.05      0.82          27.05\n#&gt;          + 12′ 56″      1      0.82          27.87      0.82          27.87\n#&gt;          + 13′ 07″      1      0.82          28.69      0.82          28.69\n#&gt;          + 14′ 13″      1      0.82          29.51      0.82          29.51\n#&gt;          + 14′ 18″      1      0.82          30.33      0.82          30.33\n#&gt;          + 14′ 34″      1      0.82          31.15      0.82          31.15\n#&gt;          + 14′ 56″      1      0.82          31.97      0.82          31.97\n#&gt;          + 15′ 49″      1      0.82          32.79      0.82          32.79\n#&gt;          + 15′ 51″      1      0.82          33.61      0.82          33.61\n#&gt;          + 17′ 52″      1      0.82          34.43      0.82          34.43\n#&gt;          + 17′ 54″      1      0.82          35.25      0.82          35.25\n#&gt;          + 18′ 27″      1      0.82          36.07      0.82          36.07\n#&gt;          + 18′ 36″      1      0.82          36.89      0.82          36.89\n#&gt;       + 1h 22′ 25″      1      0.82          37.70      0.82          37.70\n#&gt;       + 1h 42′ 54″      1      0.82          38.52      0.82          38.52\n#&gt;       + 1h 48′ 41″      1      0.82          39.34      0.82          39.34\n#&gt;           + 2′ 16″      1      0.82          40.16      0.82          40.16\n#&gt;           + 2′ 40″      1      0.82          40.98      0.82          40.98\n#&gt;           + 2′ 43″      1      0.82          41.80      0.82          41.80\n#&gt;           + 2′ 47″      1      0.82          42.62      0.82          42.62\n#&gt;          + 22′ 00″      1      0.82          43.44      0.82          43.44\n#&gt;              + 23″      1      0.82          44.26      0.82          44.26\n#&gt;          + 24′ 03″      1      0.82          45.08      0.82          45.08\n#&gt;          + 26′ 16″      1      0.82          45.90      0.82          45.90\n#&gt;          + 26′ 55″      1      0.82          46.72      0.82          46.72\n#&gt;          + 27′ 31″      1      0.82          47.54      0.82          47.54\n#&gt;          + 28′ 17″      1      0.82          48.36      0.82          48.36\n#&gt;       + 2h 16′ 14″      1      0.82          49.18      0.82          49.18\n#&gt;       + 2h 59′ 21″      1      0.82          50.00      0.82          50.00\n#&gt;           + 3′ 10″      2      1.64          51.64      1.64          51.64\n#&gt;           + 3′ 21″      2      1.64          53.28      1.64          53.28\n#&gt;           + 3′ 35″      1      0.82          54.10      0.82          54.10\n#&gt;           + 3′ 36″      1      0.82          54.92      0.82          54.92\n#&gt;           + 3′ 40″      1      0.82          55.74      0.82          55.74\n#&gt;           + 3′ 56″      1      0.82          56.56      0.82          56.56\n#&gt;           + 3′ 58″      1      0.82          57.38      0.82          57.38\n#&gt;          + 30 '41″      1      0.82          58.20      0.82          58.20\n#&gt;          + 30′ 38″      1      0.82          59.02      0.82          59.02\n#&gt;              + 32″      1      0.82          59.84      0.82          59.84\n#&gt;          + 35′ 36″      1      0.82          60.66      0.82          60.66\n#&gt;              + 38″      1      0.82          61.48      0.82          61.48\n#&gt;           + 4′ 01″      2      1.64          63.11      1.64          63.11\n#&gt;           + 4′ 04″      1      0.82          63.93      0.82          63.93\n#&gt;           + 4′ 05″      1      0.82          64.75      0.82          64.75\n#&gt;           + 4′ 11″      1      0.82          65.57      0.82          65.57\n#&gt;           + 4′ 14″      1      0.82          66.39      0.82          66.39\n#&gt;           + 4′ 20″      1      0.82          67.21      0.82          67.21\n#&gt;           + 4′ 35″      2      1.64          68.85      1.64          68.85\n#&gt;           + 4′ 53″      1      0.82          69.67      0.82          69.67\n#&gt;           + 4′ 59″      2      1.64          71.31      1.64          71.31\n#&gt;              + 40″      1      0.82          72.13      0.82          72.13\n#&gt;          + 41′ 15″      1      0.82          72.95      0.82          72.95\n#&gt;              + 48″      1      0.82          73.77      0.82          73.77\n#&gt;           + 5′ 02″      1      0.82          74.59      0.82          74.59\n#&gt;           + 5′ 20″      1      0.82          75.41      0.82          75.41\n#&gt;           + 5′ 39″      1      0.82          76.23      0.82          76.23\n#&gt;          + 50′ 07″      1      0.82          77.05      0.82          77.05\n#&gt;          + 54′ 20″      1      0.82          77.87      0.82          77.87\n#&gt;              + 54″      1      0.82          78.69      0.82          78.69\n#&gt;              + 55″      1      0.82          79.51      0.82          79.51\n#&gt;          + 57′ 21″      1      0.82          80.33      0.82          80.33\n#&gt;              + 58″      1      0.82          81.15      0.82          81.15\n#&gt;              + 59″      1      0.82          81.97      0.82          81.97\n#&gt;           + 6′ 17″      1      0.82          82.79      0.82          82.79\n#&gt;           + 6′ 21″      1      0.82          83.61      0.82          83.61\n#&gt;           + 6′ 55″      1      0.82          84.43      0.82          84.43\n#&gt;           + 7′ 13″      1      0.82          85.25      0.82          85.25\n#&gt;           + 7′ 17″      1      0.82          86.07      0.82          86.07\n#&gt;           + 7′ 29″      1      0.82          86.89      0.82          86.89\n#&gt;           + 7′ 37″      1      0.82          87.70      0.82          87.70\n#&gt;           + 8′ 04″      1      0.82          88.52      0.82          88.52\n#&gt;           + 8′ 37″      1      0.82          89.34      0.82          89.34\n#&gt;               + 8″      1      0.82          90.16      0.82          90.16\n#&gt;           + 9′ 09″      1      0.82          90.98      0.82          90.98\n#&gt;           + 9′ 30″      1      0.82          91.80      0.82          91.80\n#&gt;           + 9′ 51″      1      0.82          92.62      0.82          92.62\n#&gt;           +44′ 23″      1      0.82          93.44      0.82          93.44\n#&gt;                 18      1      0.82          94.26      0.82          94.26\n#&gt;                 19      1      0.82          95.08      0.82          95.08\n#&gt;                 20      1      0.82          95.90      0.82          95.90\n#&gt;                 26      1      0.82          96.72      0.82          96.72\n#&gt;                 32      1      0.82          97.54      0.82          97.54\n#&gt;                  4      1      0.82          98.36      0.82          98.36\n#&gt;                 59      1      0.82          99.18      0.82          99.18\n#&gt;                  8      1      0.82         100.00      0.82         100.00\n#&gt;               &lt;NA&gt;      0                               0.00         100.00\n#&gt;              Total    122    100.00         100.00    100.00         100.00\n#&gt; \n#&gt; TDF$Stage wins  \n#&gt; Type: Character  \n#&gt; \n#&gt;               Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; ----------- ------ --------- -------------- --------- --------------\n#&gt;           —     18     14.75          14.75     14.75          14.75\n#&gt;           0      8      6.56          21.31      6.56          21.31\n#&gt;           1     20     16.39          37.70     16.39          37.70\n#&gt;           2     27     22.13          59.84     22.13          59.84\n#&gt;           3     19     15.57          75.41     15.57          75.41\n#&gt;           4     12      9.84          85.25      9.84          85.25\n#&gt;           5      8      6.56          91.80      6.56          91.80\n#&gt;           6      6      4.92          96.72      4.92          96.72\n#&gt;           7      2      1.64          98.36      1.64          98.36\n#&gt;           8      2      1.64         100.00      1.64         100.00\n#&gt;        &lt;NA&gt;      0                               0.00         100.00\n#&gt;       Total    122    100.00         100.00    100.00         100.00\nBut here again we need to be careful. For example the distance variable is coded as a character, and its reporting with frequencies is thus quite stupid.\nThere is nothing automatic, you still need to think about, transform or subset your data.",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Summary tables</span>"
    ]
  },
  {
    "objectID": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "href": "903_summarizing.html#descriptive-statistics-with-modelsummary",
    "title": "22  Summary tables",
    "section": "22.2 Descriptive statistics with modelsummary",
    "text": "22.2 Descriptive statistics with modelsummary\nto be done",
    "crumbs": [
      "Appendices, Tips, tricks and miscellaneous",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Summary tables</span>"
    ]
  },
  {
    "objectID": "999_references.html",
    "href": "999_references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc, and Bera,I. 1998. “Spatial Dependence in Linear\nRegression Models with an Introduction to Spatial Econometrics:\nRegression Models with an Anselin Bera i. INTRODUCTION.” In. CRC\nPress.\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An\nAxiomatic Approach to\nGeographical Space.” Geographical\nAnalysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.\n\n\nCrawley, Michael J. 2012. The R Book. 1st ed. Wiley. https://doi.org/10.1002/9781118448908.\n\n\nFox, J, and Weisber, S. 2024. “An R Companion to Applied\nRegression.” https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125.\n\n\nHaining, Robert P. 2010. “The Nature of Georeferenced\nData.” In, edited by Manfred M. Fischer and Arthur Getis,\n197–217. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-03647-7_12.\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd\nEdition. https://rc2e.com/.\n\n\nOpenshaw, Stan. 1984. The Modifiable Areal Unit Problem.\nConcepts and Techniques in Modern Geography 38. Norwich: Geo.\n\n\nTobler, W. R. 1970. “A Computer Model Simulation of Urban Growth\nin the Detroit Region in Economic Geography 46: 2,\n234240.” Clark University, Worcester, MA.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to\nr.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d.\n“R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Bibliography",
      "References"
    ]
  },
  {
    "objectID": "010_intro.html",
    "href": "010_intro.html",
    "title": "Part I - Introduction",
    "section": "",
    "text": "1  Statistical data analysis for geographers\n2  Spatial data analysis: a definition\n3  Geographical space\n4  Spatial data issues",
    "crumbs": [
      "Part I - Introduction"
    ]
  },
  {
    "objectID": "020_getting_started.html",
    "href": "020_getting_started.html",
    "title": "Part II - Getting Started with R",
    "section": "",
    "text": "5  Getting started with RStudio\n6  Vectors\n7  Data frames and lists\n8  Working with data frames and functions\n9  Reading and writing data to and from R\nFor better introductory material, we recommend\n\nVenables, Bill and Smith, David, M (n.d.)\nLong and Teetor (n.d.)\nWickham, Çetinkaya-Rundel, and Grolemund (n.d.) (including RStudio introduction) (Note it uses a tidyverse approach while we rather stick to base R where possible, except for graphics (ggplot))\n\n\n\n\n\nLong, James (JD), and Paul Teetor. n.d. R Cookbook, 2nd Edition. https://rc2e.com/.\n\n\nVenables, Bill, and Smith, David, M. n.d. “An Introduction to r.” https://cran.r-project.org/doc/manuals/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d. “R for Data Science (2e).” https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part II - Getting Started with R"
    ]
  }
]