# Hypothesis Testing

<!-- **Outline** -->

<!-- 1. [Concept and Definitions](#test) -->

<!-- 2. [Tests on one sample](#test1) -->

<!-- 3. [Tests on two samples](#test2) -->

<!-- 4. [Tests on multiple samples](#testmulti) -->

<!-- 5. [Test of Chi-square](#testchi2) -->

<!-- Inspired by @Greene2012, @Wonnacott1990 -->

In this section we will use again the dataset `UScensus2000.txt`

```{r}
census <- read.table("data/US/UScensus2000.txt", 
                     header = T, sep = "\t")
```

## Concepts and Definitions

### Test

A statistical test is a method used to determine whether there is a significant difference between an expected model (or hypothesis) and the observed data. It is used to assess whether observed differences are due to random chance or if they are statistically significant, thereby allowing to compare a hypothesis against the collected data.

You are interested in a particular question for which only two answers are possible: *yes* or *no*. Each outcome is expressed as an hypothesis: a **null hypothesis** $H_0$ - it often represents the hypothesis of no change - and an **alternative hypothesis** $H_1$ - it often represents the hypothesis of a change. Your sample is made of observations assumed to be *random* and described by a *statistical model*. The two possible answers to the question of interest are:

-   *The null hypothesis is rejected*: it means that the answer to the question is *yes*: the observed differences are significant and sufficiently large that they our hypothesis does not hold.

-   *The null hypothesis is not rejected*: it means that the answer to the question is *no*: the differences observed are due to randomness. With this data our hypothesis still holds.

### Example:

You want to answer the question: *Is a given drug having an impact on a particular disease?*

-   The null hypothesis $H_0$ is: "The drug has no effect, the patient does not feel better or worse with the treatment."

-   The alternative hypothesis $H_1$ is: "The drug has an effect (which can be either positive or negative)."

### Uni- and bi-lateral tests:

There exist two types of tests: bilateral and unilateral.

-   Bilateral Test: $$H_0 = \{ \theta = \theta_0 \} \text{ and } H_1 = \{ \theta \neq \theta_0 \}$$
-   Unilateral Test: Right or left side:

$$H_0 = \{ \theta \leq \theta_0 \} \text{ and } H_1 = \{ \theta > \theta_0 \}$$ $$H_0 = \{ \theta \geq \theta_0 \} \text{ and } H_1 = \{ \theta < \theta_0 \}$$

### Types of errors:

Decision error terms are used to assess how often we may take a wrong decision. They are of two types.

|                    |  $H_0$ true  | $H_1$ true  |
|--------------------|:------------:|:-----------:|
| $H_0$ not rejected | $1 - \alpha$ |   $\beta$   |
| $H_0$ rejected     |   $\alpha$   | $1 - \beta$ |


In the above table, rows correspond to the decision taken while columns correspond to the reality.

Type I Error: $\alpha$

It is the **level of the test**, also called the **significance level**. It is the case of *deciding that $H_1$ is true while $H_0$ is true*. It corresponds to the probability to be wrong when deciding that the alternative hypothesis is true. Its value generally equals 1%, 5% or 10%.

Type II Error: $\beta$. It is the case of *deciding that $H_0$ is true while $H_1$ is true*. The null hypothesis is incorrectly not rejected.

### Power of a Test:

The power of a test is given by $1 - \beta$.

When deciding that $H_0$ is false and being correct. If it tends to 1 when the sample size increases to the infinite, the test is said to be **consistent**.

### Critical Region: $C_\alpha$
To test if a point estimate $\hat{\theta}$ is significantly different from a null value $\theta_0$, we have to consider uncertainty. To do so, we use the notion of **confidence interval** (CI) introduced previously. The confidence interval is also called the *acceptance region*. Any value outside this interval is part or the *rejection region*, it is called the **critical region** and can be expressed as:

    $$CI_{1-\alpha} = [\theta_0 \pm z_\alpha * SE ] = [ \theta_0 - z_{\alpha/2} * SE ; \theta_0 + z_{1-\alpha/2} * SE ] $$

    $$C_\alpha = [ min ; \theta_0 - z_{\alpha/2} * SE] \cup [ \theta_0 + z_{1-\alpha/2} * SE ; max ]$$[^062_hypothesis_test-1]

    From $Z$ the value of the observed statistic we can decide to reject or not the null hypothesis.

    -   If $Z \in C_\alpha$, we reject $H_0$
    -   If $Z \notin C_\alpha$, we do not reject $H_0$

P-Value: $p.value$ or $p$

:   Also called *probability value*. It corresponds to the probability that a given statistic of test goes over the threshold value to be inside the rejection area when $H_0$ is true. When the p-value is very low (close to 0), we are more likely to reject $H_0$.

    -   If $p.value < \alpha$, we reject $H_0$
    -   If $p.value > \alpha$, we do not reject $H_0$

Steps using the critical region

:   1.  Choose $H_0$, $H_1$ and $\alpha$
    2.  Define the test statistic
    3.  Compute the critical region $c_\alpha$ with respect to $\alpha$ and $H_0$
    4.  Compute the value of the test statistic from the observed sample
    5.  Conclusion: reject or do not reject $H_0$ at risk level $\alpha$

Steps using the p-value

:   1.  Choose $H_0$, $H_1$ and $\alpha$
    2.  Define the test statistic
    3.  Compute the $p.value$ from the sample
    4.  Conclusion: reject or do not reject $H_0$ at risk level $\alpha$

[^062_hypothesis_test-1]: The values $min$ and $max$ depend on the range of values that the studied random variable can take. For instance, if $X \in R$, then $min=-\infty$ and $max=+\infty$ as for a normal distribution. And if $X \in R_+$, then $min=0$ and $max=+\infty$, as for a $\chi^2$ distribution.

**Illustration**

```{r, echo=F}
x <- seq(-5,5,0.01)
y <- dnorm(x, 0, 1)
```

```{r}
plot(x, y, type='l', axes = F, xlab = '', ylab = '',
     main = 'alpha = 5%\n Critical region')
axis(1, mean(y), 'null value', col = 'red', col.axis = 'red')
axis(1, 2.5, 'observed value', col = 'blue', col.axis = 'blue')
text(-3, .25, "Distribution of the\n parameter if H0 true")
```

-   Testing $H_0$: $\theta = \theta_0$
-   Is the observed value significantly different from the null value at level $\alpha = 5\%$?

1.  Using the critical region

```{r}
z <- 1.96

plot(x, y, type='l', axes = F, xlab = '', ylab = '',
     main = 'alpha = 5%\n Critical region')
axis(1, mean(y), 'null value', col = 'red', col.axis = 'red')
axis(1, 2.5, 'observed value', col = 'blue', col.axis = 'blue')
text(-3, .25, "Distribution of the\n parameter if H0 true")

polygon(c(z, x[x >= z]), c(0, y[x >= z]), col = 'purple', border = 'purple')
polygon(c(-z, x[x <= -z]), c(0, y[x <= -z]), col = 'purple', border = 'purple')
text(3, .08, "P(X > 1.96) = 2.5%\n = alpha/2", col = 'purple')
text(-3, .08, "P(X < -1.96) = 2.5%\n = alpha/2", col = 'purple')
```

2.  Using the p-value

```{r}
p <- 1.5
a <- 1.96

plot(x, y, type='l', axes = F, xlab = '', ylab = '',
     main = 'alpha = 5%\n P-value')
polygon(c(p, x[x >= p]), c(0, y[x >= p]), col = 'red', border = 'red')
text(p+1, .09, "p.value", col = 'red')
polygon(c(a, x[x >= a]), c(0, y[x >= a]), col = 'purple', border = 'purple')
text(a+.5, .06, "alpha", col = 'purple')
```

<!-- \color{blue} -->

<!-- <font color="lightseagreen"> -->

<!-- Exercise  -->

<!-- :   A teatcher wants to test the following hypothesis, $H_0$: "The students understood what *hypothesis testing* is about". He/she gives a multiple choice questions with 5 questions. For each question, 4 possible answers are proposed but only one is true. Let $x_i$ be the number of good answers for student $i$. We reject $H_0$ for $X \geq 3$. There are 10 students: 3 don't understand anything, 4 understood a third, 3 understood everything. -->

<!--     1. Compute $\alpha$ and $beta$ for each group of student.  -->

<!--     2. What if the teacher decides to reject $H_0$ for $X \geq 1$? Compute $\alpha$ and $beta$.  -->

<!--     3. What if the teacher decides to ask 100 questions instead of 5? For $\alpha = 0.05$, what is the rejection area an the value of $beta$ for the students that belong to the second group? -->

<!-- </font> -->

<!-- \color{black} -->

### Tests on one sample {#test1}

Test on a mean of normal distribution with known variance

:   We consider a random variable $X \sim N(\mu, \sigma^2)$ with $\mu$ unknown and $\sigma^2$ known - very rare cases in practice. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$.

    -   **Testing**: $H_0: \mu = \mu_0$ against $H_1: \mu \neq \mu_0$.
    -   **Test Statistic**: random variable defined by $$Z_n = \dfrac{\hat{\mu_n} - \mu_0}{\sigma / \sqrt{n}} \sim N(0, 1)$$
    -   **The critical values** are read on a standard normal distribution table.

Test on a mean of normal distribution with unknown variance

:   We consider a random variable $X \sim N(\mu, \sigma^2)$ with $\mu$ and $\sigma^2$ unknown - more likely to happen. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$.

    -   **Testing**: $H_0: \mu = \mu_0$ against $H_1: \mu \neq \mu_0$.
    -   **Test Statistic**: random variable defined by $$T_{n-1} = \dfrac{\hat{\mu_n} - \mu_0}{S_{n,c} / \sqrt{n}} \sim t(n-1)$$
    -   **Decision rule**: the critical value $c_\alpha$ is read on a student distribution table.
    -   **The critical values** are read on a student distribution table.

Test on a variance of normal distribution with known mean

:   We consider a random variable $X \sim N(\mu, \sigma^2)$ with $\mu$ known and $\sigma^2$ unknown - very rare cases in practice. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$.

    -   **Testing**: $H_0: \sigma^2 = \sigma^2_0$ against $H_1: \sigma^2 \neq \sigma^2_0$.
    -   **Test Statistic**: random variable defined by $$\chi^2_n = \dfrac{n \hat{\sigma^2_n}}{\sigma^2} \sim \chi^2(n)$$
    -   **The critical values** are read on a chi-squared distribution table.

Test on a variance of normal distribution with unknown mean

:   We consider a random variable $X \sim N(\mu, \sigma^2)$ with $\mu$ and $\sigma^2$ unknown - more likely to happen. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$.

    -   **Testing**: $H_0: \sigma^2 = \sigma^2_0$ against $H_1: \sigma^2 \neq \sigma^2_0$.
    -   **Test Statistic**: random variable defined by $$\chi^2_{n-1,c} = \dfrac{n S^2_{n,c}}{\sigma^2} \sim \chi^2(n-1)$$
    -   **The critical values** are read on a binomial distribution table.

Test on a proportion - Binomial test

:   We consider a random variable $X$. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$.

    -   **Testing**: $H_0: \pi_A = \pi_0$ against $H_1: \pi_A \neq \pi_0$.
    -   **Test Statistic**: random variable defined by $$n_A = n \hat{\pi_{n,A}} \sim B(n, \pi_0)$$
    -   **The critical values** are read on a binomial distribution table.

Test on a proportion - Normal approximation test

:   We consider a random variable $X$. We use a sample of size $n$: $x = {x_1, x_2,..., x_n}$ made of independent realizations of that RV $X$ such that the following inequalities are respected:

    $n \ge 50$, $n\pi_0 \ge 16$ and $n(1-\pi_0) \ge 16$.

    -   **Testing**: $H_0: \pi_A = \pi_0$ against $H_1: \pi_A \neq \pi_0$.
    -   **Test Statistic**: random variable defined by $$Z_n = \dfrac{\hat{\pi_{n,A}} - \pi_0}{\sqrt{\dfrac{\pi_0 (1-\pi_0)}{n}}} \# N(0, 1)$$
    -   **The critical values** are read on a standard normal distribution table.

    The Yates' continuity correction allows the shift from a *discrete* to a *continuous* distribution.

### Tests on two samples {#test2}

Test on 2 means of normal distribution and known variances

:   We consider a random variable $X \sim N(\mu_1, \sigma_1^2)$ and a random variable $Y \sim N(\mu_2, \sigma_2^2)$ with $\mu_1$ and $\mu_2$ unknown, and $\sigma_1^2$ and $\sigma_2^2$ known. We use two samples of size $n_1$: $x = {x_1, x_2,..., x_{n_1}}$ and $n_2$: $y = {y_1, y_2,..., y_{n_2}}$ made of independent realizations of that RVs $X$ and $Y$. $n_1$ and $n_2$ can have different values.

    -   **Testing**: $H_0: \mu_1 = \mu_2$ against $H_1: \mu_1 \neq \mu_2$.
    -   **Test Statistic**: random variable defined by $$Z_{n_1, n_2} = \dfrac{\hat{\mu_1} - \hat{\mu_2}}{\sqrt{\dfrac{\sigma^2_1} {n_1} + \dfrac{\sigma^2_2} {n_2}}} \sim N(0, 1)$$
    -   **The critical values** are read on a standard normal distribution table.

Test on 2 means of normal distribution and unknown variances - $t$ test (Student Test)

:   We consider a random variable $X \sim N(\mu_1, \sigma_1^2)$ and a random variable $Y \sim N(\mu_2, \sigma_2^2)$ with $\mu_1$, $\mu_2$, $\sigma_1^2$ and $\sigma_2^2$ unknown. We use two samples of size $n_1$: $x = {x_1, x_2,..., x_{n_1}}$ and $n_2$: $y = {y_1, y_2,..., y_{n_2}}$ made of independent realizations of that RVs $X$ and $Y$. $n_1$ and $n_2$ can have different values but we need $\sigma^2_1 = \sigma^2_2 = \sigma^2$ (to test this hypothesis, look at the Fisher-Snedecor Test on two variances).

    -   **Testing**: $H_0: \mu_1 = \mu_2$ against $H_1: \mu_1 \neq \mu_2$.
    -   **Test Statistic**: random variable defined by $$T_{n_1 + n_2 - 2} = \dfrac{\hat{\mu_1} - \hat{\mu_2}}{\hat{\sigma} \sqrt{\dfrac{1} {n_1} + \dfrac{1} {n_2}}} \sim t(n_1 + n_2 - 2)$$ with $$\hat{\sigma} = \sqrt{ \dfrac{n_1 S^2_{n_1} + n_2 S^2_{n_2}}{n_1 + n_2 - 2} }$$
    -   **The critical values** are read on a student distribution table.

Test on 2 means of normal distribution and unknown variances - Welch test

:   We consider a random variable $X \sim N(\mu_1, \sigma_1^2)$ and a random variable $Y \sim N(\mu_2, \sigma_2^2)$ with $\mu_1$, $\mu_2$, $\sigma_1^2$ and $\sigma_2^2$ unknown. We use two samples of size $n_1$: $x = {x_1, x_2,..., x_{n_1}}$ and $n_2$: $y = {y_1, y_2,..., y_{n_2}}$ made of independent realizations of that RVs $X$ and $Y$. $n_1$ and $n_2$ can have different values and we accept $\sigma^2_1 \ne \sigma^2_2$ (to test this hypothesis, look at the Fisher-Snedecor Test).

    -   **Testing**: $H_0: \mu_1 = \mu_2$ against $H_1: \mu_1 \neq \mu_2$.
    -   **Test Statistic**: random variable defined by $$T_{v} = \dfrac{\hat{\mu_1} - \hat{\mu_2}}{\sqrt{\dfrac{S^2_{n_1}} {n_1 - 1} + \dfrac{S^2_{n_2}} {n_2 - 1}}} \sim t(v)$$ with $v$ close to $(\dfrac{S^2_{n_1}} {n_1 - 1} + \dfrac{S^2_{n_2}} {n_2 - 1})^2 / (\dfrac{S^4_{n_1}} {(n_1 - 1)^3} + \dfrac{S^4_{n_2}} {(n_2 - 1)^3})$
    -   **The critical values** are read on a student distribution table.

Test on 2 variances of normal distribution and unknown means - F test (Fisher-Snedecor Test)

:   We consider a random variable $X \sim N(\mu_1, \sigma_1^2)$ and a random variable $Y \sim N(\mu_2, \sigma_2^2)$ with $\mu_1$ and $\mu_2$ unknown, and $\sigma_1^2$ and $\sigma_2^2$ known. We use two samples of size $n_1$: $x = {x_1, x_2,..., x_{n_1}}$ and $n_n$: $y = {y_1, y_2,..., y_{n_2}}$ made of independent realizations of that RVs $X$ and $Y$. $n_1$ and $n_2$ can have different values.

    -   **Testing**: $H_0: \sigma^2_1 = \sigma^2_2$ against $H_1: \sigma^2_1 \neq  \sigma^2_2$.
    -   **Test Statistic**: random variable defined by $$F = \dfrac{S^2_{n_1,c}}{S^2_{n_2,c}} \sim F(n_1 - 1, n_2 - 1)$$
    -   **The critical values** are read on a standard normal distribution table.

<!-- ### Tests on multiple samples  {#testmulti} -->

<!-- ### Tests: Chi-square  {#chi2} -->
