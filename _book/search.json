[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and R for geographers",
    "section": "",
    "text": "Preface\nThis is the syllabus for the course Introduction to Data Analysis for Geographers with R (MAGEO0641) at the Department of Geography and Spatial Planning at the University of Luxembourg. It has been produced as a Quarto book by Geoffrey Caruso and Léandre Fabri with the aim of aggregating and homogenizing different material accumulated over the past few years.\nAs of September 2024, the assemblage is still a work in progress. We kindly ask you to refer to your notes during class to prepare for the examination and assignments.\nThe general structure and base material shown here have been organized by Geoffrey Caruso, who originally inherited teaching material from Dominique Peeters. We are grateful to the previous teaching assistants, Mirjam Schindler and Marlène Boura, as well as to David Dabin and Jonathan Jones, for their contributions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "timetable.html",
    "href": "timetable.html",
    "title": "Timetable",
    "section": "",
    "text": "The course is made of 13 sessions. Each session comprises 5 (teaching) units of 45 minutes.\nWe have attempted to balance theoretical explanations and R practice in each session.\nThe 2024-2025 schedule is set as follows. However, please refer to your student guichet and the Moodle platform for potential changes during the semester.\n\nScheduled: 16 Sep 2024 at 14:00 to 18:00, CEST\nScheduled: 23 Sep 2024 at 14:00 to 18:00, CEST\nNo session on 30 Sep 2024\nScheduled: 7 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 14 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 21 Oct 2024 at 14:00 to 18:00, CEST\nScheduled: 28 Oct 2024 at 14:00 to 18:00, CET\nScheduled: 4 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 11 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 18 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 25 Nov 2024 at 14:00 to 18:00, CET\nScheduled: 2 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 9 Dec 2024 at 14:00 to 18:00, CET\nScheduled: 16 Dec 2024 at 14:00 to 18:00, CET",
    "crumbs": [
      "Timetable"
    ]
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Objectives:",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "learning.html#objectives",
    "href": "learning.html#objectives",
    "title": "Learning outcomes and evaluation",
    "section": "",
    "text": "Overview of key concepts and formalisation in data analysis in geographical contexts\nUnderstanding and describing the statistical and spatial distribution of data (univariate statistical analysis)\nUnderstanding how a geographical pattern relate or can be understood from others (bivariate and regression analysis)\nRaising awareness as to the characteristics and difficulties of statistical and econometric (regression) analysis with geographical data.\nMastering essential R software skills for tabular data management, uni and bi-variate analysis and regression, and producing graphics",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "learning.html#expected-outcomes",
    "href": "learning.html#expected-outcomes",
    "title": "Learning outcomes and evaluation",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nOn completion, each student should be able to\n\nDescribe the key concepts in spatial statistical analysis and the specificities of geographical space and spatial data\nDemonstrate a good command of R to handle statistical datasets and perform univariate, bivariate and multiple regressions analyses with good diagnostics\nExplain and use common univariate and bivariate statistics\nExplain and use exploratory methods\nExplain and apply standard regression methods and diagnostics, and discuss limits and problems when applied to geographical data\nExplain the principles and methods used to identify local effects and spatial autocorrelation\nRead and discuss detailed results of an empirical research article that deal with data analysis including a multivariate regression in a spatial or non-spatial context\nExplain and use mixed methods (Q-Methodology: hybrid approach between quantitative and qualitative methods)",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "learning.html#evaluation",
    "href": "learning.html#evaluation",
    "title": "Learning outcomes and evaluation",
    "section": "Evaluation",
    "text": "Evaluation\n\nIndividual\n20% Continuous assessment: small tests in class and weekly exercises:\n40% R exam (3h) in GIS room, perform R analysis, answer a questionnaire and provide script\n40% Oral exam (30min): presenting a paper and answering questions about its details and the course",
    "crumbs": [
      "Learning outcomes and evaluation"
    ]
  },
  {
    "objectID": "scope.html",
    "href": "scope.html",
    "title": "1  Statistical data analysis for geographers",
    "section": "",
    "text": "1.1 Scope\nThis course is an introduction to standard statistical techniques that geographers often encounter. Being at the crossroads of different fields, it is necessary for geographers to have a basic set of tools with which to interact with other experts and modelers who, although they may use different wording and have their own technical habits, use a common set of concepts and tools to analyze data and test their hypotheses.\nIn their own work and in their interactions with others, it is also important for geographers to keep in mind that the data they use are quite specific because they are about located objects or subjects, about places and their interactions. Most of the time, the data they use is georeferenced in some way (accurately or not). This inherently geographic aspect brings with it a number of challenges. In this course, we will highlight these challenges when performing standard data analysis. However, we won’t solve any of these geographic problems, and we won’t even explicitly use geographic features, i.e., no mapping, no use of georeferencing as such. Our goal is to equip students with standard statistical methods also used in related fields, with some critical thinking about their geographical nature or underlying spatial processes. In a nutshell, this is a journey from elementary statistics to spatial autocorrelation with a standard regression detour.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "href": "scope.html#sec-transparent-geographical-analysis-empowered-with-r",
    "title": "1  Statistical data analysis for geographers",
    "section": "1.2 Transparent geographical analysis empowered with R",
    "text": "1.2 Transparent geographical analysis empowered with R\nThe course is a blend of theory and practice, which we believe enhances intuition and understanding. Direct practice also helps - especially for human geographers with little training in quantitative methods - to demystify statistical concepts and provide confidence after repeated applications and interpretations.\nSoftware for statistical analysis has evolved rapidly and R is prominent in many disciplines. It is open and free. It is simply fantastic for spatial data analysis and may well be the only tool geographers really need in their data undertaking, even replacing GIS (Geographic Information Systems) software. You just need to get started with R.\nMost of our students have had some sort of theoretical statistics course so far in their studies, so they have probably seen most of the content. Sometimes our students have had some practice with SPSS (or similar) software, but most have not used any real statistical software at all, and have a spreadsheet (e.g., Excel) as their only reference for data management, analysis, and graphing. We chose R for its openness, leadership, large community, and later for its spatial features, but also because it forces students to be transparent and think about every step they take. Data analysis and visualization can be misleading and dangerous, it is necessary to think and facilitate replication for oneself and for others. R, and more generally the use of scripts and command lines, is absolutely necessary to bring robustness and trust.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical data analysis for geographers</span>"
    ]
  },
  {
    "objectID": "space_issues.html",
    "href": "space_issues.html",
    "title": "2  Spatial data analysis: definition and challenges",
    "section": "",
    "text": "2.1 Interdisciplinarity and perspective\nData analysis and statistics are used in many scientific fields. When the focus is on geographic objects, subjects, their patterns or relationships, we like to talk about spatial analysis. However, because geographic data is relevant to many fields and statistical methods are widely used, spatial analysis can be defined and approached differently.\nWithin geography, we understand the term spatial analysis to refer to all quantitative approaches, as opposed to qualitative approaches (although these can also be spatial and analytical). Within the quantitative part of the discipline, however, spatial analysis would mostly refer to applied statistical approaches, in contrast to GIS modeling, geosimulation, transportation modeling, or mathematical models. In this sense, this course is a spatial analysis course.\nHowever, for those researchers involved in spatial analysis close to regional science and economic geography (and perhaps close to landscape ecology and GIScience), the terms spatial data analysis or spatial statistics would more strictly refer to the explicit use of geographic information in the modeling process, not just the consideration of geographic elements. See for example the (10.1007/978-3-642-03647-7?) handbook for discussion and examples.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: definition and challenges</span>"
    ]
  },
  {
    "objectID": "space_issues.html#links-with-theory",
    "href": "space_issues.html#links-with-theory",
    "title": "2  Spatial data analysis: definition and challenges",
    "section": "2.2 Links with theory",
    "text": "2.2 Links with theory\nScience progresses with tools and techniques but also by testing hypotheses and updating models and theory. How spatial analysis is linked to theory also depends on fields or subfields.\nFrom a quantitative geography viewpoint (adapted from Denise Pumain https://hypergeo.eu/theories-of-spatial-analysis/?lang=en), spatial analysis focuses on uncovering spatial structures and organizations. These structures can often be generalized into models, such as center-periphery relationships, gravity models, and urban hierarchies and networks.\nThe ultimate goal of spatial analysis is then to understand the processes that lead to the formation of these spatial structures.\nFrom a spatial economic or regional science viewpoint (as understood by a European quantitative geographer) spatial analysis consists of a set of techniques designed to:\n\nDescribe the location of activities and how they change over time\nEstimate reduced form models\n\nUnlike structural form models, which are direct representations or formulations of theoretical concepts, reduced form models are designed to better align with and fit the data.\nThere is probably no such a reduced or structural form model in quantitative geography, but in both case anyway, spatial analysis ultimately aims at testing and updating theories.\nWe very much agree with this perspective here, leading to giving more importance to the falsification of ideas and the interpretation of estimated coefficients much more than to prediction using as many data as possible. If a variable is used it is because we have some idea of its importance and influence on others, not just to obtain a fit. Hence we won’t use automatic models constructions (no automated regression variable selection), and leave aside all the methods (neural networks, random forests, etc.) from which coefficients (if any) are difficult to interpret, even if they can be considered to belong to spatial analysis and use data. This course is not about data mining or data crunching. We use a statistical lens to examine variations across space and how spatial relationships influence socio-economic patterns and behaviors or environmental effects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: definition and challenges</span>"
    ]
  },
  {
    "objectID": "space_issues.html#equivalence-and-independence",
    "href": "space_issues.html#equivalence-and-independence",
    "title": "2  Spatial data analysis: definition and challenges",
    "section": "2.3 Equivalence and Independence",
    "text": "2.3 Equivalence and Independence\nStatistical analysis is based on two key principles, or invariants:\n\nAll observations must be statistically equivalent: This means that no individual observation should be systematically different from others in the sample set. Each data point must have the same probability distribution, ensuring uniformity and comparability.\nAll observations must be independent from each other: In any statistical model, the assumption is that the occurrence of one observation does not influence or depend on another. Independence ensures the integrity of statistical results.\n\nHowever, both of these principles are challenged when applied in a spatial context. Spatial data often exhibit dependencies due to geographic proximity, which violates the assumption of independence. Similarly, the notion of statistical equivalence becomes problematic as spatial heterogeneity introduces variability across observations in different locations.\nThese challenges highlight the need for specialized approaches in spatial analysis, where standard statistical methods must be adapted to account for the structure and dependencies present in the spatial data.\n(Based on discussions in Jayet, p. 2-13)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: definition and challenges</span>"
    ]
  },
  {
    "objectID": "space_issues.html#section",
    "href": "space_issues.html#section",
    "title": "2  Spatial data analysis: definition and challenges",
    "section": "2.4 ",
    "text": "2.4",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data analysis: definition and challenges</span>"
    ]
  },
  {
    "objectID": "space_axiom.html",
    "href": "space_axiom.html",
    "title": "3  Geographical space",
    "section": "",
    "text": "Content\nAxiomatic Beguin and Thisse (1979)\n\n\n\n\nBeguin, Hubert, and Jacques-François Thisse. 1979. “An Axiomatic Approach to Geographical Space.” Geographical Analysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geographical space</span>"
    ]
  },
  {
    "objectID": "R_basics.html",
    "href": "R_basics.html",
    "title": "R basics",
    "section": "",
    "text": "Content",
    "crumbs": [
      "R basics and visualisation",
      "R basics"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "4  Graphics ggplot way",
    "section": "",
    "text": "Content",
    "crumbs": [
      "R basics and visualisation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Graphics ggplot way</span>"
    ]
  },
  {
    "objectID": "univariate.html",
    "href": "univariate.html",
    "title": "5  Univariate statistics",
    "section": "",
    "text": "Content",
    "crumbs": [
      "Univariate analysis and distributions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "6  Statistical distributions",
    "section": "",
    "text": "Content",
    "crumbs": [
      "Univariate analysis and distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical distributions</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "7  Covariance and correlation",
    "section": "",
    "text": "Content",
    "crumbs": [
      "Bivariate analysis and regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Covariance and correlation</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Content",
    "crumbs": [
      "Bivariate analysis and regression",
      "Regression"
    ]
  },
  {
    "objectID": "CIregression.html",
    "href": "CIregression.html",
    "title": "8  Confidence interval of a regression”",
    "section": "",
    "text": "8.1 Load Data and Packages\nWe use the Scotland rain data and load the ggplot package plus an additional related ggplot package (ggpmisc) to ease some formatting\nRainScotland &lt;- read.csv(\"data/Ferguson/RainScotland.csv\")\nlibrary(ggplot2)\nlibrary(ggpmisc)\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate",
    "crumbs": [
      "Bivariate analysis and regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence interval of a regression\"</span>"
    ]
  },
  {
    "objectID": "CIregression.html#scatter-plot",
    "href": "CIregression.html#scatter-plot",
    "title": "8  Confidence interval of a regression”",
    "section": "8.2 Scatter plot",
    "text": "8.2 Scatter plot\nWe plot the data and add the OLS line, its equation, vertical residual lines and the confidence interval (pink area).\nWe emphasize two points (the 7th and 18th) for which we later calculate the confidence interval. We see 18 is located outside of the confidence interval and 7 is located right on the estimated line, but is also one of the largest points in terms of elevation.\n\np&lt;-ggplot(RainScotland, aes(x = Elevation, y = Rainfall))  +\n  stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")),formula = y ~ x, parse = TRUE) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\",fill=\"lightpink\", alpha = 0.3)+\n  geom_segment(aes(xend = Elevation, yend = predict(lm(Rainfall ~ Elevation, data = RainScotland))), linetype = \"dashed\", color = \"gray\", size = 0.5) +\n  labs(title = \"Rainfall as a function of Elevation\", x = \"Elevation (m)\", y = \"Rainfall (mm/yr)\")+\n  geom_point(data=RainScotland[18,], size=5, color=\"red\")+\n  geom_point(data=RainScotland[7,], size=5, color=\"lightblue\")+\n  geom_point()+\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np\n\nWarning: The dot-dot notation (`..eq.label..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(eq.label)` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Bivariate analysis and regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence interval of a regression\"</span>"
    ]
  },
  {
    "objectID": "CIregression.html#confidence-interval-calculation",
    "href": "CIregression.html#confidence-interval-calculation",
    "title": "8  Confidence interval of a regression”",
    "section": "8.3 Confidence Interval calculation",
    "text": "8.3 Confidence Interval calculation\nWe now show how the pink area representing the 95% confidence interval is calculated.\nThe lower and upper bound for each predicted value can be obtained using the predict function. We can ask prediction and confidence bounds for any point (using newdata with the same variables as the original variables), but here we do it for all the observed ones and show the values for point 7 and 18.\n\nmodel &lt;- lm(Rainfall ~ Elevation, data = RainScotland)\n\npred &lt;- predict(model, newdata = data.frame(Elevation = RainScotland$Elevation), interval = \"confidence\")\n\npred[c(7,18),]\n\n        fit      lwr      upr\n7  2131.546 1908.562 2354.530\n18 1537.208 1415.837 1658.579\n\npredicted_value18 &lt;- pred[18,\"fit\"]\npredicted_value7 &lt;- pred[7,\"fit\"]\n\nWe now compute the total residual standard error (RSE) of the model (also called the mean square error):\n\\[RSE=\\sqrt{\\frac{\\sum (\\text{residuals(model)})^2}{\\text{model degrees of freedom}}}\\],\nwhich measures the average distance at which the observed values fall from the regression line. The residuals are the differences between the observed and predicted values (our vertical thin lines on the above figure).\nThe sum of squared residuals is divided by the degrees of freedom. In this case the degrees of freedom is 18 because we have 20 individuals and 2 estimated parameters: the slope (2.38mm/yr) and the intercept (elevation 895m).\nWe take the square root to get the RSE back to rainfall units, in this case it is equal to 243mm\n\nRSE&lt;-sqrt(sum(residuals(model)^2) / model$df.residual)\nRSE\n\n[1] 242.7918\n\n\nNote that the RSE often denoted by sigma is also found in the summary of the regression model:\n\nsummary(model)$sigma\n\n[1] 242.7918\n\n\nFor a specific observation, say the 18th or the 7th observation, we adjust the RSE for the known characteristics of the observation, i.e. its value along elevation, the predictor variable. More specifically if the observed elevation is very much away from the mean, it is going to have a large deviation to the mean, hence a large share of the total deviations (always squared to avoid mixing pluses and minuses) to the mean elevation in the sample and thus potentially more impact on the regression line.\nThese relative deviations are\n\nelevation_relative_dev18&lt;-(RainScotland$Elevation[18] - mean(RainScotland$Elevation))^2 / sum((RainScotland$Elevation - mean(RainScotland$Elevation))^2)\n\nelevation_relative_dev7&lt;-(RainScotland$Elevation[7] - mean(RainScotland$Elevation))^2 / sum((RainScotland$Elevation - mean(RainScotland$Elevation))^2)\n\nelevation_relative_dev18\n\n[1] 0.006616382\n\nelevation_relative_dev7\n\n[1] 0.1410991\n\n\nMathematically, the standard error of the fit at a given point ( ) is given by: \\[\n   \\text{SE}(\\hat{y}) = RSE\\sqrt{\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\n   \\]\nwhere we see it increases with the global variance (variance of residuals) of the model, i.e. RSE, decreases with the sample size n, and increases with the distance to the mean along the x variable, which is the relative deviation quantity we just computed (compare 7 and 18).\nLeaving aside the RSE, which we already calculated, we can combine the last two terms present under the square root and define the leverage of an observation (and simplify the standard error writing)\n\\[L=\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\\\\\n\\text{SE}(\\hat{y}) = RSE\\sqrt{L}\\]\nLeverage values range between \\(\\frac{1}{n}\\) and 1, with higher values indicating greater influence on the regression model. 1 is for an observation that would stand extremely far from the mean and taking most of the variation in x.\nPoints farther from the mean of x have higher leverage, have a greater influence on the regression line and thus can increase the uncertainty of the predicted line. In Greene (p.99) ’s terms:\n\n“the farther the forecasted point is from the center of our experience, the greater is the degree of uncertainty”\n\nThis is why the confidence intervals (pink ribbon) usually get wider away from the mean of x. This process is also reinforced by the density of data points, which is typically higher around the mean of x. The higher density providing more information and reducing uncertainty near the mean.\nThe equation above also shows that the leverage will decrease for every point as soon as n increases. When n increases the uncertainty decreases and the ribbon is narrower.\nTo compute the leverage for 7 and 18 we thus simply add \\(1/n\\) to our relative deviations:\n\nlever7&lt;-1/nrow(RainScotland)+elevation_relative_dev7\nlever18&lt;-1/ nrow(RainScotland)+elevation_relative_dev18\nlever7\n\n[1] 0.1910991\n\nlever18\n\n[1] 0.05661638\n\n\nWith p predictors (here p=1) and n observations (here n=20), a rule of thumb is to consider a leverage is too high when \\(\\frac{p+1}{n}\\) is significantly different from the average leverage. In our case, we have \\(\\frac{p+1}{n}=0.1\\) and observation 7 may be considered having too much influence. In practice however, the significance of a leverage is examined after removing high leverage points and evaluating the effect of this removal on the regression coefficients and the overall model fit.\nPursuing, we compute the standard errors of the fit at the level of our observations 7 and 18:\n\nse7&lt;-RSE*(lever7)^(1/2)\nse18&lt;-RSE*(lever18)^(1/2)\nse7\n\n[1] 106.1362\n\nse18\n\n[1] 57.77037\n\n\nIn order to draw the pink ribbon, we need to additionally set a level of confidence to our estimate. By default most researchers use a 95% confidence interval. We then multiply our point standard error by the corresponding t-statistics, and add/remove it from the prediction to obtain the upper and lower bounds of the confidence interval area:\nWe first get the critical value from the t-distribution, using half of the 5% (thus 0.975) on both side:\n\nt_value &lt;- qt(0.975, df = model$df.residual)\n\nthen multiply by the standard error and add/remove it to/from prediction:\n\nlower18&lt;-predicted_value18-t_value*se18\nlower7&lt;-predicted_value7-t_value*se7\nupper18&lt;-predicted_value18+t_value*se18\nupper7&lt;-predicted_value7+t_value*se7\n\nmanualCI&lt;-data.frame(\n  Elevation=RainScotland[c(7,18),\"Elevation\"],\n  UpperCI=c(upper7,upper18),\n  LowerCI=c(lower7,lower18)\n  )\nmanualCI\n\n  Elevation  UpperCI  LowerCI\n1       520 2354.530 1908.562\n2       270 1658.579 1415.837\n\n\nWe can verify that these values are the same as the interval made up by the predict function with the confidence option (see above)\n\npred[c(7,18),]\n\n        fit      lwr      upr\n7  2131.546 1908.562 2354.530\n18 1537.208 1415.837 1658.579\n\n\nas well as the one computed by ggplot with se=TRUE option in geom_smooth\n\np+geom_segment(data=manualCI, aes(x=Elevation,xend=Elevation,y=LowerCI, yend =UpperCI), color = \"blue\", size = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n##Prediction versus confidence interval\nFinally, note that there is another interval to be used for prediction when we get new data, i.e. the prediction interval. You can get this interval from the predict function with the prediction option instead of confidence.\nThe standard error one uses here has the same ingredients but a slightly different definition:\n\\[\\text{SE}(\\hat{y}) = RSE\\sqrt{1+ \\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\\\\\n   \\text{SE}(\\hat{y}) = RSE\\sqrt{1+L}\\]\nWhile the confidence interval we have computed previously tells you how confident you can be that the fitted model represents well the data that was used for estimating the model, the prediction interval is for use when you want to infer a value for a new data based on the fitted model. Confidence intervals focus on the precision of the sample mean, while prediction intervals account for the variability of individual data points around that mean.\nSuppose you have a new observation at 400m elevation where a rainfall of 1500 is measured. You can input the elevation (and all the other predictor variables in case of a multiple regression) in the predict function to find the range within which you can be 95% confident that the new observation is included, and thus for which the model is a reasonable tool to predict the rainfall.\n\npred_p &lt;- predict(model, newdata = data.frame(Elevation = 400), interval = \"prediction\")\n\nIn this case 1500 mm falls within the range of the model.\nBy construction, confidence intervals are narrower than prediction intervals. Prediction intervals have an extra RSE in the calculation. The intuition is that we don’t know the mean nor the variance of the population from which new observations are taken and the only thing we can do is to estimate the mean from the mean of our estimates, which itself will vary based on the variance considered. We can only use the RSE again to estimate this variance for the mean and the variance itself. Hence the RSE enters twice the equation.\nAn important implication of the presence of that extra 1 added to L under the square root is that while L decreases when sample size increases, one RSE remains and the decrease in uncertainty for the prediction interval is limited. In other words (again Greene, p99):\n\n“No matter how much data we have, we can never predict perfectly”",
    "crumbs": [
      "Bivariate analysis and regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence interval of a regression\"</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beguin, Hubert, and Jacques-François Thisse. 1979. “An\nAxiomatic Approach to\nGeographical Space.” Geographical\nAnalysis 11 (4): 325–41. https://doi.org/10.1111/j.1538-4632.1979.tb00700.x.",
    "crumbs": [
      "Bibliography",
      "References"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "1  Statistical data analysis for geographers\n3  Geographical space",
    "crumbs": [
      "Introduction"
    ]
  }
]