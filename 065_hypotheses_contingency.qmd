# Hypotheses testing - contingency tables

## Chi-squared test of independence

The **chi-squared test for independence** assesses whether the distribution of sample categorical data matches the expected distribution under the assumption of independence between the two variables.

If the observed frequencies in a contingency table that is cross tabulating two categorical variables, significantly deviate from the expected frequencies, it suggests that there is an association between some categories (levels) of the two variables (factors).

> Note: - When there are only two categories for the two variables, we have seen earlier it is similar to a test of proportions between two samples - The *chi-squared test for adequation/goodness of fit* we have seen earlier considers one variable only (could be with more than 2 levels) but uses the same function applied to a case where the contingency table has only one row.

The chi-squared test of independence is formalised as follows:

Given two random qualitative/factorial variables $X$ and $Y$, we aim to determine whether there is a dependent or independent relationship between them.

In other words we want to know if the observations are distributed within the cells of the contingency table (cross tabulation of counts) according to their respective share or if they are over- or under-represented in one or more cells.

Suppose there are $I$ categories $i$ for $X$ and $J$ categories $j$ for $Y$. Let $exp_{i,j}$ represent the theoretical frequencies and $obs_{i,j}$ the observed ones.

**Testing**: $H_0: obs_{i,j} = exp_{i,j}$ $\forall i,j$

**Test Statistic**: a random variable defined by

$$ \chi^2(\text{obs}) = \sum_{i=1}^I \sum_{j=1}^J \dfrac{(\text{obs}_{i,j} - \text{exp}_{i,j})^2}{\text{exp}_{i,j}}$$

$$\chi^2(\text{obs}) \sim \chi^2((I-1)(J-1)) $$ where $(I-1)(J-1)$ is the degrees of freedom.

The expectation in each cell is simply based on the occurrence of each categroy independently, that is $$ \text{exp}_{i,j} = \frac{(\text{obs}_{i \cdot} \cdot \text{obs}_{\cdot j})}{n} $$ where - \text{obs}*{i* \cdot} ) is the total number of observations in row $i$ - \text{obs}{\cdot j} ) is the total number of observations in column $j$ and $n$ is the total count, i.e. \$n=\sum*{i=1}\^I* \text{obs}{i \cdot}=\sum*{j=1}\^J* \text{obs}{\cdot j} \$

## Example

Suppose the following matrix representing counts of individuals from 3 countries using one of 4 modes of transportation:

```{r}
n<-1000
set.seed(15)
p<-runif(12)
v<-matrix(round(n*p/sum(p)),ncol=4)
rownames(v)<-c("LU","BE","FR")
colnames(v)<-c("foot","cycle","bus","car")
addmargins(v)

```

If the matrix is turned into a table, a kind of bar plot can represent those occurrences visually

```{r}
plot(as.table(v))
```

The chi-square test (applicable to the matrix ot to to the table) is returned as follows:

```{r}
chisq.test(v)
```

The null hypothesis is that the joint distribution of the cell counts is the product of the row and column marginal proportions.

In addition to the test itself, the function returns the sample and the expected joint distributions:

```{r}
G<-chisq.test(v)
G$observed
G$expected
```

The observed matrix is the one we provided and the expected matrix is simply the multiplication of the two margin proportions for each item, multiplied by n to turn proportions into occurrences:

```{r}
rowSums(v)/n
colSums(v)/n
```

For example the number of car drivers from France is expected to be

```{r}
n*(rowSums(v)/n)[3]*(colSums(v)/n)[4]
```

We can thus see that a chi-square test is simply an answer to "Knowing the marginal proportions, what count can you expect for each item ?" and then "Is the total difference over all items a significant one?" In the case the observed counts per cell differ from the expected based on marginal proportions, it does mean that one or several categories of one variable is affected by the categories of the other variable, e.g. transportation mode is not independent from the country of origin.

In addition to the test, the observed and epected counts, the function also returns the residual. The residual is the difference between observed and expected counts in each cell, but is divided by the square root of the expected value. Since you expect that difference to be larger for a larger occurrence you actually want to get rid of that size effect with this division and better compare cells whether they have a high or low expectation.

Note that in the case of smaller samples, expected values will get smaller as well. The `chi-square()` function will not be considered safe and returns a warning as soon one cell is expected to be less than 5.

See for example the output with a smaller sample, while keeping the same observed proportions:

```{r}
n2<-100
set.seed(15)
v2<-matrix(round(n2*p/sum(p)),ncol=4)
rownames(v2)<-c("LU","BE","FR")
colnames(v2)<-c("foot","cycle","bus","car")
addmargins(v2)
G2<-chisq.test(v2)
```

You see that several cells are expected to be below 5:

```{r}
G2$expected
```

And the conclusion drawn from the small sample is different than with the large one, since we cannot now reject the null hypothesis and must conclude that transportation is independent from the country of origin.

```{r}
G
G2
```

In such circumstances where you see that the Belgian cases were too few, it may make sense to aggregate some levels. Let's group Belgians and Luxembourgers together:

```{r}
vgr<-v2  #copy previous table
vgr[1,]<-vgr[2,]+vgr[1,] #sum BE and LU into first row
vgr<-vgr[c(1,3),] #keep only first row (BE+LU) and FR row
rownames(vgr)<-c("BE+LU","FR") #adapt name
```

Which turns the chi squared test into a valid one this time:

```{r}
G3<-chisq.test(vgr)
G3
G3$expected
```

Still, given the small sample, you see the null hypothesis can't be rejected (but the chi.square test is relevant now)

## Exercise

Explore a number of cross-tabulation tables from the Luxembourg2021 (census) data folder.
